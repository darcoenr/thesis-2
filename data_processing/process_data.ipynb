{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239fee40-d43d-440a-af55-6876ebdbc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668d2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_if_not_exists(path):\n",
    "    if not pathlib.Path(path).exists():\n",
    "        os.mkdir(path)\n",
    "\n",
    "def create_train_val_test_dir(path):\n",
    "    for dir in ['train', 'val', 'test']:\n",
    "        create_dir_if_not_exists('{}/{}'.format(path, dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f395900-dc42-497b-9cbb-749c2399ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the OAS files\n",
    "OAS_FILE_DIR = r'../../datasets/OAS_sample'\n",
    "\n",
    "DATASET_DIRECTORY = r'../datasets/'\n",
    "create_dir_if_not_exists(DATASET_DIRECTORY)\n",
    "\n",
    "# Directory to store the \"raw\" sequences\n",
    "SEQUENCES_DIRECTORY = '{}/sequences'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(SEQUENCES_DIRECTORY)\n",
    "\n",
    "PAIRED_SEQUENCES_FILE = '{}/sequences.csv'.format(SEQUENCES_DIRECTORY)\n",
    "ONLY_REPRESENTATIVE = '{}/representative.csv'.format(SEQUENCES_DIRECTORY)\n",
    "\n",
    "SEQUENCES_TRAIN = '{}/train.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_VAL = '{}/val.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_TEST = '{}/test.csv'.format(SEQUENCES_DIRECTORY)\n",
    "\n",
    "SEQUENCES_FILES = {\n",
    "    'train': SEQUENCES_TRAIN,\n",
    "    'val': SEQUENCES_VAL,\n",
    "    'test': SEQUENCES_TEST\n",
    "}\n",
    "\n",
    "# Directory to store the fasta files\n",
    "FASTA_DIRECTORY = '{}/fasta'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(FASTA_DIRECTORY)\n",
    "\n",
    "# Directory to store the clustering files\n",
    "CLUSTERING_DIRECTORY = '{}/clustering'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(CLUSTERING_DIRECTORY)\n",
    "\n",
    "CLASSIFICATOR_DIR = '{}/classificator'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(CLASSIFICATOR_DIR)\n",
    "\n",
    "RANDOM_DATASET_DIR = '{}/random'.format(CLASSIFICATOR_DIR)\n",
    "create_dir_if_not_exists(RANDOM_DATASET_DIR)\n",
    "create_train_val_test_dir(RANDOM_DATASET_DIR)\n",
    "\n",
    "GERMLINE_ALL_DIR = '{}/germline_all'.format(CLASSIFICATOR_DIR)\n",
    "create_dir_if_not_exists(GERMLINE_ALL_DIR)\n",
    "create_train_val_test_dir(GERMLINE_ALL_DIR)\n",
    "\n",
    "GERMLINE_V_DIR = '{}/germline_v'.format(CLASSIFICATOR_DIR)\n",
    "create_dir_if_not_exists(GERMLINE_V_DIR)\n",
    "create_train_val_test_dir(GERMLINE_V_DIR)\n",
    "\n",
    "# Directory to store the data for additional tests\n",
    "TEST_DIRECTORY = '{}/test'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(TEST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466061cd-0e53-4835-88a1-1631f4c82da9",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14a96e",
   "metadata": {},
   "source": [
    "### read_raw\n",
    "Process a set of OAS data chunk creating a single csv file.\n",
    "\n",
    "Arguments\n",
    "- location: directory containg the OAS source files\n",
    "- output_location: output file path\n",
    "- subsample: sample a subset of the OAS source files found in location, default None\n",
    "- only_human: focus only on human OAS source files\n",
    "- \n",
    "TODO?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4ad0e7-11df-42b3-b4f7-afd0a9f412c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27807dc0-a6a3-48d1-a45a-7833531ea475",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH_GERMLINE = 'all'\n",
    "STORE_SPECIE = False\n",
    "SUBSAMPLE = None\n",
    "ONLY_HUMAN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b77b75-d4d6-45e4-a050-f0e9da8f781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769f05b1-ed1a-4cc8-afee-311bcbae06fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Found 16 files.\n",
      "human:                 14\n",
      "rat_SD:                 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:24<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the data\n",
      "Initial number of rows: 87116\n",
      "Removed 50301 rows (-57.740%), new number of rows: 36815.\n",
      "Assining ids...\n",
      "Number of unique heavy: 36555\n",
      "Number of unique light: 28518\n",
      "Number of unique pairs:  36813\n",
      "Cleaning the germlines...\n",
      "Saved: ../datasets//sequences/sequences.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(read_raw)\n",
    "    read_raw.read_raw(OAS_FILE_DIR, PAIRED_SEQUENCES_FILE, \n",
    "                      subsample=SUBSAMPLE, only_human=ONLY_HUMAN,\n",
    "                      which_germline=WHICH_GERMLINE, store_specie=STORE_SPECIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0512e8-4d96-47aa-91ed-559984b9249f",
   "metadata": {},
   "source": [
    "### Clusterize the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f123dc84-4114-4abd-8928-4438da19d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a134344-f7ed-4bd3-9bff-3b96dea614eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH = 'both'\n",
    "\n",
    "if WHICH != 'both':\n",
    "    FASTA_SEQUENCES = 'sequences_{}.fasta'.format(WHICH)\n",
    "else:\n",
    "    FASTA_SEQUENCES = 'sequences.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da01a410-9d46-452f-87ec-35e83087a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123a7a37-e13e-4b5b-a567-c35f3ca07545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36814/36814 [00:03<00:00, 9547.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//fasta/sequences.fasta\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(generate_fasta)\n",
    "    generate_fasta.generate_fasta(PAIRED_SEQUENCES_FILE, \n",
    "                                  '{}/{}'.format(FASTA_DIRECTORY, FASTA_SEQUENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5132a43a-77d6-499f-ac4f-4a9810bbb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SEQ_ID = 0.8\n",
    "\n",
    "commands = 'source cluster.sh {} {} {} {}\\n'.format(\n",
    "    DATASET_DIRECTORY, \n",
    "    'fasta/{}'.format(FASTA_SEQUENCES), \n",
    "    'clustering/sequences', \n",
    "    MIN_SEQ_ID\n",
    ")\n",
    "\n",
    "commands += 'rm -rf {}/fasta_files'.format(DATASET_DIRECTORY)\n",
    "\n",
    "with open('clustering_commands.sh', 'w') as f:\n",
    "    f.write(commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9da49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createdb ../datasets//fasta/sequences.fasta ../datasets//DB/DB \n",
      "\n",
      "MMseqs Version:       \t62975ca936b912083c2218e4e30ad962901cbb3b\n",
      "Database type         \t0\n",
      "Shuffle input database\ttrue\n",
      "Createdb mode         \t0\n",
      "Write lookup file     \t1\n",
      "Offset of numeric ids \t0\n",
      "Compressed            \t0\n",
      "Verbosity             \t3\n",
      "\n",
      "Converting sequences\n",
      "[===\n",
      "Time for merging to DB_h: 0h 0m 0s 16ms\n",
      "Time for merging to DB: 0h 0m 0s 26ms\n",
      "Database type: Aminoacid\n",
      "Time for processing: 0h 0m 0s 327ms\n",
      "Create directory ../datasets//cluster/tmp\n",
      "linclust ../datasets//DB/DB ../datasets//cluster/cluster ../datasets//cluster/tmp --min-seq-id 0.8 \n",
      "\n",
      "MMseqs Version:                     \t62975ca936b912083c2218e4e30ad962901cbb3b\n",
      "Cluster mode                        \t0\n",
      "Max connected component depth       \t1000\n",
      "Similarity type                     \t2\n",
      "Threads                             \t4\n",
      "Compressed                          \t0\n",
      "Verbosity                           \t3\n",
      "Weight file name                    \t\n",
      "Cluster Weight threshold            \t0.9\n",
      "Substitution matrix                 \taa:blosum62.out,nucl:nucleotide.out\n",
      "Add backtrace                       \tfalse\n",
      "Alignment mode                      \t2\n",
      "Alignment mode                      \t0\n",
      "Allow wrapped scoring               \tfalse\n",
      "E-value threshold                   \t0.001\n",
      "Seq. id. threshold                  \t0.8\n",
      "Min alignment length                \t0\n",
      "Seq. id. mode                       \t0\n",
      "Alternative alignments              \t0\n",
      "Coverage threshold                  \t0.8\n",
      "Coverage mode                       \t0\n",
      "Max sequence length                 \t65535\n",
      "Compositional bias                  \t1\n",
      "Compositional bias                  \t1\n",
      "Max reject                          \t2147483647\n",
      "Max accept                          \t2147483647\n",
      "Include identical seq. id.          \tfalse\n",
      "Preload mode                        \t0\n",
      "Pseudo count a                      \tsubstitution:1.100,context:1.400\n",
      "Pseudo count b                      \tsubstitution:4.100,context:5.800\n",
      "Score bias                          \t0\n",
      "Realign hits                        \tfalse\n",
      "Realign score bias                  \t-0.2\n",
      "Realign max seqs                    \t2147483647\n",
      "Correlation score weight            \t0\n",
      "Gap open cost                       \taa:11,nucl:5\n",
      "Gap extension cost                  \taa:1,nucl:2\n",
      "Zdrop                               \t40\n",
      "Alphabet size                       \taa:21,nucl:5\n",
      "k-mers per sequence                 \t21\n",
      "Spaced k-mers                       \t0\n",
      "Spaced k-mer pattern                \t\n",
      "Scale k-mers per sequence           \taa:0.000,nucl:0.200\n",
      "Adjust k-mer length                 \tfalse\n",
      "Mask residues                       \t0\n",
      "Mask residues probability           \t0.9\n",
      "Mask lower case residues            \t0\n",
      "k-mer length                        \t0\n",
      "Shift hash                          \t67\n",
      "Split memory limit                  \t0\n",
      "Include only extendable             \tfalse\n",
      "Skip repeating k-mers               \tfalse\n",
      "Rescore mode                        \t0\n",
      "Remove hits by seq. id. and coverage\tfalse\n",
      "Sort results                        \t0\n",
      "Remove temporary files              \tfalse\n",
      "Force restart with latest tmp       \tfalse\n",
      "MPI runner                          \t\n",
      "\n",
      "Set cluster mode SET COVER.\n",
      "kmermatcher ../datasets//DB/DB ../datasets//cluster/tmp/8349843163660208370/pref --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --alph-size aa:13,nucl:5 --min-seq-id 0.8 --kmer-per-seq 21 --spaced-kmer-mode 0 --kmer-per-seq-scale aa:0.000,nucl:0.200 --adjust-kmer-len 0 --mask 0 --mask-prob 0.9 --mask-lower-case 0 --cov-mode 0 -k 0 -c 0.8 --max-seq-len 65535 --hash-shift 67 --split-memory-limit 0 --include-only-extendable 0 --ignore-multi-kmer 0 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "kmermatcher ../datasets//DB/DB ../datasets//cluster/tmp/8349843163660208370/pref --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --alph-size aa:13,nucl:5 --min-seq-id 0.8 --kmer-per-seq 21 --spaced-kmer-mode 0 --kmer-per-seq-scale aa:0.000,nucl:0.200 --adjust-kmer-len 0 --mask 0 --mask-prob 0.9 --mask-lower-case 0 --cov-mode 0 -k 0 -c 0.8 --max-seq-len 65535 --hash-shift 67 --split-memory-limit 0 --include-only-extendable 0 --ignore-multi-kmer 0 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "Database size: 36814 type: Aminoacid\n",
      "Reduced amino acid alphabet: (A S T) (C) (D B N) (E Q Z) (F Y) (G) (H) (I V) (K R) (L J M) (P) (W) (X) \n",
      "\n",
      "Generate k-mers list for 1 split\n",
      "[=================================================================] 36.81K 0s 375ms\n",
      "Sort kmer 0h 0m 0s 152ms\n",
      "Sort by rep. sequence 0h 0m 0s 81ms\n",
      "Time for fill: 0h 0m 0s 31ms\n",
      "Time for merging to pref: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 793ms\n",
      "rescorediagonal ../datasets//DB/DB ../datasets//DB/DB ../datasets//cluster/tmp/8349843163660208370/pref ../datasets//cluster/tmp/8349843163660208370/pref_rescore1 --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --rescore-mode 0 --wrapped-scoring 0 --filter-hits 0 -e 0.001 -c 0.8 -a 0 --cov-mode 0 --min-seq-id 0.8 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --db-load-mode 0 --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "[================================================================] 36.81K 0s 141ms\n",
      "=Time for merging to pref_rescore1: 0h 0m 0s 63ms\n",
      "Time for processing: 0h 0m 0s 318ms\n",
      "clust ../datasets//DB/DB ../datasets//cluster/tmp/8349843163660208370/pref_rescore1 ../datasets//cluster/tmp/8349843163660208370/pre_clust --cluster-mode 0 --max-iterations 1000 --similarity-type 2 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "Clustering mode: Set Cover\n",
      "[=================================================================] 36.81K 0s 13ms\n",
      "Sort entries\n",
      "Find missing connections\n",
      "Found 2364 new connections.\n",
      "Reconstruct initial order\n",
      "[=================================================================] 36.81K 0s 25ms\n",
      "Add missing connections\n",
      "[=================================================================] 36.81K 0s 6ms\n",
      "\n",
      "Time for read in: 0h 0m 0s 64ms\n",
      "Total time: 0h 0m 0s 115ms\n",
      "\n",
      "Size of the sequence database: 36814\n",
      "Size of the alignment database: 36814\n",
      "Number of clusters: 34730\n",
      "\n",
      "Writing results 0h 0m 0s 14ms\n",
      "Time for merging to pre_clust: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 202ms\n",
      "createsubdb ../datasets//cluster/tmp/8349843163660208370/order_redundancy ../datasets//DB/DB ../datasets//cluster/tmp/8349843163660208370/input_step_redundancy -v 3 --subdb-mode 1 \n",
      "\n",
      "Time for merging to input_step_redundancy: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 24ms\n",
      "createsubdb ../datasets//cluster/tmp/8349843163660208370/order_redundancy ../datasets//cluster/tmp/8349843163660208370/pref ../datasets//cluster/tmp/8349843163660208370/pref_filter1 -v 3 --subdb-mode 1 \n",
      "\n",
      "Time for merging to pref_filter1: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 73ms\n",
      "filterdb ../datasets//cluster/tmp/8349843163660208370/pref_filter1 ../datasets//cluster/tmp/8349843163660208370/pref_filter2 --filter-file ../datasets//cluster/tmp/8349843163660208370/order_redundancy --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "Filtering using file(s)\n",
      "[=================================================================] 34.73K 0s 101ms\n",
      "Time for merging to pref_filter2: 0h 0m 0s 16ms\n",
      "Time for processing: 0h 0m 0s 184ms\n",
      "rescorediagonal ../datasets//cluster/tmp/8349843163660208370/input_step_redundancy ../datasets//cluster/tmp/8349843163660208370/input_step_redundancy ../datasets//cluster/tmp/8349843163660208370/pref_filter2 ../datasets//cluster/tmp/8349843163660208370/pref_rescore2 --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --rescore-mode 1 --wrapped-scoring 0 --filter-hits 1 -e 0.001 -c 0.8 -a 0 --cov-mode 0 --min-seq-id 0.8 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --db-load-mode 0 --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "[=================================================================] 34.73K 0s 204ms\n",
      "Time for merging to pref_rescore2: 0h 0m 0s 44ms\n",
      "Time for processing: 0h 0m 0s 289ms\n",
      "align ../datasets//cluster/tmp/8349843163660208370/input_step_redundancy ../datasets//cluster/tmp/8349843163660208370/input_step_redundancy ../datasets//cluster/tmp/8349843163660208370/pref_rescore2 ../datasets//cluster/tmp/8349843163660208370/aln --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' -a 0 --alignment-mode 2 --alignment-output-mode 0 --wrapped-scoring 0 -e 0.001 --min-seq-id 0.8 --min-aln-len 0 --seq-id-mode 0 --alt-ali 0 -c 0.8 --cov-mode 0 --max-seq-len 65535 --comp-bias-corr 1 --comp-bias-corr-scale 1 --max-rejected 2147483647 --max-accept 2147483647 --add-self-matches 0 --db-load-mode 0 --pca substitution:1.100,context:1.400 --pcb substitution:4.100,context:5.800 --score-bias 0 --realign 0 --realign-score-bias -0.2 --realign-max-seqs 2147483647 --corr-score-weight 0 --gap-open aa:11,nucl:5 --gap-extend aa:1,nucl:2 --zdrop 40 --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "Compute score and coverage\n",
      "Query database size: 34730 type: Aminoacid\n",
      "Target database size: 34730 type: Aminoacid\n",
      "Calculation of alignments\n",
      "[=================================================================] 34.73K 21s 882ms\n",
      "Time for merging to aln: 0h 0m 0s 15ms\n",
      "255058 alignments calculated\n",
      "42255 sequence pairs passed the thresholds (0.165668 of overall calculated)\n",
      "1.216671 hits per query sequence\n",
      "Time for processing: 0h 0m 21s 943ms\n",
      "clust ../datasets//cluster/tmp/8349843163660208370/input_step_redundancy ../datasets//cluster/tmp/8349843163660208370/aln ../datasets//cluster/tmp/8349843163660208370/clust --cluster-mode 0 --max-iterations 1000 --similarity-type 2 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "Clustering mode: Set Cover\n",
      "[=================================================================] 34.73K 0s 12ms\n",
      "Sort entries\n",
      "Find missing connections\n",
      "Found 7525 new connections.\n",
      "Reconstruct initial order\n",
      "[=================================================================] 34.73K 0s 16ms\n",
      "Add missing connections\n",
      "[=================================================================] 34.73K 0s 5ms\n",
      "\n",
      "Time for read in: 0h 0m 0s 41ms\n",
      "Total time: 0h 0m 0s 51ms\n",
      "\n",
      "Size of the sequence database: 34730\n",
      "Size of the alignment database: 34730\n",
      "Number of clusters: 29007\n",
      "\n",
      "Writing results 0h 0m 0s 18ms\n",
      "Time for merging to clust: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 86ms\n",
      "mergeclusters ../datasets//DB/DB ../datasets//cluster/cluster ../datasets//cluster/tmp/8349843163660208370/pre_clust ../datasets//cluster/tmp/8349843163660208370/clust --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "Clustering step 1\n",
      "[=================================================================] 34.73K 0s 11ms\n",
      "Clustering step 2\n",
      "[=================================================================] 29.01K 0s 26ms\n",
      "Write merged clustering\n",
      "[=================================================================] 36.81K 0s 40ms\n",
      "Time for merging to cluster: 0h 0m 0s 22ms\n",
      "Time for processing: 0h 0m 0s 118ms\n",
      "createtsv ../datasets//DB/DB ../datasets//DB/DB ../datasets//cluster/cluster ../datasets//clustering/sequences.tsv \n",
      "\n",
      "MMseqs Version:                 \t62975ca936b912083c2218e4e30ad962901cbb3b\n",
      "First sequence as representative\tfalse\n",
      "Target column                   \t1\n",
      "Add full header                 \tfalse\n",
      "Sequence source                 \t0\n",
      "Database output                 \tfalse\n",
      "Threads                         \t4\n",
      "Compressed                      \t0\n",
      "Verbosity                       \t3\n",
      "\n",
      "Time for merging to sequences.tsv: 0h 0m 0s 14ms\n",
      "Time for processing: 0h 0m 0s 73ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('chmod +x clustering_commands.sh')\n",
    "os.system('bash ./clustering_commands.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4888b4-da93-4559-9a9e-0e719d6a0a50",
   "metadata": {},
   "source": [
    "### Get only representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f8bab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6810d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f96b722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting only the representative sequences...\n",
      "Saved: ../datasets//sequences/representative.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_representative)\n",
    "    get_representative.get_representative('{}/sequences.tsv'.format(CLUSTERING_DIRECTORY), \n",
    "                                          PAIRED_SEQUENCES_FILE, ONLY_REPRESENTATIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee16658-1c9e-4720-a947-b5003cdebe19",
   "metadata": {},
   "source": [
    "### Split in train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5937f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c964f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c1d339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start splitting\n",
      "Save training split: ../datasets//sequences/train.csv\n",
      "Save validation split: ../datasets//sequences/val.csv\n",
      "Save test split: ../datasets//sequences/test.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(split)\n",
    "    split.split(ONLY_REPRESENTATIVE, SEQUENCES_TRAIN, SEQUENCES_VAL, SEQUENCES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea3de7-dfcd-45af-ace2-ea68e64abd4c",
   "metadata": {},
   "source": [
    "### Get germline files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b977f641-a749-46b6-b2a7-d88f227f4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_germlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74115ddf-b611-45fb-966c-2fa5f572f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85bcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 17\n",
      "Number of possibile heavy and light combinations: 119\n",
      "Saved: ../datasets//classificator/germline_v/train/train_seq.csv, ../datasets//classificator/germline_v/train/train_germ.csv\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 16\n",
      "Number of possibile heavy and light combinations: 112\n",
      "Saved: ../datasets//classificator/germline_v/val/val_seq.csv, ../datasets//classificator/germline_v/val/val_germ.csv\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 16\n",
      "Number of possibile heavy and light combinations: 112\n",
      "Saved: ../datasets//classificator/germline_v/test/test_seq.csv, ../datasets//classificator/germline_v/test/test_germ.csv\n"
     ]
    }
   ],
   "source": [
    "# Aggiunge il pairing ID\n",
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    for key, value in SEQUENCES_FILES.items():\n",
    "        get_germlines.get_germlines(\n",
    "            value, \n",
    "            '{}/{}/{}_seq.csv'.format(GERMLINE_V_DIR, key, key),\n",
    "            '{}/{}/{}_germ.csv'.format(GERMLINE_V_DIR, key, key),\n",
    "            which='v'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14a3213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 225\n",
      "Number of unique light combinations: 59\n",
      "Number of possibile heavy and light combinations: 13275\n",
      "Saved: ../datasets//classificator/germline_all/train/train_seq.csv, ../datasets//classificator/germline_all/train/train_germ.csv\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 207\n",
      "Number of unique light combinations: 59\n",
      "Number of possibile heavy and light combinations: 12213\n",
      "Saved: ../datasets//classificator/germline_all/val/val_seq.csv, ../datasets//classificator/germline_all/val/val_germ.csv\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 225\n",
      "Number of unique light combinations: 62\n",
      "Number of possibile heavy and light combinations: 13950\n",
      "Saved: ../datasets//classificator/germline_all/test/test_seq.csv, ../datasets//classificator/germline_all/test/test_germ.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    for key, value in SEQUENCES_FILES.items():\n",
    "        get_germlines.get_germlines(\n",
    "            value, \n",
    "            '{}/{}/{}_seq.csv'.format(GERMLINE_ALL_DIR, key, key),\n",
    "            '{}/{}/{}_germ.csv'.format(GERMLINE_ALL_DIR, key, key),\n",
    "            which='all'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb43f7-90eb-4e7b-8db0-1c8e65dce095",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Germline pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b386087-206c-415d-be54-17c18e0f2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import germline_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a10c6e7-9041-41b3-ab40-6ba8aed9e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1000\n",
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee2bc241-03a7-4048-9ee8-cb9d0107aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64748452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10361 out of 13275 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10361/10361 [00:15<00:00, 657.98it/s]\n",
      "100%|██████████| 10361/10361 [00:00<00:00, 168720.55it/s]\n",
      "100%|██████████| 10361/10361 [00:00<00:00, 11796.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5965/5965 [00:02<00:00, 2053.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//classificator/germline_all/train/train_germline_pairing_alpha-1000.csv\n",
      "10033 out of 12213 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10033/10033 [00:15<00:00, 643.01it/s]\n",
      "100%|██████████| 10033/10033 [00:00<00:00, 203128.16it/s]\n",
      "100%|██████████| 10033/10033 [00:01<00:00, 8615.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4069/4069 [00:02<00:00, 1364.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//classificator/germline_all/val/val_germline_pairing_alpha-1000.csv\n",
      "11311 out of 13950 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11311/11311 [00:23<00:00, 488.98it/s]\n",
      "100%|██████████| 11311/11311 [00:00<00:00, 138288.81it/s]\n",
      "100%|██████████| 11311/11311 [00:01<00:00, 7298.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4889/4889 [00:04<00:00, 1048.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//classificator/germline_all/test/test_germline_pairing_alpha-1000.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        germline_pairing.germline_pairing(\n",
    "            '{}/{}/{}_seq.csv'.format(GERMLINE_ALL_DIR, split, split),\n",
    "            '{}/{}/{}_germline_pairing_alpha-{}.csv'.format(GERMLINE_ALL_DIR, split, split, ALPHA),\n",
    "            '{}/{}/{}_germ'.format(GERMLINE_ALL_DIR, split, split),\n",
    "            NUMBER, ALPHA\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88e5c841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 out of 119 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 413.66it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 39670.98it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 1788.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 529.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//classificator/germline_v/train/train_germline_pairing_alpha-1000.csv\n",
      "22 out of 112 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 489.15it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 38161.57it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 2096.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 576.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//classificator/germline_v/val/val_germline_pairing_alpha-1000.csv\n",
      "20 out of 112 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 467.48it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 22121.86it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 2844.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 751.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//classificator/germline_v/test/test_germline_pairing_alpha-1000.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        germline_pairing.germline_pairing(\n",
    "            '{}/{}/{}_seq.csv'.format(GERMLINE_V_DIR, split, split),\n",
    "            '{}/{}/{}_germline_pairing_alpha-{}.csv'.format(GERMLINE_V_DIR, split, split, ALPHA),\n",
    "            '{}/{}/{}_germ'.format(GERMLINE_V_DIR, split, split),\n",
    "            NUMBER, ALPHA\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f6347-7492-4803-ac5a-a6bc6c3a0a07",
   "metadata": {},
   "source": [
    "### Random pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d218d8a-9425-43e0-86bf-2bd4450a3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "241cec3b-91ed-47fb-bd61-845bfd9efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92773ce6-6bdc-4050-b3a1-d767cdf62d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc55cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        random_pairing.random_pairing(\n",
    "            '{}/{}/{}_seq.csv'.format(GERMLINE_V_DIR, split, split),\n",
    "            '{}/{}/{}_germ'.format(GERMLINE_V_DIR, split, split),\n",
    "            NUMBER, ALPHA\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6df8697-9a8f-4956-a2b2-5c121ec7b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 716325 random pairs\n",
      "Saved: ../datasets/new/classificator/train_random/train_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4edc944-e3ac-4a45-aa5e-b3fcd1703211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 271233 random pairs\n",
      "Saved: ../datasets/new/classificator/val_random/val_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32549e2-d65c-4d49-a758-2af41a724926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 369597 random pairs\n",
      "Saved: ../datasets/new/classificator/test_random/test_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d5994-deb3-44ff-a4ef-7ec205be6ad7",
   "metadata": {},
   "source": [
    "### Merge positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random dataset\n",
    "if COMPUTE_NEW:\n",
    "    importlib.reload(merge)\n",
    "    merge.merge('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd52e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germline only V dataset\n",
    "if COMPUTE_NEW:\n",
    "    importlib.reload(merge)\n",
    "    merge.merge('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germline dataset\n",
    "if COMPUTE_NEW:\n",
    "    importlib.reload(merge)\n",
    "    merge.merge('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc790b-5dcc-47f5-a395-70962d024165",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c33034-cc61-4671-a96f-24d9493b2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(SEQUENCES_VAL, index_col=0)\n",
    "light_sequences = test_df[['pair_id', 'light_id', 'light']].drop_duplicates().reset_index(drop=True)\n",
    "light_sequences = light_sequences.sample(len(light_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d98abc-e9cb-47ae-9167-2fb7b57ac218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from Bio import Align\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21272877-e2cf-4b2b-8e01-6559128051e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = light_sequences['light'].drop_duplicates().sample(10000).to_numpy()\n",
    "sim = []\n",
    "for x, y in combinations(ls, 2):\n",
    "    sim.append(Levenshtein.ratio(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c8a96d-e033-443a-be7c-7f9bc62a88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6231716007187188 0.11292343526922217 0.3677130044843049 0.9956331877729258\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sim), np.std(sim), np.min(sim), np.max(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df00a458-d9e5-4fe3-be59-a86e75284256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 271230/271230 [06:35<00:00, 685.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_pairs(light_sequences, threshold, sim_func):\n",
    "    pairs = []\n",
    "    index = 0\n",
    "    for _, r in tqdm(light_sequences.iterrows(), total=len(light_sequences)):\n",
    "        found = False\n",
    "        #print('searching a fella for seq', r['light_id'])\n",
    "        while not found:\n",
    "            sim = sim_func(r['light'], light_sequences.iloc[index, 2])\n",
    "            if sim < threshold:\n",
    "                found = True\n",
    "                pairs.append((r['pair_id'], r['light_id'], light_sequences.iloc[index, 1]))\n",
    "            index += 1\n",
    "            if index == len(light_sequences): \n",
    "                index = 0\n",
    "        #pairs.append((r['light_id'], light_sequences.iloc[index, 0]))\n",
    "    return pairs\n",
    "\n",
    "def create_pairs_random(light_sequences):\n",
    "    pairs = []\n",
    "    sampled = light_sequences.sample(len(light_sequences))\n",
    "    for (_, r1), (_, r2) in zip(light_sequences.iterrows(), sampled.iterrows()):\n",
    "        print(r1, r2)\n",
    "    \n",
    "        \n",
    "        \n",
    "pairs_list = create_pairs(light_sequences, np.mean(sim) - np.std(sim), Levenshtein.ratio)\n",
    "\n",
    "#create_pairs_random(light_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc334ba4-239e-4471-9b68-1db60962ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame({\n",
    "    'pair_id': [x for x, _, _ in pairs_list],\n",
    "    'positive_light': [x for _, x, _ in pairs_list],\n",
    "    'negative_light': [x for _, _, x in pairs_list]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5363e5f-2b37-425a-b1c5-92f136ab6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='positive_light', how='right').rename({\n",
    "    'light_id': 'light_id_pos',\n",
    "    'light': 'light_pos'\n",
    "}, axis=1)[['pair_id', 'light_id_pos', 'light_pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7156f05-0711-4ade-a2f2-4f72bbf9a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='negative_light', how='right').rename({\n",
    "    'light_id': 'light_id_neg',\n",
    "    'light': 'light_neg'\n",
    "}, axis=1)[['pair_id', 'light_id_neg', 'light_neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58567a00-473e-41ed-9e31-c9dead382298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_pos, df_neg)\n",
    "df = test_df[['pair_id', 'heavy_id', 'heavy']].merge(df)\n",
    "df.to_csv('{}/val.csv'.format(TEST_DIRECTORY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
