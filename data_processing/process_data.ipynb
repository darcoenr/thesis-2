{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239fee40-d43d-440a-af55-6876ebdbc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668d2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_if_not_exists(path):\n",
    "    if not pathlib.Path(path).exists():\n",
    "        os.mkdir(path)\n",
    "\n",
    "def create_train_val_test_dir(path):\n",
    "    for dir in ['train', 'val', 'test']:\n",
    "        create_dir_if_not_exists('{}/{}'.format(path, dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f395900-dc42-497b-9cbb-749c2399ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the OAS files\n",
    "OAS_FILE_DIR = r'../../datasets/OAS_sample'\n",
    "\n",
    "DATASET_DIRECTORY = r'../datasets/'\n",
    "create_dir_if_not_exists(DATASET_DIRECTORY)\n",
    "\n",
    "# Directory to store the \"raw\" sequences\n",
    "SEQUENCES_DIRECTORY = '{}/sequences'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(SEQUENCES_DIRECTORY)\n",
    "\n",
    "PAIRED_SEQUENCES_FILE = '{}/sequences.csv'.format(SEQUENCES_DIRECTORY)\n",
    "ONLY_REPRESENTATIVE = '{}/representative.csv'.format(SEQUENCES_DIRECTORY)\n",
    "\n",
    "SEQUENCES_TRAIN = '{}/train.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_VAL = '{}/val.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_TEST = '{}/test.csv'.format(SEQUENCES_DIRECTORY)\n",
    "\n",
    "SEQUENCES_FILES = {\n",
    "    'train': SEQUENCES_TRAIN,\n",
    "    'val': SEQUENCES_VAL,\n",
    "    'test': SEQUENCES_TEST\n",
    "}\n",
    "\n",
    "# Directory to store the fasta files\n",
    "FASTA_DIRECTORY = '{}/fasta'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(FASTA_DIRECTORY)\n",
    "\n",
    "# Directory to store the clustering files\n",
    "CLUSTERING_DIRECTORY = '{}/clustering'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(CLUSTERING_DIRECTORY)\n",
    "\n",
    "CLASSIFICATOR_DIR = '{}/classificator'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(CLASSIFICATOR_DIR)\n",
    "\n",
    "RANDOM_DATASET_DIR = '{}/random'.format(CLASSIFICATOR_DIR)\n",
    "create_dir_if_not_exists(RANDOM_DATASET_DIR)\n",
    "create_train_val_test_dir(RANDOM_DATASET_DIR)\n",
    "\n",
    "GERMLINE_ALL_DIR = '{}/germline_all'.format(CLASSIFICATOR_DIR)\n",
    "create_dir_if_not_exists(GERMLINE_ALL_DIR)\n",
    "create_train_val_test_dir(GERMLINE_ALL_DIR)\n",
    "\n",
    "GERMLINE_V_DIR = '{}/germline_v'.format(CLASSIFICATOR_DIR)\n",
    "create_dir_if_not_exists(GERMLINE_V_DIR)\n",
    "create_train_val_test_dir(GERMLINE_V_DIR)\n",
    "\n",
    "# Directory to store the data for additional tests\n",
    "TEST_DIRECTORY = '{}/test'.format(DATASET_DIRECTORY)\n",
    "create_dir_if_not_exists(TEST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466061cd-0e53-4835-88a1-1631f4c82da9",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4ad0e7-11df-42b3-b4f7-afd0a9f412c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27807dc0-a6a3-48d1-a45a-7833531ea475",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH_GERMLINE = 'all'\n",
    "STORE_SPECIE = False\n",
    "SUBSAMPLE = None\n",
    "ONLY_HUMAN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73b77b75-d4d6-45e4-a050-f0e9da8f781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769f05b1-ed1a-4cc8-afee-311bcbae06fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Found 16 files.\n",
      "human:                 14\n",
      "rat_SD:                 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:18<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the data\n",
      "Initial number of rows: 87116\n",
      "Removed 50301 rows (-57.740%), new number of rows: 36815.\n",
      "Assining ids...\n",
      "Number of unique heavy: 36555\n",
      "Number of unique light: 28518\n",
      "Number of unique pairs:  36813\n",
      "Cleaning the germlines...\n",
      "Saved: ../datasets//sequences/sequences.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(read_raw)\n",
    "    read_raw.read_raw(OAS_FILE_DIR, PAIRED_SEQUENCES_FILE, \n",
    "                      subsample=SUBSAMPLE, only_human=ONLY_HUMAN,\n",
    "                      which_germline=WHICH_GERMLINE, store_specie=STORE_SPECIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0512e8-4d96-47aa-91ed-559984b9249f",
   "metadata": {},
   "source": [
    "### Clusterize the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f123dc84-4114-4abd-8928-4438da19d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a134344-f7ed-4bd3-9bff-3b96dea614eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH = 'both'\n",
    "\n",
    "if WHICH != 'both':\n",
    "    FASTA_SEQUENCES = 'sequences_{}.fasta'.format(WHICH)\n",
    "else:\n",
    "    FASTA_SEQUENCES = 'sequences.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da01a410-9d46-452f-87ec-35e83087a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123a7a37-e13e-4b5b-a567-c35f3ca07545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36814/36814 [00:04<00:00, 8893.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets//fasta/sequences.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(generate_fasta)\n",
    "    generate_fasta.generate_fasta(PAIRED_SEQUENCES_FILE, \n",
    "                                  '{}/{}'.format(FASTA_DIRECTORY, FASTA_SEQUENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5132a43a-77d6-499f-ac4f-4a9810bbb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SEQ_ID = 0.8\n",
    "\n",
    "commands = 'source cluster.sh {} {} {} {}\\n'.format(\n",
    "    DATASET_DIRECTORY, \n",
    "    'fasta/{}'.format(FASTA_SEQUENCES), \n",
    "    'clustering/sequences', \n",
    "    MIN_SEQ_ID\n",
    ")\n",
    "\n",
    "commands += 'rm -rf {}/fasta_files'.format(DATASET_DIRECTORY)\n",
    "\n",
    "with open('clustering_commands.sh', 'w') as f:\n",
    "    f.write(commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9da49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createdb ../datasets//fasta/sequences.fasta ../datasets//DB/DB \n",
      "\n",
      "MMseqs Version:       \t62975ca936b912083c2218e4e30ad962901cbb3b\n",
      "Database type         \t0\n",
      "Shuffle input database\ttrue\n",
      "Createdb mode         \t0\n",
      "Write lookup file     \t1\n",
      "Offset of numeric ids \t0\n",
      "Compressed            \t0\n",
      "Verbosity             \t3\n",
      "\n",
      "Converting sequences\n",
      "[===\n",
      "Time for merging to DB_h: 0h 0m 0s 15ms\n",
      "Time for merging to DB: 0h 0m 0s 23ms\n",
      "Database type: Aminoacid\n",
      "Time for processing: 0h 0m 0s 228ms\n",
      "Create directory ../datasets//cluster/tmp\n",
      "linclust ../datasets//DB/DB ../datasets//cluster/cluster ../datasets//cluster/tmp --min-seq-id 0.8 \n",
      "\n",
      "MMseqs Version:                     \t62975ca936b912083c2218e4e30ad962901cbb3b\n",
      "Cluster mode                        \t0\n",
      "Max connected component depth       \t1000\n",
      "Similarity type                     \t2\n",
      "Threads                             \t4\n",
      "Compressed                          \t0\n",
      "Verbosity                           \t3\n",
      "Weight file name                    \t\n",
      "Cluster Weight threshold            \t0.9\n",
      "Substitution matrix                 \taa:blosum62.out,nucl:nucleotide.out\n",
      "Add backtrace                       \tfalse\n",
      "Alignment mode                      \t2\n",
      "Alignment mode                      \t0\n",
      "Allow wrapped scoring               \tfalse\n",
      "E-value threshold                   \t0.001\n",
      "Seq. id. threshold                  \t0.8\n",
      "Min alignment length                \t0\n",
      "Seq. id. mode                       \t0\n",
      "Alternative alignments              \t0\n",
      "Coverage threshold                  \t0.8\n",
      "Coverage mode                       \t0\n",
      "Max sequence length                 \t65535\n",
      "Compositional bias                  \t1\n",
      "Compositional bias                  \t1\n",
      "Max reject                          \t2147483647\n",
      "Max accept                          \t2147483647\n",
      "Include identical seq. id.          \tfalse\n",
      "Preload mode                        \t0\n",
      "Pseudo count a                      \tsubstitution:1.100,context:1.400\n",
      "Pseudo count b                      \tsubstitution:4.100,context:5.800\n",
      "Score bias                          \t0\n",
      "Realign hits                        \tfalse\n",
      "Realign score bias                  \t-0.2\n",
      "Realign max seqs                    \t2147483647\n",
      "Correlation score weight            \t0\n",
      "Gap open cost                       \taa:11,nucl:5\n",
      "Gap extension cost                  \taa:1,nucl:2\n",
      "Zdrop                               \t40\n",
      "Alphabet size                       \taa:21,nucl:5\n",
      "k-mers per sequence                 \t21\n",
      "Spaced k-mers                       \t0\n",
      "Spaced k-mer pattern                \t\n",
      "Scale k-mers per sequence           \taa:0.000,nucl:0.200\n",
      "Adjust k-mer length                 \tfalse\n",
      "Mask residues                       \t0\n",
      "Mask residues probability           \t0.9\n",
      "Mask lower case residues            \t0\n",
      "k-mer length                        \t0\n",
      "Shift hash                          \t67\n",
      "Split memory limit                  \t0\n",
      "Include only extendable             \tfalse\n",
      "Skip repeating k-mers               \tfalse\n",
      "Rescore mode                        \t0\n",
      "Remove hits by seq. id. and coverage\tfalse\n",
      "Sort results                        \t0\n",
      "Remove temporary files              \tfalse\n",
      "Force restart with latest tmp       \tfalse\n",
      "MPI runner                          \t\n",
      "\n",
      "Set cluster mode SET COVER.\n",
      "kmermatcher ../datasets//DB/DB ../datasets//cluster/tmp/16952373815507216587/pref --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --alph-size aa:13,nucl:5 --min-seq-id 0.8 --kmer-per-seq 21 --spaced-kmer-mode 0 --kmer-per-seq-scale aa:0.000,nucl:0.200 --adjust-kmer-len 0 --mask 0 --mask-prob 0.9 --mask-lower-case 0 --cov-mode 0 -k 0 -c 0.8 --max-seq-len 65535 --hash-shift 67 --split-memory-limit 0 --include-only-extendable 0 --ignore-multi-kmer 0 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "kmermatcher ../datasets//DB/DB ../datasets//cluster/tmp/16952373815507216587/pref --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --alph-size aa:13,nucl:5 --min-seq-id 0.8 --kmer-per-seq 21 --spaced-kmer-mode 0 --kmer-per-seq-scale aa:0.000,nucl:0.200 --adjust-kmer-len 0 --mask 0 --mask-prob 0.9 --mask-lower-case 0 --cov-mode 0 -k 0 -c 0.8 --max-seq-len 65535 --hash-shift 67 --split-memory-limit 0 --include-only-extendable 0 --ignore-multi-kmer 0 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "Database size: 36814 type: Aminoacid\n",
      "Reduced amino acid alphabet: (A S T) (C) (D B N) (E Q Z) (F Y) (G) (H) (I V) (K R) (L J M) (P) (W) (X) \n",
      "\n",
      "Generate k-mers list for 1 split\n",
      "[=================================================================] 36.81K 0s 503ms\n",
      "Sort kmer 0h 0m 0s 153ms\n",
      "Sort by rep. sequence 0h 0m 0s 67ms\n",
      "Time for fill: 0h 0m 0s 28ms\n",
      "Time for merging to pref: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 850ms\n",
      "rescorediagonal ../datasets//DB/DB ../datasets//DB/DB ../datasets//cluster/tmp/16952373815507216587/pref ../datasets//cluster/tmp/16952373815507216587/pref_rescore1 --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --rescore-mode 0 --wrapped-scoring 0 --filter-hits 0 -e 0.001 -c 0.8 -a 0 --cov-mode 0 --min-seq-id 0.8 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --db-load-mode 0 --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "[================================================================] 36.81K 0s 105ms\n",
      "=Time for merging to pref_rescore1: 0h 0m 0s 16ms\n",
      "Time for processing: 0h 0m 0s 156ms\n",
      "clust ../datasets//DB/DB ../datasets//cluster/tmp/16952373815507216587/pref_rescore1 ../datasets//cluster/tmp/16952373815507216587/pre_clust --cluster-mode 0 --max-iterations 1000 --similarity-type 2 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "Clustering mode: Set Cover\n",
      "[=================================================================] 36.81K 0s 12ms\n",
      "Sort entries\n",
      "Find missing connections\n",
      "Found 2364 new connections.\n",
      "Reconstruct initial order\n",
      "[=================================================================] 36.81K 0s 17ms\n",
      "Add missing connections\n",
      "[=================================================================] 36.81K 0s 3ms\n",
      "\n",
      "Time for read in: 0h 0m 0s 42ms\n",
      "Total time: 0h 0m 0s 48ms\n",
      "\n",
      "Size of the sequence database: 36814\n",
      "Size of the alignment database: 36814\n",
      "Number of clusters: 34730\n",
      "\n",
      "Writing results 0h 0m 0s 12ms\n",
      "Time for merging to pre_clust: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 81ms\n",
      "createsubdb ../datasets//cluster/tmp/16952373815507216587/order_redundancy ../datasets//DB/DB ../datasets//cluster/tmp/16952373815507216587/input_step_redundancy -v 3 --subdb-mode 1 \n",
      "\n",
      "Time for merging to input_step_redundancy: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 15ms\n",
      "createsubdb ../datasets//cluster/tmp/16952373815507216587/order_redundancy ../datasets//cluster/tmp/16952373815507216587/pref ../datasets//cluster/tmp/16952373815507216587/pref_filter1 -v 3 --subdb-mode 1 \n",
      "\n",
      "Time for merging to pref_filter1: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 19ms\n",
      "filterdb ../datasets//cluster/tmp/16952373815507216587/pref_filter1 ../datasets//cluster/tmp/16952373815507216587/pref_filter2 --filter-file ../datasets//cluster/tmp/16952373815507216587/order_redundancy --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "Filtering using file(s)\n",
      "[=================================================================] 34.73K 0s 110ms\n",
      "Time for merging to pref_filter2: 0h 0m 0s 18ms\n",
      "Time for processing: 0h 0m 0s 171ms\n",
      "rescorediagonal ../datasets//cluster/tmp/16952373815507216587/input_step_redundancy ../datasets//cluster/tmp/16952373815507216587/input_step_redundancy ../datasets//cluster/tmp/16952373815507216587/pref_filter2 ../datasets//cluster/tmp/16952373815507216587/pref_rescore2 --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --rescore-mode 1 --wrapped-scoring 0 --filter-hits 1 -e 0.001 -c 0.8 -a 0 --cov-mode 0 --min-seq-id 0.8 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --db-load-mode 0 --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "[=================================================================] 34.73K 0s 220ms\n",
      "Time for merging to pref_rescore2: 0h 0m 0s 30ms\n",
      "Time for processing: 0h 0m 0s 300ms\n",
      "align ../datasets//cluster/tmp/16952373815507216587/input_step_redundancy ../datasets//cluster/tmp/16952373815507216587/input_step_redundancy ../datasets//cluster/tmp/16952373815507216587/pref_rescore2 ../datasets//cluster/tmp/16952373815507216587/aln --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' -a 0 --alignment-mode 2 --alignment-output-mode 0 --wrapped-scoring 0 -e 0.001 --min-seq-id 0.8 --min-aln-len 0 --seq-id-mode 0 --alt-ali 0 -c 0.8 --cov-mode 0 --max-seq-len 65535 --comp-bias-corr 1 --comp-bias-corr-scale 1 --max-rejected 2147483647 --max-accept 2147483647 --add-self-matches 0 --db-load-mode 0 --pca substitution:1.100,context:1.400 --pcb substitution:4.100,context:5.800 --score-bias 0 --realign 0 --realign-score-bias -0.2 --realign-max-seqs 2147483647 --corr-score-weight 0 --gap-open aa:11,nucl:5 --gap-extend aa:1,nucl:2 --zdrop 40 --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "Compute score and coverage\n",
      "Query database size: 34730 type: Aminoacid\n",
      "Target database size: 34730 type: Aminoacid\n",
      "Calculation of alignments\n",
      "[=================================================================] 34.73K 22s 690ms\n",
      "Time for merging to aln: 0h 0m 0s 18ms\n",
      "255058 alignments calculated\n",
      "42255 sequence pairs passed the thresholds (0.165668 of overall calculated)\n",
      "1.216671 hits per query sequence\n",
      "Time for processing: 0h 0m 22s 815ms\n",
      "clust ../datasets//cluster/tmp/16952373815507216587/input_step_redundancy ../datasets//cluster/tmp/16952373815507216587/aln ../datasets//cluster/tmp/16952373815507216587/clust --cluster-mode 0 --max-iterations 1000 --similarity-type 2 --threads 4 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
      "\n",
      "Clustering mode: Set Cover\n",
      "[=================================================================] 34.73K 0s 14ms\n",
      "Sort entries\n",
      "Find missing connections\n",
      "Found 7525 new connections.\n",
      "Reconstruct initial order\n",
      "[=================================================================] 34.73K 0s 18ms\n",
      "Add missing connections\n",
      "[=================================================================] 34.73K 0s 2ms\n",
      "\n",
      "Time for read in: 0h 0m 0s 45ms\n",
      "Total time: 0h 0m 0s 53ms\n",
      "\n",
      "Size of the sequence database: 34730\n",
      "Size of the alignment database: 34730\n",
      "Number of clusters: 29007\n",
      "\n",
      "Writing results 0h 0m 0s 14ms\n",
      "Time for merging to clust: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 82ms\n",
      "mergeclusters ../datasets//DB/DB ../datasets//cluster/cluster ../datasets//cluster/tmp/16952373815507216587/pre_clust ../datasets//cluster/tmp/16952373815507216587/clust --threads 4 --compressed 0 -v 3 \n",
      "\n",
      "Clustering step 1\n",
      "[=================================================================] 34.73K 0s 10ms\n",
      "Clustering step 2\n",
      "[=================================================================] 29.01K 0s 21ms\n",
      "Write merged clustering\n",
      "[=================================================================] 36.81K 0s 35ms\n",
      "Time for merging to cluster: 0h 0m 0s 17ms\n",
      "Time for processing: 0h 0m 0s 72ms\n",
      "createtsv ../datasets//DB/DB ../datasets//DB/DB ../datasets//cluster/cluster ../datasets//clustering/sequences.tsv \n",
      "\n",
      "MMseqs Version:                 \t62975ca936b912083c2218e4e30ad962901cbb3b\n",
      "First sequence as representative\tfalse\n",
      "Target column                   \t1\n",
      "Add full header                 \tfalse\n",
      "Sequence source                 \t0\n",
      "Database output                 \tfalse\n",
      "Threads                         \t4\n",
      "Compressed                      \t0\n",
      "Verbosity                       \t3\n",
      "\n",
      "Time for merging to sequences.tsv: 0h 0m 0s 24ms\n",
      "Time for processing: 0h 0m 0s 79ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('chmod +x clustering_commands.sh')\n",
    "os.system('bash ./clustering_commands.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4888b4-da93-4559-9a9e-0e719d6a0a50",
   "metadata": {},
   "source": [
    "### Get only representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8bab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6810d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96b722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting only the representative sequences...\n",
      "Saved: ../datasets//sequences/representative.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_representative)\n",
    "    get_representative.get_representative('{}/sequences.tsv'.format(CLUSTERING_DIRECTORY), \n",
    "                                          PAIRED_SEQUENCES_FILE, ONLY_REPRESENTATIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee16658-1c9e-4720-a947-b5003cdebe19",
   "metadata": {},
   "source": [
    "### Split in train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5937f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c964f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c1d339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start splitting\n",
      "Save training split: ../datasets//sequences/train.csv\n",
      "Save validation split: ../datasets//sequences/val.csv\n",
      "Save test split: ../datasets//sequences/test.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(split)\n",
    "    split.split(ONLY_REPRESENTATIVE, SEQUENCES_TRAIN, SEQUENCES_VAL, SEQUENCES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea3de7-dfcd-45af-ace2-ea68e64abd4c",
   "metadata": {},
   "source": [
    "### Get germline files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b977f641-a749-46b6-b2a7-d88f227f4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_germlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74115ddf-b611-45fb-966c-2fa5f572f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b85bcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 16\n",
      "Number of possibile heavy and light combinations: 112\n",
      "Saved: ../datasets//classificator/germline_v/train/train_seq_only_v, ../datasets//classificator/germline_v/train/train_germ_only_v\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 16\n",
      "Number of possibile heavy and light combinations: 112\n",
      "Saved: ../datasets//classificator/germline_v/val/val_seq_only_v, ../datasets//classificator/germline_v/val/val_germ_only_v\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 17\n",
      "Number of possibile heavy and light combinations: 119\n",
      "Saved: ../datasets//classificator/germline_v/test/test_seq_only_v, ../datasets//classificator/germline_v/test/test_germ_only_v\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    for key, value in SEQUENCES_FILES.items():\n",
    "        get_germlines.get_germlines(\n",
    "            value, \n",
    "            '{}/{}/{}_seq_only_v'.format(GERMLINE_V_DIR, key, key),\n",
    "            '{}/{}/{}_germ_only_v'.format(GERMLINE_V_DIR, key, key),\n",
    "            which='v'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14a3213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 220\n",
      "Number of unique light combinations: 59\n",
      "Number of possibile heavy and light combinations: 12980\n",
      "Saved: ../datasets//classificator/germline_all/train/train_seq_all, ../datasets//classificator/germline_all/train/train_germ_all\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 205\n",
      "Number of unique light combinations: 57\n",
      "Number of possibile heavy and light combinations: 11685\n",
      "Saved: ../datasets//classificator/germline_all/val/val_seq_all, ../datasets//classificator/germline_all/val/val_germ_all\n",
      "Get germlines id...\n",
      "Number of unique heavy combinations: 235\n",
      "Number of unique light combinations: 62\n",
      "Number of possibile heavy and light combinations: 14570\n",
      "Saved: ../datasets//classificator/germline_all/test/test_seq_all, ../datasets//classificator/germline_all/test/test_germ_all\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    for key, value in SEQUENCES_FILES.items():\n",
    "        get_germlines.get_germlines(\n",
    "            value, \n",
    "            '{}/{}/{}_seq_all'.format(GERMLINE_ALL_DIR, key, key),\n",
    "            '{}/{}/{}_germ_all'.format(GERMLINE_ALL_DIR, key, key),\n",
    "            which='all'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb43f7-90eb-4e7b-8db0-1c8e65dce095",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Germline pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b386087-206c-415d-be54-17c18e0f2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import germline_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a10c6e7-9041-41b3-ab40-6ba8aed9e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1000\n",
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee2bc241-03a7-4048-9ee8-cb9d0107aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ac946a2-69ac-47cc-96f9-2ee44df35ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 out of 126 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 13/13 [00:00<00:00, 1124.04it/s]\n",
      "100%|████████████████████████████████████████| 13/13 [00:00<00:00, 55809.57it/s]\n",
      "100%|██████████████████████████████████████████| 13/13 [00:00<00:00, 141.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 30.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/train/train-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/train_seq_only_v.csv'.format(TRAIN_DIR), \n",
    "                                      '{}/train-germline_pairing-alpha_{}_only_v.csv'.format(TRAIN_DIR, ALPHA),\n",
    "                                      '{}/train_germ_only_v.csv'.format(TRAIN_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7a47983-6123-4f93-9b12-74600c27b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 out of 119 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 1049.44it/s]\n",
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 34419.85it/s]\n",
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 198.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 46.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/val/val-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/val_seq_only_v.csv'.format(VAL_DIR), \n",
    "                                      '{}/val-germline_pairing-alpha_{}_only_v.csv'.format(VAL_DIR, ALPHA),\n",
    "                                      '{}/val_germ_only_v.csv'.format(VAL_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "288242e5-880c-4e06-837d-6d684dd0e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 out of 119 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 1008.63it/s]\n",
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 37211.82it/s]\n",
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 145.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 33.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/test/test-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/test_seq_only_v.csv'.format(TEST_DIR), \n",
    "                                      '{}/test-germline_pairing-alpha_{}_only_v.csv'.format(TEST_DIR, ALPHA),\n",
    "                                      '{}/test_germ_only_v.csv'.format(TEST_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f6347-7492-4803-ac5a-a6bc6c3a0a07",
   "metadata": {},
   "source": [
    "### Random pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d218d8a-9425-43e0-86bf-2bd4450a3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241cec3b-91ed-47fb-bd61-845bfd9efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92773ce6-6bdc-4050-b3a1-d767cdf62d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6df8697-9a8f-4956-a2b2-5c121ec7b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 716325 random pairs\n",
      "Saved: ../datasets/new/classificator/train_random/train_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4edc944-e3ac-4a45-aa5e-b3fcd1703211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 271233 random pairs\n",
      "Saved: ../datasets/new/classificator/val_random/val_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32549e2-d65c-4d49-a758-2af41a724926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 369597 random pairs\n",
      "Saved: ../datasets/new/classificator/test_random/test_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d5994-deb3-44ff-a4ef-7ec205be6ad7",
   "metadata": {},
   "source": [
    "### Merge positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random dataset\n",
    "if COMPUTE_NEW:\n",
    "    importlib.reload(merge)\n",
    "    merge.merge('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd52e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germline only V dataset\n",
    "if COMPUTE_NEW:\n",
    "    importlib.reload(merge)\n",
    "    merge.merge('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germline dataset\n",
    "if COMPUTE_NEW:\n",
    "    importlib.reload(merge)\n",
    "    merge.merge('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/train_random/train.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/val_random/val.csv'.format(DATASET_DIRECTORY))\n",
    "    merge.merge('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY),\n",
    "                '{}/classificator/test_random/test.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc790b-5dcc-47f5-a395-70962d024165",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c33034-cc61-4671-a96f-24d9493b2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(SEQUENCES_VAL, index_col=0)\n",
    "light_sequences = test_df[['pair_id', 'light_id', 'light']].drop_duplicates().reset_index(drop=True)\n",
    "light_sequences = light_sequences.sample(len(light_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d98abc-e9cb-47ae-9167-2fb7b57ac218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from Bio import Align\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21272877-e2cf-4b2b-8e01-6559128051e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = light_sequences['light'].drop_duplicates().sample(10000).to_numpy()\n",
    "sim = []\n",
    "for x, y in combinations(ls, 2):\n",
    "    sim.append(Levenshtein.ratio(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c8a96d-e033-443a-be7c-7f9bc62a88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6231716007187188 0.11292343526922217 0.3677130044843049 0.9956331877729258\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sim), np.std(sim), np.min(sim), np.max(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df00a458-d9e5-4fe3-be59-a86e75284256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 271230/271230 [06:35<00:00, 685.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_pairs(light_sequences, threshold, sim_func):\n",
    "    pairs = []\n",
    "    index = 0\n",
    "    for _, r in tqdm(light_sequences.iterrows(), total=len(light_sequences)):\n",
    "        found = False\n",
    "        #print('searching a fella for seq', r['light_id'])\n",
    "        while not found:\n",
    "            sim = sim_func(r['light'], light_sequences.iloc[index, 2])\n",
    "            if sim < threshold:\n",
    "                found = True\n",
    "                pairs.append((r['pair_id'], r['light_id'], light_sequences.iloc[index, 1]))\n",
    "            index += 1\n",
    "            if index == len(light_sequences): \n",
    "                index = 0\n",
    "        #pairs.append((r['light_id'], light_sequences.iloc[index, 0]))\n",
    "    return pairs\n",
    "\n",
    "def create_pairs_random(light_sequences):\n",
    "    pairs = []\n",
    "    sampled = light_sequences.sample(len(light_sequences))\n",
    "    for (_, r1), (_, r2) in zip(light_sequences.iterrows(), sampled.iterrows()):\n",
    "        print(r1, r2)\n",
    "    \n",
    "        \n",
    "        \n",
    "pairs_list = create_pairs(light_sequences, np.mean(sim) - np.std(sim), Levenshtein.ratio)\n",
    "\n",
    "#create_pairs_random(light_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc334ba4-239e-4471-9b68-1db60962ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame({\n",
    "    'pair_id': [x for x, _, _ in pairs_list],\n",
    "    'positive_light': [x for _, x, _ in pairs_list],\n",
    "    'negative_light': [x for _, _, x in pairs_list]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5363e5f-2b37-425a-b1c5-92f136ab6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='positive_light', how='right').rename({\n",
    "    'light_id': 'light_id_pos',\n",
    "    'light': 'light_pos'\n",
    "}, axis=1)[['pair_id', 'light_id_pos', 'light_pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7156f05-0711-4ade-a2f2-4f72bbf9a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='negative_light', how='right').rename({\n",
    "    'light_id': 'light_id_neg',\n",
    "    'light': 'light_neg'\n",
    "}, axis=1)[['pair_id', 'light_id_neg', 'light_neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58567a00-473e-41ed-9e31-c9dead382298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_pos, df_neg)\n",
    "df = test_df[['pair_id', 'heavy_id', 'heavy']].merge(df)\n",
    "df.to_csv('{}/val.csv'.format(TEST_DIRECTORY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
