{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239fee40-d43d-440a-af55-6876ebdbc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f395900-dc42-497b-9cbb-749c2399ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OAS_FILE_DIR = r'../../datasets/OAS'\n",
    "DATASET_DIRECTORY = '../datasets/new'\n",
    "\n",
    "# Directory to store the \"raw\" sequences\n",
    "SEQUENCES_DIRECTORY = '{}/sequences'.format(DATASET_DIRECTORY)\n",
    "\n",
    "if not pathlib.Path(SEQUENCES_DIRECTORY).exists():\n",
    "    os.mkdir(SEQUENCES_DIRECTORY)\n",
    "\n",
    "PAIRED_SEQUENCES_FILE = '{}/sequences.csv'.format(SEQUENCES_DIRECTORY)\n",
    "ONLY_REPRESENTATIVE = '{}/representative.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_TRAIN = '{}/train.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_VAL = '{}/val.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_TEST = '{}/test.csv'.format(SEQUENCES_DIRECTORY)\n",
    "\n",
    "# Directory to store the fasta files\n",
    "FASTA_DIRECTORY = '{}/fasta'.format(DATASET_DIRECTORY)\n",
    "if not pathlib.Path(FASTA_DIRECTORY).exists():\n",
    "    os.mkdir(FASTA_DIRECTORY)\n",
    "\n",
    "# Directory to store the clustering files\n",
    "CLUSTERING_DIRECTORY = '{}/clustering'.format(DATASET_DIRECTORY)\n",
    "if not pathlib.Path(CLUSTERING_DIRECTORY).exists():\n",
    "    os.makedirs(CLUSTERING_DIRECTORY)\n",
    "\n",
    "\n",
    "CLASSIFICATOR_DIR = '{}/classificator'.format(DATASET_DIRECTORY)\n",
    "if not pathlib.Path(CLASSIFICATOR_DIR).exists():\n",
    "    os.makedirs(CLASSIFICATOR_DIR)\n",
    "\n",
    "TRAIN_DIR = '{}/train'.format(CLASSIFICATOR_DIR)\n",
    "if not pathlib.Path(TRAIN_DIR).exists():\n",
    "    os.makedirs(TRAIN_DIR)\n",
    "\n",
    "VAL_DIR = '{}/val'.format(CLASSIFICATOR_DIR)\n",
    "if not pathlib.Path(VAL_DIR).exists():\n",
    "    os.makedirs(VAL_DIR)\n",
    "\n",
    "TEST_DIR = '{}/test'.format(CLASSIFICATOR_DIR)\n",
    "if not pathlib.Path(TEST_DIR).exists():\n",
    "    os.makedirs(TEST_DIR)\n",
    "\n",
    "\n",
    "TEST_DIRECTORY = '{}/test'.format(DATASET_DIRECTORY)\n",
    "\n",
    "if not pathlib.Path(TEST_DIRECTORY).exists():\n",
    "    os.makedirs(TEST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466061cd-0e53-4835-88a1-1631f4c82da9",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4ad0e7-11df-42b3-b4f7-afd0a9f412c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27807dc0-a6a3-48d1-a45a-7833531ea475",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH_GERMLINE = 'v'\n",
    "STORE_SPECIE = False\n",
    "SUBSAMPLE = None\n",
    "ONLY_HUMAN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b77b75-d4d6-45e4-a050-f0e9da8f781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769f05b1-ed1a-4cc8-afee-311bcbae06fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Found 310 files.\n",
      "human:                280\n",
      "mouse_C57BL/6:          2\n",
      "mouse_BALB/c:           8\n",
      "rat_SD:                20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 280/280 [04:47<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the data\n",
      "Initial number of rows: 1954079\n",
      "Removed 279826 rows (-14.320%), new number of rows: 1674253.\n",
      "Assining ids...\n",
      "Number of unique heavy: 1654917\n",
      "Number of unique light: 724832\n",
      "Number of unique pairs:  1674177\n",
      "Cleaning the germlines...\n",
      "Saved: ../datasets/new_only_v/sequences/sequences.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(read_raw)\n",
    "    read_raw.read_raw(OAS_FILE_DIR, PAIRED_SEQUENCES_FILE, \n",
    "                      subsample=SUBSAMPLE, only_human=ONLY_HUMAN,\n",
    "                      which_germline=WHICH_GERMLINE, store_specie=STORE_SPECIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0512e8-4d96-47aa-91ed-559984b9249f",
   "metadata": {},
   "source": [
    "### Clusterize the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f123dc84-4114-4abd-8928-4438da19d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a134344-f7ed-4bd3-9bff-3b96dea614eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH = 'both'\n",
    "\n",
    "if WHICH != 'both':\n",
    "    FASTA_SEQUENCES = 'sequences_{}.fasta'.format(WHICH)\n",
    "else:\n",
    "    FASTA_SEQUENCES = 'sequences.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da01a410-9d46-452f-87ec-35e83087a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "123a7a37-e13e-4b5b-a567-c35f3ca07545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1674177/1674177 [01:13<00:00, 22781.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new_only_v/fasta/sequences.fasta\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(generate_fasta)\n",
    "    generate_fasta.generate_fasta(PAIRED_SEQUENCES_FILE, \n",
    "                                  '{}/{}'.format(FASTA_DIRECTORY, FASTA_SEQUENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5132a43a-77d6-499f-ac4f-4a9810bbb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SEQ_ID = 0.8\n",
    "\n",
    "commands = 'source cluster.sh {} {} {} {}\\n'.format(\n",
    "    DATASET_DIRECTORY, \n",
    "    'fasta/{}'.format(FASTA_SEQUENCES), \n",
    "    'clustering/sequences', \n",
    "    MIN_SEQ_ID\n",
    ")\n",
    "\n",
    "commands += 'rm -rf {}/fasta_files'.format(DATASET_DIRECTORY)\n",
    "\n",
    "with open('clustering_commands.sh', 'w') as f:\n",
    "    f.write(commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4888b4-da93-4559-9a9e-0e719d6a0a50",
   "metadata": {},
   "source": [
    "### Get only representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06caa02d-3015-4b75-b533-d9f480468beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_csv('{}/sequences.tsv'.format(CLUSTERING_DIRECTORY), sep='\\t', header=None).rename(\n",
    "    {\n",
    "        0: 'representative',\n",
    "        1: 'sequences'\n",
    "    },\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c8e05b-53df-49be-aca2-d079e75cb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = {\n",
    "    'representative': [],\n",
    "    'size': []\n",
    "}\n",
    "\n",
    "for seq, data in clusters.groupby('representative'):\n",
    "    cluster_sizes['representative'].append(seq)\n",
    "    cluster_sizes['size'].append(len(data))\n",
    "\n",
    "cluster_sizes = pd.DataFrame(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96334e92-fa3d-4c4c-883a-3ab83c8d07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota! Ci sono delle sequenze uguali ma con germline diverse\n",
    "\n",
    "sequences = pd.read_csv(PAIRED_SEQUENCES_FILE, index_col=0)\n",
    "representative = clusters['representative'].drop_duplicates()\n",
    "representative = sequences.merge(representative, left_on='pair_id', right_on='representative', how='right')[sequences.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01ce13b6-bdea-4709-90d1-1b4cb1589621",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative.to_csv(ONLY_REPRESENTATIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee16658-1c9e-4720-a947-b5003cdebe19",
   "metadata": {},
   "source": [
    "### Split in train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fafe30d-1e0a-47fe-9cde-8f197ac45b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_germline_data(df):\n",
    "    from itertools import product\n",
    "    \n",
    "    germlines_dict = {'heavy_germline': [], 'light_germline': [], 'counter': []}\n",
    "\n",
    "    for heavy_germline, data in df.groupby('heavy_germline'):\n",
    "        for light_germline, data2 in data.groupby('light_germline'):\n",
    "            germlines_dict['heavy_germline'].append(heavy_germline)\n",
    "            germlines_dict['light_germline'].append(light_germline)\n",
    "            germlines_dict['counter'].append(len(data2))\n",
    "\n",
    "    germlines = pd.DataFrame(germlines_dict)\n",
    "\n",
    "    all_germline_pairs = set(product(germlines['heavy_germline'].unique(), germlines['light_germline'].unique()))\n",
    "    found_germline_pairs = set([(row['heavy_germline'], row['light_germline']) for _, row in germlines.iterrows()])\n",
    "    not_found_germline_pairs = all_germline_pairs.difference(found_germline_pairs)\n",
    "\n",
    "    zero_germlines = {'heavy_germline': [], 'light_germline': [], 'counter': []}\n",
    "\n",
    "    for h, l in not_found_germline_pairs:\n",
    "        zero_germlines['heavy_germline'].append(h)\n",
    "        zero_germlines['light_germline'].append(l)\n",
    "        zero_germlines['counter'].append(0)\n",
    "\n",
    "    germlines = pd.concat([germlines, pd.DataFrame(zero_germlines)], axis=0).sort_values(by='counter', ascending=False)\n",
    "\n",
    "    return germlines\n",
    "\n",
    "def split(germlines, sequences):\n",
    "    # Select some heavy germlines exclusive to dataset 2\n",
    "    heavy_germlines_dict = {'germline': [], 'counter': []}\n",
    "    for g, data in germlines.groupby('heavy_germline'):\n",
    "        heavy_germlines_dict['germline'].append(g)\n",
    "        heavy_germlines_dict['counter'].append(data.sum()['counter'])\n",
    "    heavy_germlines = pd.DataFrame(heavy_germlines_dict)\n",
    "\n",
    "    heavy_germlines = heavy_germlines.sort_values(by='counter', ascending=False)\n",
    "\n",
    "    lower = heavy_germlines['counter'].quantile(0.20)\n",
    "    higher = heavy_germlines['counter'].quantile(0.80)\n",
    "\n",
    "    selected_heavy_germlines = heavy_germlines[(heavy_germlines['counter'] > lower) & (heavy_germlines['counter'] < higher)]\n",
    "    selected_heavy_germlines = selected_heavy_germlines.sample(int(len(selected_heavy_germlines)*0.1))\n",
    "\n",
    "    # Select some light germlines exlcusive to dataset 2\n",
    "    light_germlines_dict = {'germline': [], 'counter': []}\n",
    "    for g, data in germlines.groupby('light_germline'):\n",
    "        light_germlines_dict['germline'].append(g)\n",
    "        light_germlines_dict['counter'].append(data.sum()['counter'])\n",
    "    light_germlines = pd.DataFrame(light_germlines_dict)\n",
    "\n",
    "    light_germlines = light_germlines.sort_values(by='counter', ascending=False)\n",
    "\n",
    "    lower = light_germlines['counter'].quantile(0.20)\n",
    "    higher = light_germlines['counter'].quantile(0.80)\n",
    "\n",
    "    selected_light_germlines = light_germlines[(light_germlines['counter'] > lower) & (light_germlines['counter'] < higher)]\n",
    "    selected_light_germlines = selected_light_germlines.sample(int(len(selected_light_germlines)*0.1))\n",
    "\n",
    "    merged = sequences.merge(selected_heavy_germlines, left_on='heavy_germline', right_on='germline', how='left', indicator=True)\n",
    "    merged = merged[merged['_merge'] == 'left_only'][sequences.columns]\n",
    "    merged = merged.merge(selected_light_germlines, left_on='light_germline', right_on='germline', how='left', indicator=True)\n",
    "    merged = merged[merged['_merge'] == 'left_only'][sequences.columns]\n",
    "    to_split = merged\n",
    "\n",
    "    df1, df2 = train_test_split(to_split, test_size=0.25, random_state=1234567890)\n",
    "\n",
    "    df2 = pd.concat([\n",
    "        df2, \n",
    "        pd.merge(sequences, selected_heavy_germlines['germline'], \n",
    "                 left_on='heavy_germline', right_on='germline')[sequences.columns],\n",
    "        pd.merge(sequences, selected_light_germlines['germline'], \n",
    "                 left_on='light_germline', right_on='germline')[sequences.columns]\n",
    "    ])\n",
    "\n",
    "    df2 = df2.drop_duplicates()\n",
    "\n",
    "    return df1, df2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9737fab4-5a6b-41cb-88a5-cb2e0d4b06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative = pd.read_csv(ONLY_REPRESENTATIVE, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9761e055-749c-42d5-b2ef-2452b1c545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "germlines_all = compute_germline_data(representative)\n",
    "trainval, test = split(germlines_all, representative)\n",
    "test.to_csv(SEQUENCES_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7edd197-8983-4b5a-95a1-820e0a392426",
   "metadata": {},
   "outputs": [],
   "source": [
    "germlines_trainval = compute_germline_data(trainval)\n",
    "train, val = split(germlines_trainval, trainval)\n",
    "train.to_csv(SEQUENCES_TRAIN)\n",
    "val.to_csv(SEQUENCES_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea3de7-dfcd-45af-ace2-ea68e64abd4c",
   "metadata": {},
   "source": [
    "### Get germline files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b977f641-a749-46b6-b2a7-d88f227f4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_germlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74115ddf-b611-45fb-966c-2fa5f572f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37e0f797-c36d-48cf-a13b-3f92c2b9a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 18\n",
      "Number of possibile heavy and light combinations: 126\n",
      "Saved: ../datasets/new/classificator/train/train_seq_only_v.csv, ../datasets/new/classificator/train/train_germ_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    get_germlines.get_germlines(\n",
    "        SEQUENCES_TRAIN, \n",
    "        '{}/train_seq_only_v.csv'.format(TRAIN_DIR),\n",
    "        '{}/train_germ_only_v.csv'.format(TRAIN_DIR),\n",
    "        which='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaf25b23-98ba-4647-89c6-1086c686e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 17\n",
      "Number of possibile heavy and light combinations: 119\n",
      "Saved: ../datasets/new/classificator/val/val_seq_only_v.csv, ../datasets/new/classificator/val/val_germ_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    get_germlines.get_germlines(\n",
    "        SEQUENCES_VAL, \n",
    "        '{}/val_seq_only_v.csv'.format(VAL_DIR),\n",
    "        '{}/val_germ_only_v.csv'.format(VAL_DIR),\n",
    "        which='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2dda9a5-1b36-4d88-91af-adca0173f169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 17\n",
      "Number of possibile heavy and light combinations: 119\n",
      "Saved: ../datasets/new/classificator/test/test_seq_only_v.csv, ../datasets/new/classificator/test/test_germ_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    get_germlines.get_germlines(\n",
    "        SEQUENCES_TEST, \n",
    "        '{}/test_seq_only_v.csv'.format(TEST_DIR),\n",
    "        '{}/test_germ_only_v.csv'.format(TEST_DIR),\n",
    "        which='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb43f7-90eb-4e7b-8db0-1c8e65dce095",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Germline pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b386087-206c-415d-be54-17c18e0f2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import germline_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a10c6e7-9041-41b3-ab40-6ba8aed9e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1000\n",
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee2bc241-03a7-4048-9ee8-cb9d0107aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ac946a2-69ac-47cc-96f9-2ee44df35ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 out of 126 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 13/13 [00:00<00:00, 1124.04it/s]\n",
      "100%|████████████████████████████████████████| 13/13 [00:00<00:00, 55809.57it/s]\n",
      "100%|██████████████████████████████████████████| 13/13 [00:00<00:00, 141.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 30.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/train/train-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/train_seq_only_v.csv'.format(TRAIN_DIR), \n",
    "                                      '{}/train-germline_pairing-alpha_{}_only_v.csv'.format(TRAIN_DIR, ALPHA),\n",
    "                                      '{}/train_germ_only_v.csv'.format(TRAIN_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7a47983-6123-4f93-9b12-74600c27b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 out of 119 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 1049.44it/s]\n",
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 34419.85it/s]\n",
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 198.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 46.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/val/val-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/val_seq_only_v.csv'.format(VAL_DIR), \n",
    "                                      '{}/val-germline_pairing-alpha_{}_only_v.csv'.format(VAL_DIR, ALPHA),\n",
    "                                      '{}/val_germ_only_v.csv'.format(VAL_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "288242e5-880c-4e06-837d-6d684dd0e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 out of 119 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 1008.63it/s]\n",
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 37211.82it/s]\n",
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 145.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 33.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/test/test-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/test_seq_only_v.csv'.format(TEST_DIR), \n",
    "                                      '{}/test-germline_pairing-alpha_{}_only_v.csv'.format(TEST_DIR, ALPHA),\n",
    "                                      '{}/test_germ_only_v.csv'.format(TEST_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f6347-7492-4803-ac5a-a6bc6c3a0a07",
   "metadata": {},
   "source": [
    "### Random pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d218d8a-9425-43e0-86bf-2bd4450a3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241cec3b-91ed-47fb-bd61-845bfd9efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92773ce6-6bdc-4050-b3a1-d767cdf62d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6df8697-9a8f-4956-a2b2-5c121ec7b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 716325 random pairs\n",
      "Saved: ../datasets/new/classificator/train_random/train_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4edc944-e3ac-4a45-aa5e-b3fcd1703211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 271233 random pairs\n",
      "Saved: ../datasets/new/classificator/val_random/val_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32549e2-d65c-4d49-a758-2af41a724926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 369597 random pairs\n",
      "Saved: ../datasets/new/classificator/test_random/test_random.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                 '{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY), \n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d5994-deb3-44ff-a4ef-7ec205be6ad7",
   "metadata": {},
   "source": [
    "### Merge positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9749c037-c550-4ac5-9ff8-3e6286f9113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_positive_and_negative(pos, neg):\n",
    "    pos = pd.concat([pos[['pair_id', 'heavy', 'light']], \n",
    "                     pd.DataFrame({'class': np.zeros(len(pos), dtype=int)})],\n",
    "                     axis=1)\n",
    "    neg = pd.concat([neg[['pair_id', 'heavy', 'light']], \n",
    "                     pd.DataFrame({'class': np.ones(len(pos), dtype=int)})],\n",
    "                     axis=1)\n",
    "    data = pd.concat([pos, neg])\n",
    "    return data.sample(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e01b465a-42f8-4857-9e93-c8be81aa2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('{}/classificator/train/train_seq.csv'.format(DATASET_DIRECTORY), index_col=0)\n",
    "neg = pd.read_csv('{}/classificator/train_random/train_random.csv'.format(DATASET_DIRECTORY))\n",
    "data = merge_positive_and_negative(pos, neg)\n",
    "data.to_csv('{}/classificator/train_random/train.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d88f302-8f54-4323-b36f-0cf67a7760d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('{}/classificator/val/val_seq.csv'.format(DATASET_DIRECTORY), index_col=0)\n",
    "neg = pd.read_csv('{}/classificator/val_random/val_random.csv'.format(DATASET_DIRECTORY))\n",
    "data = merge_positive_and_negative(pos, neg)\n",
    "data.to_csv('{}/classificator/val_random/val.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c6ffd67-9532-4715-b2d0-5d616a2f2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('{}/classificator/test/test_seq.csv'.format(DATASET_DIRECTORY), index_col=0)\n",
    "neg = pd.read_csv('{}/classificator/test_random/test_random.csv'.format(DATASET_DIRECTORY))\n",
    "data = merge_positive_and_negative(pos, neg)\n",
    "data.to_csv('{}/classificator/test_random/test.csv'.format(DATASET_DIRECTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc790b-5dcc-47f5-a395-70962d024165",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c33034-cc61-4671-a96f-24d9493b2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(SEQUENCES_VAL, index_col=0)\n",
    "light_sequences = test_df[['pair_id', 'light_id', 'light']].drop_duplicates().reset_index(drop=True)\n",
    "light_sequences = light_sequences.sample(len(light_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d98abc-e9cb-47ae-9167-2fb7b57ac218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from Bio import Align\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21272877-e2cf-4b2b-8e01-6559128051e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = light_sequences['light'].drop_duplicates().sample(10000).to_numpy()\n",
    "sim = []\n",
    "for x, y in combinations(ls, 2):\n",
    "    sim.append(Levenshtein.ratio(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c8a96d-e033-443a-be7c-7f9bc62a88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6231716007187188 0.11292343526922217 0.3677130044843049 0.9956331877729258\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sim), np.std(sim), np.min(sim), np.max(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df00a458-d9e5-4fe3-be59-a86e75284256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 271230/271230 [06:35<00:00, 685.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_pairs(light_sequences, threshold, sim_func):\n",
    "    pairs = []\n",
    "    index = 0\n",
    "    for _, r in tqdm(light_sequences.iterrows(), total=len(light_sequences)):\n",
    "        found = False\n",
    "        #print('searching a fella for seq', r['light_id'])\n",
    "        while not found:\n",
    "            sim = sim_func(r['light'], light_sequences.iloc[index, 2])\n",
    "            if sim < threshold:\n",
    "                found = True\n",
    "                pairs.append((r['pair_id'], r['light_id'], light_sequences.iloc[index, 1]))\n",
    "            index += 1\n",
    "            if index == len(light_sequences): \n",
    "                index = 0\n",
    "        #pairs.append((r['light_id'], light_sequences.iloc[index, 0]))\n",
    "    return pairs\n",
    "\n",
    "def create_pairs_random(light_sequences):\n",
    "    pairs = []\n",
    "    sampled = light_sequences.sample(len(light_sequences))\n",
    "    for (_, r1), (_, r2) in zip(light_sequences.iterrows(), sampled.iterrows()):\n",
    "        print(r1, r2)\n",
    "    \n",
    "        \n",
    "        \n",
    "pairs_list = create_pairs(light_sequences, np.mean(sim) - np.std(sim), Levenshtein.ratio)\n",
    "\n",
    "#create_pairs_random(light_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc334ba4-239e-4471-9b68-1db60962ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame({\n",
    "    'pair_id': [x for x, _, _ in pairs_list],\n",
    "    'positive_light': [x for _, x, _ in pairs_list],\n",
    "    'negative_light': [x for _, _, x in pairs_list]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5363e5f-2b37-425a-b1c5-92f136ab6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='positive_light', how='right').rename({\n",
    "    'light_id': 'light_id_pos',\n",
    "    'light': 'light_pos'\n",
    "}, axis=1)[['pair_id', 'light_id_pos', 'light_pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7156f05-0711-4ade-a2f2-4f72bbf9a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='negative_light', how='right').rename({\n",
    "    'light_id': 'light_id_neg',\n",
    "    'light': 'light_neg'\n",
    "}, axis=1)[['pair_id', 'light_id_neg', 'light_neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58567a00-473e-41ed-9e31-c9dead382298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_pos, df_neg)\n",
    "df = test_df[['pair_id', 'heavy_id', 'heavy']].merge(df)\n",
    "df.to_csv('{}/val.csv'.format(TEST_DIRECTORY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
