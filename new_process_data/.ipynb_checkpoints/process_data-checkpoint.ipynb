{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239fee40-d43d-440a-af55-6876ebdbc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f395900-dc42-497b-9cbb-749c2399ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OAS_FILE_DIR = r'../../datasets/OAS'\n",
    "DATASET_DIRECTORY = '../datasets/new'\n",
    "\n",
    "# Directory to store the \"raw\" sequences\n",
    "SEQUENCES_DIRECTORY = '{}/sequences'.format(DATASET_DIRECTORY)\n",
    "\n",
    "if not pathlib.Path(SEQUENCES_DIRECTORY).exists():\n",
    "    os.mkdir(SEQUENCES_DIRECTORY)\n",
    "\n",
    "PAIRED_SEQUENCES_FILE = '{}/sequences.csv'.format(SEQUENCES_DIRECTORY)\n",
    "ONLY_REPRESENTATIVE = '{}/representative.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_TRAIN = '{}/train.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_VAL = '{}/val.csv'.format(SEQUENCES_DIRECTORY)\n",
    "SEQUENCES_TEST = '{}/test.csv'.format(SEQUENCES_DIRECTORY)\n",
    "\n",
    "# Directory to store the fasta files\n",
    "FASTA_DIRECTORY = '{}/fasta'.format(DATASET_DIRECTORY)\n",
    "if not pathlib.Path(FASTA_DIRECTORY).exists():\n",
    "    os.mkdir(FASTA_DIRECTORY)\n",
    "\n",
    "# Directory to store the clustering files\n",
    "CLUSTERING_DIRECTORY = '{}/clustering'.format(DATASET_DIRECTORY)\n",
    "if not pathlib.Path(CLUSTERING_DIRECTORY).exists():\n",
    "    os.makedirs(CLUSTERING_DIRECTORY)\n",
    "\n",
    "\n",
    "CLASSIFICATOR_DIR = '{}/classificator'.format(DATASET_DIRECTORY)\n",
    "if not pathlib.Path(CLASSIFICATOR_DIR).exists():\n",
    "    os.makedirs(CLASSIFICATOR_DIR)\n",
    "\n",
    "TRAIN_DIR = '{}/train'.format(CLASSIFICATOR_DIR)\n",
    "if not pathlib.Path(TRAIN_DIR).exists():\n",
    "    os.makedirs(TRAIN_DIR)\n",
    "\n",
    "VAL_DIR = '{}/val'.format(CLASSIFICATOR_DIR)\n",
    "if not pathlib.Path(VAL_DIR).exists():\n",
    "    os.makedirs(VAL_DIR)\n",
    "\n",
    "TEST_DIR = '{}/test'.format(CLASSIFICATOR_DIR)\n",
    "if not pathlib.Path(TEST_DIR).exists():\n",
    "    os.makedirs(TEST_DIR)\n",
    "\n",
    "\n",
    "TEST_DIRECTORY = '{}/test'.format(DATASET_DIRECTORY)\n",
    "\n",
    "if not pathlib.Path(TEST_DIRECTORY).exists():\n",
    "    os.makedirs(TEST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466061cd-0e53-4835-88a1-1631f4c82da9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4ad0e7-11df-42b3-b4f7-afd0a9f412c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27807dc0-a6a3-48d1-a45a-7833531ea475",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH_GERMLINE = 'v'\n",
    "STORE_SPECIE = False\n",
    "SUBSAMPLE = None\n",
    "ONLY_HUMAN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b77b75-d4d6-45e4-a050-f0e9da8f781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769f05b1-ed1a-4cc8-afee-311bcbae06fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Found 310 files.\n",
      "human:                280\n",
      "mouse_C57BL/6:          2\n",
      "mouse_BALB/c:           8\n",
      "rat_SD:                20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 280/280 [04:47<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the data\n",
      "Initial number of rows: 1954079\n",
      "Removed 279826 rows (-14.320%), new number of rows: 1674253.\n",
      "Assining ids...\n",
      "Number of unique heavy: 1654917\n",
      "Number of unique light: 724832\n",
      "Number of unique pairs:  1674177\n",
      "Cleaning the germlines...\n",
      "Saved: ../datasets/new_only_v/sequences/sequences.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(read_raw)\n",
    "    read_raw.read_raw(OAS_FILE_DIR, PAIRED_SEQUENCES_FILE, \n",
    "                      subsample=SUBSAMPLE, only_human=ONLY_HUMAN,\n",
    "                      which_germline=WHICH_GERMLINE, store_specie=STORE_SPECIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0512e8-4d96-47aa-91ed-559984b9249f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Clusterize the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f123dc84-4114-4abd-8928-4438da19d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a134344-f7ed-4bd3-9bff-3b96dea614eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICH = 'both'\n",
    "\n",
    "if WHICH != 'both':\n",
    "    FASTA_SEQUENCES = 'sequences_{}.fasta'.format(WHICH)\n",
    "else:\n",
    "    FASTA_SEQUENCES = 'sequences.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da01a410-9d46-452f-87ec-35e83087a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "123a7a37-e13e-4b5b-a567-c35f3ca07545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1674177/1674177 [01:13<00:00, 22781.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new_only_v/fasta/sequences.fasta\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(generate_fasta)\n",
    "    generate_fasta.generate_fasta(PAIRED_SEQUENCES_FILE, \n",
    "                                  '{}/{}'.format(FASTA_DIRECTORY, FASTA_SEQUENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5132a43a-77d6-499f-ac4f-4a9810bbb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SEQ_ID = 0.8\n",
    "\n",
    "commands = 'source cluster.sh {} {} {} {}\\n'.format(\n",
    "    DATASET_DIRECTORY, \n",
    "    'fasta/{}'.format(FASTA_SEQUENCES), \n",
    "    'clustering/sequences', \n",
    "    MIN_SEQ_ID\n",
    ")\n",
    "\n",
    "commands += 'rm -rf {}/fasta_files'.format(DATASET_DIRECTORY)\n",
    "\n",
    "with open('clustering_commands.sh', 'w') as f:\n",
    "    f.write(commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4888b4-da93-4559-9a9e-0e719d6a0a50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get only representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06caa02d-3015-4b75-b533-d9f480468beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_csv('{}/sequences.tsv'.format(CLUSTERING_DIRECTORY), sep='\\t', header=None).rename(\n",
    "    {\n",
    "        0: 'representative',\n",
    "        1: 'sequences'\n",
    "    },\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c8e05b-53df-49be-aca2-d079e75cb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = {\n",
    "    'representative': [],\n",
    "    'size': []\n",
    "}\n",
    "\n",
    "for seq, data in clusters.groupby('representative'):\n",
    "    cluster_sizes['representative'].append(seq)\n",
    "    cluster_sizes['size'].append(len(data))\n",
    "\n",
    "cluster_sizes = pd.DataFrame(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96334e92-fa3d-4c4c-883a-3ab83c8d07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota! Ci sono delle sequenze uguali ma con germline diverse\n",
    "\n",
    "sequences = pd.read_csv(PAIRED_SEQUENCES_FILE, index_col=0)\n",
    "representative = clusters['representative'].drop_duplicates()\n",
    "representative = sequences.merge(representative, left_on='pair_id', right_on='representative', how='right')[sequences.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01ce13b6-bdea-4709-90d1-1b4cb1589621",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative.to_csv(ONLY_REPRESENTATIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee16658-1c9e-4720-a947-b5003cdebe19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Split in train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fafe30d-1e0a-47fe-9cde-8f197ac45b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_germline_data(df):\n",
    "    from itertools import product\n",
    "    \n",
    "    germlines_dict = {'heavy_germline': [], 'light_germline': [], 'counter': []}\n",
    "\n",
    "    for heavy_germline, data in df.groupby('heavy_germline'):\n",
    "        for light_germline, data2 in data.groupby('light_germline'):\n",
    "            germlines_dict['heavy_germline'].append(heavy_germline)\n",
    "            germlines_dict['light_germline'].append(light_germline)\n",
    "            germlines_dict['counter'].append(len(data2))\n",
    "\n",
    "    germlines = pd.DataFrame(germlines_dict)\n",
    "\n",
    "    all_germline_pairs = set(product(germlines['heavy_germline'].unique(), germlines['light_germline'].unique()))\n",
    "    found_germline_pairs = set([(row['heavy_germline'], row['light_germline']) for _, row in germlines.iterrows()])\n",
    "    not_found_germline_pairs = all_germline_pairs.difference(found_germline_pairs)\n",
    "\n",
    "    zero_germlines = {'heavy_germline': [], 'light_germline': [], 'counter': []}\n",
    "\n",
    "    for h, l in not_found_germline_pairs:\n",
    "        zero_germlines['heavy_germline'].append(h)\n",
    "        zero_germlines['light_germline'].append(l)\n",
    "        zero_germlines['counter'].append(0)\n",
    "\n",
    "    germlines = pd.concat([germlines, pd.DataFrame(zero_germlines)], axis=0).sort_values(by='counter', ascending=False)\n",
    "\n",
    "    return germlines\n",
    "\n",
    "def split(germlines, sequences):\n",
    "    # Select some heavy germlines exclusive to dataset 2\n",
    "    heavy_germlines_dict = {'germline': [], 'counter': []}\n",
    "    for g, data in germlines.groupby('heavy_germline'):\n",
    "        heavy_germlines_dict['germline'].append(g)\n",
    "        heavy_germlines_dict['counter'].append(data.sum()['counter'])\n",
    "    heavy_germlines = pd.DataFrame(heavy_germlines_dict)\n",
    "\n",
    "    heavy_germlines = heavy_germlines.sort_values(by='counter', ascending=False)\n",
    "\n",
    "    lower = heavy_germlines['counter'].quantile(0.20)\n",
    "    higher = heavy_germlines['counter'].quantile(0.80)\n",
    "\n",
    "    selected_heavy_germlines = heavy_germlines[(heavy_germlines['counter'] > lower) & (heavy_germlines['counter'] < higher)]\n",
    "    selected_heavy_germlines = selected_heavy_germlines.sample(int(len(selected_heavy_germlines)*0.1))\n",
    "\n",
    "    # Select some light germlines exlcusive to dataset 2\n",
    "    light_germlines_dict = {'germline': [], 'counter': []}\n",
    "    for g, data in germlines.groupby('light_germline'):\n",
    "        light_germlines_dict['germline'].append(g)\n",
    "        light_germlines_dict['counter'].append(data.sum()['counter'])\n",
    "    light_germlines = pd.DataFrame(light_germlines_dict)\n",
    "\n",
    "    light_germlines = light_germlines.sort_values(by='counter', ascending=False)\n",
    "\n",
    "    lower = light_germlines['counter'].quantile(0.20)\n",
    "    higher = light_germlines['counter'].quantile(0.80)\n",
    "\n",
    "    selected_light_germlines = light_germlines[(light_germlines['counter'] > lower) & (light_germlines['counter'] < higher)]\n",
    "    selected_light_germlines = selected_light_germlines.sample(int(len(selected_light_germlines)*0.1))\n",
    "\n",
    "    merged = sequences.merge(selected_heavy_germlines, left_on='heavy_germline', right_on='germline', how='left', indicator=True)\n",
    "    merged = merged[merged['_merge'] == 'left_only'][sequences.columns]\n",
    "    merged = merged.merge(selected_light_germlines, left_on='light_germline', right_on='germline', how='left', indicator=True)\n",
    "    merged = merged[merged['_merge'] == 'left_only'][sequences.columns]\n",
    "    to_split = merged\n",
    "\n",
    "    df1, df2 = train_test_split(to_split, test_size=0.25, random_state=1234567890)\n",
    "\n",
    "    df2 = pd.concat([\n",
    "        df2, \n",
    "        pd.merge(sequences, selected_heavy_germlines['germline'], \n",
    "                 left_on='heavy_germline', right_on='germline')[sequences.columns],\n",
    "        pd.merge(sequences, selected_light_germlines['germline'], \n",
    "                 left_on='light_germline', right_on='germline')[sequences.columns]\n",
    "    ])\n",
    "\n",
    "    df2 = df2.drop_duplicates()\n",
    "\n",
    "    return df1, df2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9737fab4-5a6b-41cb-88a5-cb2e0d4b06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative = pd.read_csv(ONLY_REPRESENTATIVE, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9761e055-749c-42d5-b2ef-2452b1c545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "germlines_all = compute_germline_data(representative)\n",
    "trainval, test = split(germlines_all, representative)\n",
    "test.to_csv(SEQUENCES_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7edd197-8983-4b5a-95a1-820e0a392426",
   "metadata": {},
   "outputs": [],
   "source": [
    "germlines_trainval = compute_germline_data(trainval)\n",
    "train, val = split(germlines_trainval, trainval)\n",
    "train.to_csv(SEQUENCES_TRAIN)\n",
    "val.to_csv(SEQUENCES_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea3de7-dfcd-45af-ace2-ea68e64abd4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get germline files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b977f641-a749-46b6-b2a7-d88f227f4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_germlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74115ddf-b611-45fb-966c-2fa5f572f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37e0f797-c36d-48cf-a13b-3f92c2b9a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 18\n",
      "Number of possibile heavy and light combinations: 126\n",
      "Saved: ../datasets/new/classificator/train/train_seq_only_v.csv, ../datasets/new/classificator/train/train_germ_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    get_germlines.get_germlines(\n",
    "        SEQUENCES_TRAIN, \n",
    "        '{}/train_seq_only_v.csv'.format(TRAIN_DIR),\n",
    "        '{}/train_germ_only_v.csv'.format(TRAIN_DIR),\n",
    "        which='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaf25b23-98ba-4647-89c6-1086c686e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 17\n",
      "Number of possibile heavy and light combinations: 119\n",
      "Saved: ../datasets/new/classificator/val/val_seq_only_v.csv, ../datasets/new/classificator/val/val_germ_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    get_germlines.get_germlines(\n",
    "        SEQUENCES_VAL, \n",
    "        '{}/val_seq_only_v.csv'.format(VAL_DIR),\n",
    "        '{}/val_germ_only_v.csv'.format(VAL_DIR),\n",
    "        which='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2dda9a5-1b36-4d88-91af-adca0173f169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get germlines id...\n",
      "Number of unique heavy combinations: 7\n",
      "Number of unique light combinations: 17\n",
      "Number of possibile heavy and light combinations: 119\n",
      "Saved: ../datasets/new/classificator/test/test_seq_only_v.csv, ../datasets/new/classificator/test/test_germ_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(get_germlines)\n",
    "    get_germlines.get_germlines(\n",
    "        SEQUENCES_TEST, \n",
    "        '{}/test_seq_only_v.csv'.format(TEST_DIR),\n",
    "        '{}/test_germ_only_v.csv'.format(TEST_DIR),\n",
    "        which='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb43f7-90eb-4e7b-8db0-1c8e65dce095",
   "metadata": {},
   "source": [
    "### Germline pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b386087-206c-415d-be54-17c18e0f2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import germline_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a10c6e7-9041-41b3-ab40-6ba8aed9e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1000\n",
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee2bc241-03a7-4048-9ee8-cb9d0107aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ac946a2-69ac-47cc-96f9-2ee44df35ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 out of 126 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 13/13 [00:00<00:00, 1124.04it/s]\n",
      "100%|████████████████████████████████████████| 13/13 [00:00<00:00, 55809.57it/s]\n",
      "100%|██████████████████████████████████████████| 13/13 [00:00<00:00, 141.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 30.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/train/train-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/train_seq_only_v.csv'.format(TRAIN_DIR), \n",
    "                                      '{}/train-germline_pairing-alpha_{}_only_v.csv'.format(TRAIN_DIR, ALPHA),\n",
    "                                      '{}/train_germ_only_v.csv'.format(TRAIN_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7a47983-6123-4f93-9b12-74600c27b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 out of 119 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 1049.44it/s]\n",
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 34419.85it/s]\n",
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 198.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 46.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/val/val-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/val_seq_only_v.csv'.format(VAL_DIR), \n",
    "                                      '{}/val-germline_pairing-alpha_{}_only_v.csv'.format(VAL_DIR, ALPHA),\n",
    "                                      '{}/val_germ_only_v.csv'.format(VAL_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "288242e5-880c-4e06-837d-6d684dd0e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 out of 119 of only zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 1008.63it/s]\n",
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 37211.82it/s]\n",
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 145.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 33.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../datasets/new/classificator/test/test-germline_pairing-alpha_1000_only_v.csv\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(germline_pairing)\n",
    "    germline_pairing.germline_pairing('{}/test_seq_only_v.csv'.format(TEST_DIR), \n",
    "                                      '{}/test-germline_pairing-alpha_{}_only_v.csv'.format(TEST_DIR, ALPHA),\n",
    "                                      '{}/test_germ_only_v.csv'.format(TEST_DIR),\n",
    "                                      NUMBER, ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f6347-7492-4803-ac5a-a6bc6c3a0a07",
   "metadata": {},
   "source": [
    "### Random pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ac553b-a185-4a3b-a8c2-5a58e1831641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heavy_id</th>\n",
       "      <th>light_id</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>pairing_index</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>323767</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>9835</td>\n",
       "      <td>QVQLVQSGSELKKPGASVKVSCKASGYTFTSYAMNWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTQSPLSLPVTPGEPASISCRSSQSLLHSNGYNYLDWYLQKPG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>882323</td>\n",
       "      <td>489318</td>\n",
       "      <td>1</td>\n",
       "      <td>12722</td>\n",
       "      <td>EVQLVQSGAEVKKPGESLKISCKGSGYSFTSYWIGWVRQMPGKGLE...</td>\n",
       "      <td>QAGLTQPPSISKGLRETATLTCTGNSENVGSHGAAWLQQHQGHPPK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45772</td>\n",
       "      <td>16293</td>\n",
       "      <td>2</td>\n",
       "      <td>11984</td>\n",
       "      <td>QVQLQESGPGLVKPSETLSLTCTISGGSISGYYWSWIRQPPGKGLE...</td>\n",
       "      <td>DIVMTQSPLSLPVTPGEPASISCRSSQSLLHSNGYNYLDWYVQKPG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>404987</td>\n",
       "      <td>196781</td>\n",
       "      <td>3</td>\n",
       "      <td>12555</td>\n",
       "      <td>QVQLQQSGPGLVKPSQTLSLTCAISGDSVSSNSAAWNWIRQSPSRG...</td>\n",
       "      <td>NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>461907</td>\n",
       "      <td>79910</td>\n",
       "      <td>4</td>\n",
       "      <td>11178</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKASGYTFTSYAMHWVRQAPGQRLE...</td>\n",
       "      <td>ETTLTQSPAFMSATPGDKVNISCKASQDIDDDMNWYQQKPGEAAIF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716320</th>\n",
       "      <td>1261220</td>\n",
       "      <td>67919</td>\n",
       "      <td>702813</td>\n",
       "      <td>12226</td>\n",
       "      <td>EVQLVQSGAQVKKPGESLKISCKASGYSFTSYWIGWVRQMSGEGLE...</td>\n",
       "      <td>QSVLTQPPSVSGAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716321</th>\n",
       "      <td>851718</td>\n",
       "      <td>490193</td>\n",
       "      <td>702814</td>\n",
       "      <td>13033</td>\n",
       "      <td>QVQVVQSGSELKEPGASVRISCRTSGYPFTTYPIHWVRQAPGHGLE...</td>\n",
       "      <td>QLVLTQSPSASASLGASVKLTCSLSRGHSSYAIAWHQQQPEKGPRY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716322</th>\n",
       "      <td>730126</td>\n",
       "      <td>205339</td>\n",
       "      <td>702815</td>\n",
       "      <td>17200</td>\n",
       "      <td>QVTLRESGPALVKPKETLTLTCSFSGFSLSTAGMCMSWIRQPPGKA...</td>\n",
       "      <td>NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716323</th>\n",
       "      <td>335218</td>\n",
       "      <td>388503</td>\n",
       "      <td>702816</td>\n",
       "      <td>16528</td>\n",
       "      <td>QVQLVQSGSELKKPGASVKVSCKASGYTFTSYAMNWVRQAPGQGLE...</td>\n",
       "      <td>QPVLTQPPSASASLGASVTLTCTLSSGYSSCNVDWYQQRPGKGPRF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716324</th>\n",
       "      <td>1336900</td>\n",
       "      <td>332277</td>\n",
       "      <td>702817</td>\n",
       "      <td>13998</td>\n",
       "      <td>QVQLQQSGPGLVKPSQTLSLTCAISGDTVSSNGAAWNWIRLSPSRG...</td>\n",
       "      <td>QTVVTQEPSLSVSPGGTVTLTCGLRSGSVSIAHYPSWYQQTPGQAP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716325 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        heavy_id  light_id  pair_id  pairing_index  \\\n",
       "0         323767       201        0           9835   \n",
       "1         882323    489318        1          12722   \n",
       "2          45772     16293        2          11984   \n",
       "3         404987    196781        3          12555   \n",
       "4         461907     79910        4          11178   \n",
       "...          ...       ...      ...            ...   \n",
       "716320   1261220     67919   702813          12226   \n",
       "716321    851718    490193   702814          13033   \n",
       "716322    730126    205339   702815          17200   \n",
       "716323    335218    388503   702816          16528   \n",
       "716324   1336900    332277   702817          13998   \n",
       "\n",
       "                                                    heavy  \\\n",
       "0       QVQLVQSGSELKKPGASVKVSCKASGYTFTSYAMNWVRQAPGQGLE...   \n",
       "1       EVQLVQSGAEVKKPGESLKISCKGSGYSFTSYWIGWVRQMPGKGLE...   \n",
       "2       QVQLQESGPGLVKPSETLSLTCTISGGSISGYYWSWIRQPPGKGLE...   \n",
       "3       QVQLQQSGPGLVKPSQTLSLTCAISGDSVSSNSAAWNWIRQSPSRG...   \n",
       "4       QVQLVQSGAEVKKPGASVKVSCKASGYTFTSYAMHWVRQAPGQRLE...   \n",
       "...                                                   ...   \n",
       "716320  EVQLVQSGAQVKKPGESLKISCKASGYSFTSYWIGWVRQMSGEGLE...   \n",
       "716321  QVQVVQSGSELKEPGASVRISCRTSGYPFTTYPIHWVRQAPGHGLE...   \n",
       "716322  QVTLRESGPALVKPKETLTLTCSFSGFSLSTAGMCMSWIRQPPGKA...   \n",
       "716323  QVQLVQSGSELKKPGASVKVSCKASGYTFTSYAMNWVRQAPGQGLE...   \n",
       "716324  QVQLQQSGPGLVKPSQTLSLTCAISGDTVSSNGAAWNWIRLSPSRG...   \n",
       "\n",
       "                                                    light  \n",
       "0       DIVMTQSPLSLPVTPGEPASISCRSSQSLLHSNGYNYLDWYLQKPG...  \n",
       "1       QAGLTQPPSISKGLRETATLTCTGNSENVGSHGAAWLQQHQGHPPK...  \n",
       "2       DIVMTQSPLSLPVTPGEPASISCRSSQSLLHSNGYNYLDWYVQKPG...  \n",
       "3       NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...  \n",
       "4       ETTLTQSPAFMSATPGDKVNISCKASQDIDDDMNWYQQKPGEAAIF...  \n",
       "...                                                   ...  \n",
       "716320  QSVLTQPPSVSGAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAP...  \n",
       "716321  QLVLTQSPSASASLGASVKLTCSLSRGHSSYAIAWHQQQPEKGPRY...  \n",
       "716322  NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...  \n",
       "716323  QPVLTQPPSASASLGASVTLTCTLSSGYSSCNVDWYQQRPGKGPRF...  \n",
       "716324  QTVVTQEPSLSVSPGGTVTLTCGLRSGSVSIAHYPSWYQQTPGQAP...  \n",
       "\n",
       "[716325 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('../datasets/new/classificator/train/train-germline_pairing-alpha_1000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d218d8a-9425-43e0-86bf-2bd4450a3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "241cec3b-91ed-47fb-bd61-845bfd9efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92773ce6-6bdc-4050-b3a1-d767cdf62d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_NEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6df8697-9a8f-4956-a2b2-5c121ec7b9be",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/new/classificator/new/train/train_seq.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COMPUTE_NEW:\n\u001b[1;32m      2\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mreload(random_pairing)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mrandom_pairing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_pairing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/classificator/new/train/train_seq.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_DIRECTORY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/classificator/new/train_random/train_random.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_DIRECTORY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNUMBER\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk1/e.darco/thesis/new_process_data/random_pairing.py:75\u001b[0m, in \u001b[0;36mrandom_pairing\u001b[0;34m(location, output_location, germline_ids_location, n)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_pairing\u001b[39m(location, output_location, germline_ids_location, n):\n\u001b[0;32m---> 75\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# All the heavy and light sequences\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     heavy_ids \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheavy_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheavy\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[0;32m/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/new/classificator/new/train/train_seq.csv'"
     ]
    }
   ],
   "source": [
    "if COMPUTE_NEW:\n",
    "    importlib.reload(random_pairing)\n",
    "    random_pairing.random_pairing('{}/classificator/new/train/train_seq.csv'.format(DATASET_DIRECTORY),\n",
    "                                  '{}/classificator/new/train_random/train_random.csv'.format(DATASET_DIRECTORY),\n",
    "                                  NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d5994-deb3-44ff-a4ef-7ec205be6ad7",
   "metadata": {},
   "source": [
    "### Merge positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9749c037-c550-4ac5-9ff8-3e6286f9113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_positive_and_negative(pos, neg):\n",
    "    pos = pd.concat([pos[['pair_id', 'heavy', 'light']], \n",
    "                     pd.DataFrame({'class': np.zeros(len(pos), dtype=int)})],\n",
    "                     axis=1)\n",
    "    neg = pd.concat([neg[['pair_id', 'heavy', 'light']], \n",
    "                     pd.DataFrame({'class': np.ones(len(pos), dtype=int)})],\n",
    "                     axis=1)\n",
    "    data = pd.concat([pos, neg])\n",
    "    return data.sample(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e01b465a-42f8-4857-9e93-c8be81aa2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('{}/train_seq_only_v.csv'.format(TRAIN_DIR), index_col=0)\n",
    "neg = pd.read_csv('{}/train-germline_pairing-alpha_{}_only_v.csv'.format(TRAIN_DIR, ALPHA))\n",
    "data = merge_positive_and_negative(pos, neg)\n",
    "data.to_csv('{}/train_only_v.csv'.format(TRAIN_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d88f302-8f54-4323-b36f-0cf67a7760d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('{}/val_seq_only_v.csv'.format(VAL_DIR), index_col=0)\n",
    "neg = pd.read_csv('{}/val-germline_pairing-alpha_{}_only_v.csv'.format(VAL_DIR, ALPHA))\n",
    "data = merge_positive_and_negative(pos, neg)\n",
    "data.to_csv('{}/val_only_v.csv'.format(VAL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c6ffd67-9532-4715-b2d0-5d616a2f2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('{}/test_seq_only_v.csv'.format(TEST_DIR), index_col=0)\n",
    "neg = pd.read_csv('{}/test-germline_pairing-alpha_{}_only_v.csv'.format(TEST_DIR, ALPHA))\n",
    "data = merge_positive_and_negative(pos, neg)\n",
    "data.to_csv('{}/test_only_v.csv'.format(TEST_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc790b-5dcc-47f5-a395-70962d024165",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c33034-cc61-4671-a96f-24d9493b2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(SEQUENCES_VAL, index_col=0)\n",
    "light_sequences = test_df[['pair_id', 'light_id', 'light']].drop_duplicates().reset_index(drop=True)\n",
    "light_sequences = light_sequences.sample(len(light_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d98abc-e9cb-47ae-9167-2fb7b57ac218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from Bio import Align\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21272877-e2cf-4b2b-8e01-6559128051e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = light_sequences['light'].drop_duplicates().sample(10000).to_numpy()\n",
    "sim = []\n",
    "for x, y in combinations(ls, 2):\n",
    "    sim.append(Levenshtein.ratio(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c8a96d-e033-443a-be7c-7f9bc62a88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6231716007187188 0.11292343526922217 0.3677130044843049 0.9956331877729258\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sim), np.std(sim), np.min(sim), np.max(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df00a458-d9e5-4fe3-be59-a86e75284256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 271230/271230 [06:35<00:00, 685.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_pairs(light_sequences, threshold, sim_func):\n",
    "    pairs = []\n",
    "    index = 0\n",
    "    for _, r in tqdm(light_sequences.iterrows(), total=len(light_sequences)):\n",
    "        found = False\n",
    "        #print('searching a fella for seq', r['light_id'])\n",
    "        while not found:\n",
    "            sim = sim_func(r['light'], light_sequences.iloc[index, 2])\n",
    "            if sim < threshold:\n",
    "                found = True\n",
    "                pairs.append((r['pair_id'], r['light_id'], light_sequences.iloc[index, 1]))\n",
    "            index += 1\n",
    "            if index == len(light_sequences): \n",
    "                index = 0\n",
    "        #pairs.append((r['light_id'], light_sequences.iloc[index, 0]))\n",
    "    return pairs\n",
    "\n",
    "def create_pairs_random(light_sequences):\n",
    "    pairs = []\n",
    "    sampled = light_sequences.sample(len(light_sequences))\n",
    "    for (_, r1), (_, r2) in zip(light_sequences.iterrows(), sampled.iterrows()):\n",
    "        print(r1, r2)\n",
    "    \n",
    "        \n",
    "        \n",
    "pairs_list = create_pairs(light_sequences, np.mean(sim) - np.std(sim), Levenshtein.ratio)\n",
    "\n",
    "#create_pairs_random(light_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc334ba4-239e-4471-9b68-1db60962ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame({\n",
    "    'pair_id': [x for x, _, _ in pairs_list],\n",
    "    'positive_light': [x for _, x, _ in pairs_list],\n",
    "    'negative_light': [x for _, _, x in pairs_list]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5363e5f-2b37-425a-b1c5-92f136ab6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='positive_light', how='right').rename({\n",
    "    'light_id': 'light_id_pos',\n",
    "    'light': 'light_pos'\n",
    "}, axis=1)[['pair_id', 'light_id_pos', 'light_pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7156f05-0711-4ade-a2f2-4f72bbf9a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.merge(light_sequences[['light_id', 'light']].drop_duplicates(), \n",
    "                  pairs, \n",
    "                  left_on='light_id', right_on='negative_light', how='right').rename({\n",
    "    'light_id': 'light_id_neg',\n",
    "    'light': 'light_neg'\n",
    "}, axis=1)[['pair_id', 'light_id_neg', 'light_neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58567a00-473e-41ed-9e31-c9dead382298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_pos, df_neg)\n",
    "df = test_df[['pair_id', 'heavy_id', 'heavy']].merge(df)\n",
    "df.to_csv('{}/val.csv'.format(TEST_DIRECTORY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
