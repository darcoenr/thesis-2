/disk1/e.darco/venvs/pytorch-nightly/bin/python: Error while finding module specification for 'grid_search.py' (ModuleNotFoundError: No module named 'grid_search'). Try using 'grid_search' instead of 'grid_search.py' as the module name.
Error in cpuinfo: prctl(PR_SVE_GET_VL) failed
Traceback (most recent call last):
  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/disk1/e.darco/thesis/training/grid-search.py", line 48, in <module>
    train.train(**parameters_dict)
  File "/disk1/e.darco/thesis/training/train.py", line 80, in train
    train_ds, train_ds_hyperpar = utils.get_dataset(train_data_location, shuffle=shuffle, subsample=subsample, frac=frac, seed=seed)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk1/e.darco/thesis/training/utils.py", line 96, in get_dataset
    df = pd.read_csv(location, index_col=0)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1880, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/disk1/e.darco/venvs/pytorch-nightly/lib/python3.11/site-packages/pandas/io/common.py", line 873, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '../datasets/new/classificator/random/train/train.csv'
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train/train.csv...
Error in cpuinfo: prctl(PR_SVE_GET_VL) failed
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6481 ± 0.0269 | 0.6438 ± 0.0940 | 0.6469 ± 0.0314 | 0.6312 ± 0.0667 |
    2 | 0.6628 ± 0.0573 | 0.5563 ± 0.0606 | 0.6656 ± 0.0248 | 0.5844 ± 0.0699 |
    3 | 0.6156 ± 0.0715 | 0.6625 ± 0.1090 | 0.6479 ± 0.0625 | 0.6500 ± 0.0800 |
    4 | 0.5648 ± 0.0792 | 0.7500 ± 0.0713 | 0.6142 ± 0.0678 | 0.6875 ± 0.0713 |
    5 | 0.6203 ± 0.0789 | 0.6500 ± 0.1209 | 0.5986 ± 0.0898 | 0.7031 ± 0.1129 |
    6 | 0.6440 ± 0.0712 | 0.7000 ± 0.0781 | 0.6191 ± 0.0576 | 0.6844 ± 0.0963 |
    7 | 0.5798 ± 0.0459 | 0.6625 ± 0.0667 | 0.6514 ± 0.0543 | 0.6375 ± 0.0563 |
    8 | 0.6449 ± 0.0900 | 0.6312 ± 0.1159 | 0.6211 ± 0.0787 | 0.6000 ± 0.1116 |
    9 | 0.6192 ± 0.0283 | 0.6562 ± 0.0559 | 0.6084 ± 0.0180 | 0.6969 ± 0.0505 |
   10 | 0.6521 ± 0.0538 | 0.6625 ± 0.0750 | 0.6625 ± 0.0379 | 0.5563 ± 0.0710 |
   11 | 0.6155 ± 0.0410 | 0.6562 ± 0.0198 | 0.5834 ± 0.0415 | 0.7219 ± 0.0647 |
   12 | 0.5994 ± 0.0640 | 0.6500 ± 0.0723 | 0.5939 ± 0.0428 | 0.6937 ± 0.0459 |
   13 | 0.6368 ± 0.0877 | 0.6438 ± 0.0852 | 0.5967 ± 0.0325 | 0.6875 ± 0.0895 |
   14 | 0.5948 ± 0.0868 | 0.7312 ± 0.0980 | 0.6257 ± 0.0426 | 0.6719 ± 0.0597 |
   15 | 0.6192 ± 0.0215 | 0.6937 ± 0.0364 | 0.6227 ± 0.0298 | 0.6906 ± 0.0808 |
   16 | 0.5980 ± 0.0225 | 0.6750 ± 0.0319 | 0.6256 ± 0.0441 | 0.6094 ± 0.0401 |
   17 | 0.6283 ± 0.0512 | 0.6062 ± 0.0852 | 0.6145 ± 0.0424 | 0.6719 ± 0.0674 |
   18 | 0.6325 ± 0.1036 | 0.6813 ± 0.1287 | 0.6284 ± 0.0458 | 0.7000 ± 0.0612 |
   19 | 0.6150 ± 0.0398 | 0.6687 ± 0.0468 | 0.6007 ± 0.0764 | 0.7063 ± 0.0864 |
   20 | 0.6204 ± 0.0396 | 0.6312 ± 0.0459 | 0.6345 ± 0.0423 | 0.6531 ± 0.0691 |
   21 | 0.5970 ± 0.0785 | 0.6875 ± 0.0839 | 0.6063 ± 0.0472 | 0.6906 ± 0.0584 |
   22 | 0.6503 ± 0.0994 | 0.6375 ± 0.1576 | 0.6797 ± 0.0750 | 0.6656 ± 0.0740 |
   23 | 0.5863 ± 0.0407 | 0.6438 ± 0.0545 | 0.6420 ± 0.0649 | 0.6656 ± 0.0839 |
   24 | 0.6248 ± 0.0921 | 0.6813 ± 0.0871 | 0.6231 ± 0.0449 | 0.6219 ± 0.0647 |
   25 | 0.6823 ± 0.0283 | 0.6125 ± 0.0424 | 0.6342 ± 0.0512 | 0.6500 ± 0.1134 |
   26 | 0.5419 ± 0.0481 | 0.7625 ± 0.0319 | 0.6381 ± 0.0869 | 0.6656 ± 0.1144 |
   27 | 0.6324 ± 0.0546 | 0.6625 ± 0.0800 | 0.5804 ± 0.0624 | 0.7125 ± 0.0824 |
   28 | 0.6844 ± 0.0592 | 0.5875 ± 0.1035 | 0.6365 ± 0.0704 | 0.6156 ± 0.1037 |
   29 | 0.6046 ± 0.0894 | 0.7000 ± 0.0980 | 0.5875 ± 0.0492 | 0.7094 ± 0.0727 |
   30 | 0.6218 ± 0.0847 | 0.6937 ± 0.0776 | 0.5782 ± 0.0700 | 0.6906 ± 0.0796 |
   31 | 0.6234 ± 0.0629 | 0.6438 ± 0.0580 | 0.6077 ± 0.0528 | 0.6719 ± 0.0853 |
   32 | 0.6152 ± 0.0527 | 0.6750 ± 0.0755 | 0.6232 ± 0.0570 | 0.6562 ± 0.0593 |
   33 | 0.6954 ± 0.0731 | 0.5687 ± 0.0776 | 0.6682 ± 0.0608 | 0.5281 ± 0.0647 |
   34 | 0.6240 ± 0.0566 | 0.6750 ± 0.0673 | 0.6207 ± 0.0637 | 0.6656 ± 0.0740 |
   35 | 0.6490 ± 0.0854 | 0.6375 ± 0.0673 | 0.6099 ± 0.0282 | 0.6312 ± 0.0573 |
   36 | 0.6256 ± 0.0865 | 0.6875 ± 0.1064 | 0.7463 ± 0.0975 | 0.5844 ± 0.1008 |
   37 | 0.6418 ± 0.0872 | 0.6312 ± 0.1272 | 0.5957 ± 0.0849 | 0.6750 ± 0.0908 |
   38 | 0.6035 ± 0.0603 | 0.6562 ± 0.0559 | 0.6277 ± 0.0628 | 0.6500 ± 0.0800 |
   39 | 0.6086 ± 0.0491 | 0.6500 ± 0.0750 | 0.5998 ± 0.0415 | 0.6906 ± 0.0567 |
   40 | 0.6758 ± 0.0705 | 0.5875 ± 0.0637 | 0.5947 ± 0.0387 | 0.6687 ± 0.0643 |
   41 | 0.6185 ± 0.0476 | 0.5813 ± 0.0702 | 0.6786 ± 0.0772 | 0.5250 ± 0.0622 |
   42 | 0.6670 ± 0.0713 | 0.5813 ± 0.0319 | 0.6561 ± 0.0446 | 0.5375 ± 0.0653 |
   43 | 0.6090 ± 0.0340 | 0.6750 ± 0.0250 | 0.5759 ± 0.0657 | 0.7188 ± 0.1036 |
   44 | 0.6081 ± 0.0430 | 0.6750 ± 0.0580 | 0.6088 ± 0.0643 | 0.6625 ± 0.1090 |
   45 | 0.6522 ± 0.0366 | 0.6312 ± 0.0723 | 0.6221 ± 0.0643 | 0.6562 ± 0.0978 |
   46 | 0.6607 ± 0.0653 | 0.6250 ± 0.0656 | 0.6638 ± 0.0622 | 0.6281 ± 0.0584 |
   47 | 0.5883 ± 0.0928 | 0.7188 ± 0.0765 | 0.6448 ± 0.0788 | 0.6531 ± 0.0808 |
   48 | 0.5653 ± 0.0606 | 0.7188 ± 0.0280 | 0.5898 ± 0.0634 | 0.7031 ± 0.0349 |
   49 | 0.6077 ± 0.0948 | 0.7000 ± 0.0805 | 0.6138 ± 0.0958 | 0.7031 ± 0.0853 |
   50 | 0.5672 ± 0.0978 | 0.6875 ± 0.0740 | 0.6362 ± 0.0709 | 0.6500 ± 0.0556 |
   51 | 0.5642 ± 0.0445 | 0.7125 ± 0.0415 | 0.5860 ± 0.0650 | 0.6875 ± 0.0815 |
   52 | 0.5783 ± 0.0683 | 0.6813 ± 0.0573 | 0.5923 ± 0.0801 | 0.6656 ± 0.0839 |
   53 | 0.5917 ± 0.0182 | 0.7125 ± 0.0667 | 0.5890 ± 0.0517 | 0.7000 ± 0.0742 |
   54 | 0.5954 ± 0.0584 | 0.7063 ± 0.0729 | 0.5717 ± 0.0457 | 0.6844 ± 0.0452 |
   55 | 0.5964 ± 0.0550 | 0.6750 ± 0.0673 | 0.6106 ± 0.0886 | 0.7000 ± 0.0805 |
   56 | 0.6547 ± 0.0811 | 0.6188 ± 0.0956 | 0.6463 ± 0.0493 | 0.6094 ± 0.0629 |
   57 | 0.5873 ± 0.0700 | 0.6687 ± 0.1000 | 0.6517 ± 0.0999 | 0.6312 ± 0.1168 |
   58 | 0.6017 ± 0.0602 | 0.6500 ± 0.0637 | 0.5837 ± 0.0617 | 0.7031 ± 0.0991 |
   59 | 0.6709 ± 0.0820 | 0.6375 ± 0.0960 | 0.5699 ± 0.0670 | 0.7125 ± 0.0859 |
   60 | 0.5837 ± 0.0508 | 0.7063 ± 0.0643 | 0.6002 ± 0.0443 | 0.7000 ± 0.0805 |
   61 | 0.5406 ± 0.0651 | 0.7312 ± 0.0643 | 0.5761 ± 0.0548 | 0.7188 ± 0.0640 |
   62 | 0.5748 ± 0.0596 | 0.7312 ± 0.0755 | 0.6058 ± 0.0723 | 0.6875 ± 0.0803 |
   63 | 0.5390 ± 0.0612 | 0.7250 ± 0.0606 | 0.5856 ± 0.0559 | 0.6906 ± 0.0677 |
   64 | 0.5458 ± 0.0351 | 0.7312 ± 0.0508 | 0.6604 ± 0.0594 | 0.5750 ± 0.0702 |
   65 | 0.5920 ± 0.0247 | 0.6937 ± 0.0234 | 0.5790 ± 0.0669 | 0.6906 ± 0.1050 |
   66 | 0.5925 ± 0.0454 | 0.6750 ± 0.0702 | 0.6094 ± 0.0679 | 0.6781 ± 0.0938 |
   67 | 0.5949 ± 0.0509 | 0.6312 ± 0.0364 | 0.5810 ± 0.0623 | 0.7219 ± 0.0973 |
   68 | 0.5894 ± 0.0371 | 0.7125 ± 0.1053 | 0.6379 ± 0.0621 | 0.6250 ± 0.0753 |
   69 | 0.6094 ± 0.0559 | 0.6687 ± 0.0468 | 0.5847 ± 0.0549 | 0.7094 ± 0.0577 |
   70 | 0.5845 ± 0.0927 | 0.6937 ± 0.0996 | 0.6216 ± 0.0573 | 0.6625 ± 0.0696 |
   71 | 0.5797 ± 0.0535 | 0.7063 ± 0.0781 | 0.5721 ± 0.0614 | 0.6844 ± 0.0820 |
   72 | 0.5677 ± 0.0742 | 0.7125 ± 0.0871 | 0.5953 ± 0.0457 | 0.6813 ± 0.0637 |
   73 | 0.5562 ± 0.0644 | 0.6750 ± 0.0829 | 0.6148 ± 0.0830 | 0.6156 ± 0.0803 |
   74 | 0.6476 ± 0.0908 | 0.5625 ± 0.0625 | 0.6126 ± 0.0456 | 0.6469 ± 0.0577 |
   75 | 0.5751 ± 0.0699 | 0.7438 ± 0.0914 | 0.5929 ± 0.0832 | 0.7031 ± 0.0887 |
   76 | 0.5826 ± 0.0718 | 0.7000 ± 0.0508 | 0.6133 ± 0.0537 | 0.6750 ± 0.0715 |
   77 | 0.6285 ± 0.0493 | 0.6875 ± 0.0713 | 0.5739 ± 0.0517 | 0.7219 ± 0.0662 |
   78 | 0.5585 ± 0.0500 | 0.6188 ± 0.0573 | 0.6237 ± 0.0525 | 0.6406 ± 0.0853 |
   79 | 0.6379 ± 0.0709 | 0.6500 ± 0.0935 | 0.5745 ± 0.0629 | 0.7000 ± 0.0658 |
   80 | 0.5877 ± 0.0810 | 0.6562 ± 0.1064 | 0.5904 ± 0.0733 | 0.6813 ± 0.0871 |
   81 | 0.5620 ± 0.0842 | 0.6813 ± 0.1035 | 0.5708 ± 0.0234 | 0.6906 ± 0.0295 |
   82 | 0.6047 ± 0.0720 | 0.6562 ± 0.0740 | 0.5585 ± 0.0607 | 0.7188 ± 0.1017 |
   83 | 0.5585 ± 0.0342 | 0.7063 ± 0.0424 | 0.5961 ± 0.0672 | 0.6906 ± 0.0808 |
   84 | 0.5938 ± 0.0667 | 0.6438 ± 0.1019 | 0.5801 ± 0.0770 | 0.7219 ± 0.0662 |
   85 | 0.5957 ± 0.0535 | 0.7063 ± 0.0580 | 0.6268 ± 0.0375 | 0.6757 ± 0.0764 |
   86 | 0.6235 ± 0.0263 | 0.6562 ± 0.0839 | 0.6250 ± 0.0349 | 0.6469 ± 0.0626 |
   87 | 0.6050 ± 0.0570 | 0.6937 ± 0.0606 | 0.5745 ± 0.0855 | 0.7156 ± 0.1012 |
   88 | 0.5901 ± 0.0384 | 0.7000 ± 0.0250 | 0.5792 ± 0.0479 | 0.6719 ± 0.0629 |
   89 | 0.6278 ± 0.0694 | 0.6813 ± 0.1035 | 0.6142 ± 0.0403 | 0.6531 ± 0.0758 |
   90 | 0.6335 ± 0.1057 | 0.6750 ± 0.1163 | 0.5941 ± 0.0976 | 0.6906 ± 0.1131 |
   91 | 0.5593 ± 0.0279 | 0.7250 ± 0.0234 | 0.6136 ± 0.0615 | 0.6687 ± 0.0742 |
   92 | 0.6368 ± 0.0704 | 0.5813 ± 0.0875 | 0.6280 ± 0.0805 | 0.6687 ± 0.0793 |
   93 | 0.6453 ± 0.1439 | 0.6500 ± 0.1523 | 0.6403 ± 0.0956 | 0.6406 ± 0.0919 |
   94 | 0.5763 ± 0.0628 | 0.6813 ± 0.0723 | 0.5944 ± 0.0541 | 0.6969 ± 0.0371 |
   95 | 0.5977 ± 0.0957 | 0.6813 ± 0.0935 | 0.5813 ± 0.0625 | 0.6719 ± 0.0613 |
   96 | 0.6315 ± 0.0300 | 0.6562 ± 0.0280 | 0.5680 ± 0.0545 | 0.7156 ± 0.0705 |
   97 | 0.5629 ± 0.0796 | 0.7125 ± 0.0723 | 0.5737 ± 0.0672 | 0.7125 ± 0.0573 |
   98 | 0.5947 ± 0.0383 | 0.6750 ± 0.0545 | 0.5771 ± 0.0394 | 0.6844 ± 0.0584 |
   99 | 0.6417 ± 0.0693 | 0.6312 ± 0.0538 | 0.6270 ± 0.0511 | 0.6531 ± 0.0745 |
  100 | 0.5964 ± 0.1031 | 0.7063 ± 0.0940 | 0.5839 ± 0.0585 | 0.7063 ± 0.0817 |
  101 | 0.5923 ± 0.0491 | 0.7063 ± 0.0468 | 0.6162 ± 0.0393 | 0.6844 ± 0.0549 |
  102 | 0.5791 ± 0.0736 | 0.6875 ± 0.0927 | 0.6219 ± 0.0713 | 0.6750 ± 0.0817 |
  103 | 0.6393 ± 0.0763 | 0.6250 ± 0.0948 | 0.5961 ± 0.0674 | 0.7094 ± 0.0713 |
  104 | 0.5599 ± 0.0706 | 0.7438 ± 0.0893 | 0.6189 ± 0.0908 | 0.6844 ± 0.0705 |
  105 | 0.6262 ± 0.0497 | 0.6687 ± 0.0545 | 0.6100 ± 0.0564 | 0.6719 ± 0.0563 |
  106 | 0.5812 ± 0.0386 | 0.7188 ± 0.0442 | 0.6222 ± 0.0675 | 0.6656 ± 0.0740 |
  107 | 0.5673 ± 0.0561 | 0.7375 ± 0.0580 | 0.6030 ± 0.0550 | 0.6625 ± 0.0750 |
  108 | 0.5809 ± 0.0478 | 0.6937 ± 0.0637 | 0.6367 ± 0.0793 | 0.6406 ± 0.0981 |
  109 | 0.6312 ± 0.0533 | 0.6500 ± 0.0848 | 0.6540 ± 0.0812 | 0.6312 ± 0.0925 |
  110 | 0.6281 ± 0.0535 | 0.6750 ± 0.0643 | 0.6093 ± 0.0592 | 0.6687 ± 0.0960 |
  111 | 0.5467 ± 0.0971 | 0.7375 ± 0.1038 | 0.6385 ± 0.1138 | 0.6625 ± 0.1108 |
  112 | 0.6383 ± 0.0847 | 0.6125 ± 0.0940 | 0.5773 ± 0.0509 | 0.7094 ± 0.0862 |
  113 | 0.5802 ± 0.0706 | 0.6750 ± 0.0424 | 0.5980 ± 0.0716 | 0.6813 ± 0.0871 |
  114 | 0.6057 ± 0.0339 | 0.6438 ± 0.0424 | 0.5767 ± 0.0583 | 0.6906 ± 0.0808 |
  115 | 0.5463 ± 0.0471 | 0.7000 ± 0.0673 | 0.5666 ± 0.0614 | 0.7250 ± 0.0848 |
  116 | 0.6447 ± 0.0633 | 0.6188 ± 0.0800 | 0.6240 ± 0.0503 | 0.6469 ± 0.0671 |
  117 | 0.5928 ± 0.0570 | 0.6813 ± 0.0696 | 0.6132 ± 0.0686 | 0.6594 ± 0.0855 |
  118 | 0.5336 ± 0.0604 | 0.7375 ± 0.1212 | 0.6103 ± 0.0674 | 0.6531 ± 0.0719 |
  119 | 0.5707 ± 0.0625 | 0.7063 ± 0.1075 | 0.6207 ± 0.0733 | 0.6312 ± 0.0519 |
  120 | 0.6371 ± 0.0437 | 0.6500 ± 0.0606 | 0.5772 ± 0.0345 | 0.6969 ± 0.0895 |
  121 | 0.5799 ± 0.0402 | 0.6687 ± 0.0829 | 0.5783 ± 0.0637 | 0.6937 ± 0.0763 |
  122 | 0.6212 ± 0.0899 | 0.6562 ± 0.0906 | 0.6102 ± 0.0852 | 0.6594 ± 0.0973 |
  123 | 0.5458 ± 0.0443 | 0.7125 ± 0.0637 | 0.6025 ± 0.0469 | 0.6750 ± 0.0793 |
  124 | 0.6059 ± 0.0677 | 0.6875 ± 0.0948 | 0.5739 ± 0.0486 | 0.6781 ± 0.0485 |
  125 | 0.5712 ± 0.0431 | 0.7188 ± 0.0625 | 0.5862 ± 0.0429 | 0.6875 ± 0.0827 |
  126 | 0.5621 ± 0.0530 | 0.7125 ± 0.0871 | 0.5764 ± 0.0999 | 0.7312 ± 0.0612 |
  127 | 0.5907 ± 0.0406 | 0.7125 ± 0.0234 | 0.5991 ± 0.0462 | 0.6844 ± 0.0677 |
  128 | 0.6495 ± 0.0968 | 0.6312 ± 0.0306 | 0.5694 ± 0.0734 | 0.7344 ± 0.0981 |
  129 | 0.5245 ± 0.0791 | 0.7875 ± 0.0415 | 0.5784 ± 0.0678 | 0.7000 ± 0.0908 |
  130 | 0.6303 ± 0.0587 | 0.6188 ± 0.0800 | 0.5971 ± 0.0691 | 0.6875 ± 0.0625 |
  131 | 0.6022 ± 0.0506 | 0.6125 ± 0.0580 | 0.6421 ± 0.0918 | 0.6062 ± 0.0563 |
  132 | 0.5261 ± 0.0520 | 0.7562 ± 0.0824 | 0.5730 ± 0.0479 | 0.6594 ± 0.0662 |
  133 | 0.5829 ± 0.0363 | 0.6937 ± 0.0538 | 0.5812 ± 0.0671 | 0.6813 ± 0.0556 |
  134 | 0.6327 ± 0.0726 | 0.6125 ± 0.1019 | 0.6142 ± 0.0766 | 0.6781 ± 0.0713 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6725 ± 0.0420 | 0.6438 ± 0.0580 | 0.6289 ± 0.0497 | 0.5906 ± 0.0983 |
    2 | 0.6204 ± 0.1008 | 0.6875 ± 0.1008 | 0.6469 ± 0.0549 | 0.6344 ± 0.0610 |
    3 | 0.6526 ± 0.0321 | 0.5437 ± 0.0805 | 0.6462 ± 0.0173 | 0.6125 ± 0.0527 |
    4 | 0.6025 ± 0.0650 | 0.7000 ± 0.0875 | 0.5956 ± 0.0508 | 0.7188 ± 0.0685 |
    5 | 0.6123 ± 0.0194 | 0.7125 ± 0.0306 | 0.6178 ± 0.0478 | 0.6844 ± 0.0632 |
    6 | 0.6472 ± 0.0967 | 0.5875 ± 0.1053 | 0.6763 ± 0.0501 | 0.5563 ± 0.0538 |
    7 | 0.6799 ± 0.0431 | 0.5687 ± 0.0606 | 0.6173 ± 0.0397 | 0.6875 ± 0.0541 |
    8 | 0.6597 ± 0.0508 | 0.5563 ± 0.1574 | 0.6435 ± 0.0614 | 0.6469 ± 0.1211 |
    9 | 0.5780 ± 0.1101 | 0.7063 ± 0.0755 | 0.6157 ± 0.0845 | 0.6281 ± 0.0452 |
   10 | 0.5960 ± 0.0222 | 0.6937 ± 0.0234 | 0.6310 ± 0.0961 | 0.6844 ± 0.0796 |
   11 | 0.6231 ± 0.0684 | 0.6125 ± 0.0755 | 0.6450 ± 0.0501 | 0.5687 ± 0.0800 |
   12 | 0.6005 ± 0.0268 | 0.6562 ± 0.0523 | 0.5896 ± 0.0592 | 0.6937 ± 0.0871 |
   13 | 0.6540 ± 0.0542 | 0.6500 ± 0.0750 | 0.6353 ± 0.0473 | 0.6438 ± 0.1179 |
   14 | 0.6048 ± 0.0461 | 0.6937 ± 0.0637 | 0.6106 ± 0.0838 | 0.6813 ± 0.0606 |
   15 | 0.6320 ± 0.0433 | 0.6312 ± 0.0696 | 0.5751 ± 0.0509 | 0.7156 ± 0.0796 |
   16 | 0.5992 ± 0.0943 | 0.6687 ± 0.1320 | 0.6621 ± 0.1259 | 0.6719 ± 0.0887 |
   17 | 0.6410 ± 0.0468 | 0.6562 ± 0.0593 | 0.5988 ± 0.0669 | 0.6969 ± 0.0851 |
   18 | 0.6637 ± 0.0310 | 0.6062 ± 0.1093 | 0.5896 ± 0.0608 | 0.6969 ± 0.0727 |
   19 | 0.5958 ± 0.0828 | 0.6875 ± 0.0791 | 0.5754 ± 0.0979 | 0.7344 ± 0.0930 |
   20 | 0.6296 ± 0.0982 | 0.6375 ± 0.1146 | 0.6138 ± 0.1101 | 0.6562 ± 0.0873 |
   21 | 0.6528 ± 0.0313 | 0.6438 ± 0.0319 | 0.5863 ± 0.0688 | 0.6781 ± 0.0465 |
   22 | 0.5840 ± 0.0408 | 0.6937 ± 0.0306 | 0.6113 ± 0.0748 | 0.6750 ± 0.0781 |
   23 | 0.6228 ± 0.0514 | 0.6188 ± 0.1016 | 0.6280 ± 0.0378 | 0.6562 ± 0.0740 |
   24 | 0.5980 ± 0.0197 | 0.7188 ± 0.0625 | 0.5913 ± 0.0708 | 0.7156 ± 0.0820 |
   25 | 0.6139 ± 0.0708 | 0.6562 ± 0.0523 | 0.6215 ± 0.0484 | 0.6813 ± 0.0556 |
   26 | 0.5676 ± 0.0409 | 0.7812 ± 0.0395 | 0.6181 ± 0.0424 | 0.6656 ± 0.0560 |
   27 | 0.5917 ± 0.0578 | 0.6750 ± 0.0580 | 0.6063 ± 0.0612 | 0.6813 ± 0.0848 |
   28 | 0.5244 ± 0.0753 | 0.7625 ± 0.0643 | 0.6224 ± 0.1004 | 0.6594 ± 0.1041 |
   29 | 0.5630 ± 0.0345 | 0.7063 ± 0.0375 | 0.5688 ± 0.0512 | 0.7063 ± 0.0580 |
   30 | 0.6123 ± 0.1168 | 0.7188 ± 0.1008 | 0.6276 ± 0.1017 | 0.6937 ± 0.0904 |
   31 | 0.5862 ± 0.0736 | 0.6937 ± 0.0667 | 0.6406 ± 0.0387 | 0.6281 ± 0.0616 |
   32 | 0.5727 ± 0.0698 | 0.7375 ± 0.0580 | 0.6241 ± 0.0669 | 0.6750 ± 0.0715 |
   33 | 0.6146 ± 0.0451 | 0.6562 ± 0.0740 | 0.5840 ± 0.0864 | 0.6844 ± 0.0832 |
   34 | 0.5571 ± 0.0653 | 0.7375 ± 0.0508 | 0.5786 ± 0.0549 | 0.6969 ± 0.0610 |
   35 | 0.5439 ± 0.0388 | 0.6875 ± 0.0198 | 0.5607 ± 0.0590 | 0.7000 ± 0.0886 |
   36 | 0.6030 ± 0.0602 | 0.6875 ± 0.0342 | 0.6117 ± 0.0806 | 0.6656 ± 0.1018 |
   37 | 0.5792 ± 0.0654 | 0.6875 ± 0.0948 | 0.6302 ± 0.0543 | 0.6312 ± 0.0813 |
   38 | 0.6664 ± 0.0792 | 0.5938 ± 0.1135 | 0.5694 ± 0.0619 | 0.7125 ± 0.0848 |
   39 | 0.5596 ± 0.0604 | 0.6813 ± 0.0637 | 0.5649 ± 0.0424 | 0.7219 ± 0.0295 |
   40 | 0.5989 ± 0.0728 | 0.6250 ± 0.0395 | 0.5728 ± 0.0751 | 0.7063 ± 0.0687 |
   41 | 0.6365 ± 0.0646 | 0.6250 ± 0.0713 | 0.5644 ± 0.0257 | 0.7063 ± 0.0508 |
   42 | 0.5533 ± 0.0577 | 0.7188 ± 0.0815 | 0.5785 ± 0.0555 | 0.7188 ± 0.0576 |
   43 | 0.5648 ± 0.0538 | 0.7188 ± 0.0395 | 0.5546 ± 0.0836 | 0.7375 ± 0.0781 |
   44 | 0.5788 ± 0.0407 | 0.7188 ± 0.0740 | 0.6302 ± 0.1085 | 0.6656 ± 0.0938 |
   45 | 0.6081 ± 0.0933 | 0.7063 ± 0.0852 | 0.6096 ± 0.0554 | 0.6469 ± 0.0542 |
   46 | 0.6013 ± 0.0773 | 0.6562 ± 0.0791 | 0.6182 ± 0.0798 | 0.6531 ± 0.1113 |
   47 | 0.6037 ± 0.0769 | 0.6875 ± 0.0862 | 0.6119 ± 0.1250 | 0.6656 ± 0.1250 |
   48 | 0.6328 ± 0.0743 | 0.6500 ± 0.0637 | 0.5793 ± 0.0426 | 0.7156 ± 0.0493 |
   49 | 0.5622 ± 0.1453 | 0.7438 ± 0.1549 | 0.6065 ± 0.0656 | 0.6906 ± 0.0808 |
   50 | 0.6079 ± 0.0410 | 0.6438 ± 0.0545 | 0.6873 ± 0.0847 | 0.6219 ± 0.0745 |
   51 | 0.6288 ± 0.0285 | 0.6438 ± 0.0250 | 0.5587 ± 0.0852 | 0.7312 ± 0.0628 |
   52 | 0.6018 ± 0.0750 | 0.6687 ± 0.0781 | 0.5828 ± 0.0711 | 0.6781 ± 0.0766 |
   53 | 0.6476 ± 0.0370 | 0.6250 ± 0.0523 | 0.6105 ± 0.0849 | 0.6656 ± 0.1363 |
   54 | 0.6480 ± 0.1049 | 0.6375 ± 0.1196 | 0.6129 ± 0.0907 | 0.6750 ± 0.0715 |
   55 | 0.5980 ± 0.0385 | 0.6687 ± 0.0580 | 0.6017 ± 0.0360 | 0.6719 ± 0.0716 |
   56 | 0.6527 ± 0.0410 | 0.5563 ± 0.1053 | 0.5899 ± 0.0449 | 0.6875 ± 0.0670 |
   57 | 0.6023 ± 0.0372 | 0.6937 ± 0.0306 | 0.6491 ± 0.0721 | 0.6344 ± 0.0779 |
   58 | 0.6060 ± 0.0427 | 0.6687 ± 0.0702 | 0.6282 ± 0.0480 | 0.6531 ± 0.0705 |
   59 | 0.5708 ± 0.0546 | 0.6750 ± 0.0673 | 0.5552 ± 0.0789 | 0.6906 ± 0.1031 |
   60 | 0.6020 ± 0.0918 | 0.6438 ± 0.0424 | 0.6103 ± 0.0886 | 0.6875 ± 0.0873 |
   61 | 0.5871 ± 0.0255 | 0.6937 ± 0.0500 | 0.5829 ± 0.0582 | 0.6937 ± 0.0723 |
   62 | 0.6585 ± 0.1209 | 0.6375 ± 0.0960 | 0.6103 ± 0.0503 | 0.6531 ± 0.0745 |
   63 | 0.6349 ± 0.0685 | 0.6687 ± 0.0829 | 0.6120 ± 0.0617 | 0.6312 ± 0.0859 |
   64 | 0.5399 ± 0.0490 | 0.7188 ± 0.0523 | 0.5900 ± 0.0524 | 0.6781 ± 0.0727 |
   65 | 0.5920 ± 0.0405 | 0.7000 ± 0.0319 | 0.5680 ± 0.0835 | 0.7000 ± 0.0841 |
   66 | 0.5461 ± 0.0897 | 0.6875 ± 0.0988 | 0.5723 ± 0.0946 | 0.6937 ± 0.0966 |
   67 | 0.6036 ± 0.0608 | 0.6687 ± 0.0424 | 0.5943 ± 0.0498 | 0.6625 ± 0.0556 |
   68 | 0.6316 ± 0.0494 | 0.6312 ± 0.0848 | 0.6434 ± 0.0466 | 0.6219 ± 0.0832 |
   69 | 0.5348 ± 0.0632 | 0.7312 ± 0.0940 | 0.5716 ± 0.0399 | 0.7000 ± 0.0508 |
   70 | 0.6012 ± 0.0856 | 0.6687 ± 0.1111 | 0.6048 ± 0.0514 | 0.6594 ± 0.0784 |
   71 | 0.5706 ± 0.0316 | 0.6375 ± 0.0673 | 0.5801 ± 0.0832 | 0.7063 ± 0.0852 |
   72 | 0.6028 ± 0.0766 | 0.6750 ± 0.0897 | 0.5710 ± 0.0475 | 0.7094 ± 0.0641 |
   73 | 0.6011 ± 0.0825 | 0.7000 ± 0.0545 | 0.5556 ± 0.0795 | 0.7281 ± 0.0656 |
   74 | 0.6364 ± 0.0658 | 0.6188 ± 0.0750 | 0.6752 ± 0.0607 | 0.5969 ± 0.0921 |
   75 | 0.5610 ± 0.0440 | 0.7250 ± 0.0776 | 0.5974 ± 0.0655 | 0.6438 ± 0.0742 |
   76 | 0.6196 ± 0.0715 | 0.6375 ± 0.0852 | 0.6227 ± 0.0853 | 0.6188 ± 0.0966 |
   77 | 0.6020 ± 0.0779 | 0.6625 ± 0.0637 | 0.5950 ± 0.0705 | 0.7063 ± 0.0852 |
   78 | 0.5707 ± 0.0542 | 0.6875 ± 0.0656 | 0.6399 ± 0.0807 | 0.6250 ± 0.0884 |
   79 | 0.6254 ± 0.0590 | 0.6625 ± 0.0637 | 0.5912 ± 0.0940 | 0.6562 ± 0.0884 |
   80 | 0.5405 ± 0.0847 | 0.7188 ± 0.0927 | 0.5919 ± 0.0940 | 0.7344 ± 0.0876 |
   81 | 0.5523 ± 0.0571 | 0.7438 ± 0.0637 | 0.6010 ± 0.0872 | 0.6750 ± 0.0768 |
   82 | 0.5554 ± 0.0519 | 0.7375 ± 0.1196 | 0.5734 ± 0.0339 | 0.7094 ± 0.0594 |
   83 | 0.5958 ± 0.0320 | 0.6562 ± 0.0765 | 0.6025 ± 0.0422 | 0.6906 ± 0.0952 |
   84 | 0.6598 ± 0.0480 | 0.6375 ± 0.0702 | 0.6626 ± 0.0721 | 0.6062 ± 0.0897 |
   85 | 0.5523 ± 0.0319 | 0.7312 ± 0.0545 | 0.6321 ± 0.0861 | 0.6581 ± 0.0782 |
   86 | 0.6024 ± 0.1351 | 0.7063 ± 0.0755 | 0.5656 ± 0.0609 | 0.6844 ± 0.0600 |
   87 | 0.5945 ± 0.0392 | 0.6312 ± 0.0538 | 0.6536 ± 0.0684 | 0.6219 ± 0.0647 |
   88 | 0.6405 ± 0.0938 | 0.6562 ± 0.0988 | 0.6000 ± 0.0709 | 0.6906 ± 0.1031 |
   89 | 0.5434 ± 0.0737 | 0.7125 ± 0.0723 | 0.5657 ± 0.0795 | 0.7469 ± 0.0732 |
   90 | 0.5950 ± 0.0935 | 0.6813 ± 0.0750 | 0.5730 ± 0.0621 | 0.6937 ± 0.0682 |
   91 | 0.5397 ± 0.0500 | 0.7875 ± 0.0935 | 0.5738 ± 0.0709 | 0.7250 ± 0.0996 |
   92 | 0.6383 ± 0.0633 | 0.6312 ± 0.0667 | 0.5966 ± 0.0673 | 0.6594 ± 0.1087 |
   93 | 0.6167 ± 0.0650 | 0.6500 ± 0.0723 | 0.5783 ± 0.0494 | 0.6969 ± 0.0671 |
   94 | 0.6226 ± 0.1162 | 0.6813 ± 0.0914 | 0.5768 ± 0.0723 | 0.7031 ± 0.0756 |
   95 | 0.5707 ± 0.0623 | 0.7000 ± 0.0468 | 0.5975 ± 0.0782 | 0.6906 ± 0.0867 |
   96 | 0.6146 ± 0.0543 | 0.6562 ± 0.0559 | 0.5839 ± 0.0494 | 0.7094 ± 0.0727 |
   97 | 0.5370 ± 0.0850 | 0.7125 ± 0.0893 | 0.5496 ± 0.0692 | 0.7125 ± 0.0696 |
   98 | 0.6284 ± 0.0681 | 0.6687 ± 0.0319 | 0.6018 ± 0.1010 | 0.6719 ± 0.1138 |
   99 | 0.5532 ± 0.0543 | 0.7375 ± 0.0612 | 0.5781 ± 0.0808 | 0.6906 ± 0.0921 |
  100 | 0.6110 ± 0.0475 | 0.6500 ± 0.0667 | 0.5415 ± 0.0532 | 0.7063 ± 0.0841 |
  101 | 0.5349 ± 0.0783 | 0.7688 ± 0.0424 | 0.5687 ± 0.0918 | 0.7063 ± 0.0829 |
  102 | 0.5881 ± 0.0312 | 0.6500 ± 0.0500 | 0.6009 ± 0.0434 | 0.6562 ± 0.0685 |
  103 | 0.5655 ± 0.0441 | 0.7375 ± 0.0468 | 0.5364 ± 0.0462 | 0.7312 ± 0.0643 |
  104 | 0.6271 ± 0.0978 | 0.6625 ± 0.0956 | 0.5687 ± 0.0906 | 0.7063 ± 0.0980 |
  105 | 0.5850 ± 0.0560 | 0.6937 ± 0.0824 | 0.5657 ± 0.1035 | 0.6937 ± 0.1272 |
  106 | 0.6336 ± 0.0801 | 0.6438 ± 0.1290 | 0.6648 ± 0.0494 | 0.5563 ± 0.0776 |
  107 | 0.6235 ± 0.1131 | 0.6750 ± 0.0424 | 0.7203 ± 0.1300 | 0.6594 ± 0.0732 |
  108 | 0.5981 ± 0.0549 | 0.6813 ± 0.0415 | 0.6249 ± 0.0824 | 0.6562 ± 0.1046 |
  109 | 0.6097 ± 0.1171 | 0.6375 ± 0.1212 | 0.5795 ± 0.0443 | 0.7000 ± 0.0545 |
  110 | 0.6171 ± 0.0937 | 0.6188 ± 0.1125 | 0.6081 ± 0.0836 | 0.7000 ± 0.0563 |
  111 | 0.5465 ± 0.0710 | 0.7250 ± 0.0723 | 0.6288 ± 0.0470 | 0.6562 ± 0.0593 |
  112 | 0.5634 ± 0.0618 | 0.7125 ± 0.0848 | 0.5900 ± 0.0615 | 0.6906 ± 0.0844 |
  113 | 0.5852 ± 0.0615 | 0.6875 ± 0.0280 | 0.5928 ± 0.0726 | 0.7000 ± 0.0875 |
  114 | 0.5662 ± 0.0308 | 0.6625 ± 0.0306 | 0.5489 ± 0.0560 | 0.7125 ± 0.0573 |
  115 | 0.5360 ± 0.0548 | 0.7500 ± 0.0442 | 0.5901 ± 0.0567 | 0.6906 ± 0.0796 |
  116 | 0.6299 ± 0.0405 | 0.6625 ± 0.0637 | 0.6046 ± 0.0440 | 0.6844 ± 0.0662 |
  117 | 0.5566 ± 0.0581 | 0.7312 ± 0.0729 | 0.6072 ± 0.0808 | 0.6750 ± 0.0793 |
  118 | 0.6586 ± 0.0433 | 0.6062 ± 0.0424 | 0.5749 ± 0.0450 | 0.6906 ± 0.0705 |
  119 | 0.5981 ± 0.0574 | 0.7063 ± 0.0468 | 0.5631 ± 0.0459 | 0.7125 ± 0.0723 |
  120 | 0.5637 ± 0.0932 | 0.7500 ± 0.0791 | 0.6210 ± 0.0858 | 0.6219 ± 0.0732 |
  121 | 0.5894 ± 0.0868 | 0.6813 ± 0.0723 | 0.6051 ± 0.0816 | 0.6750 ± 0.0702 |
  122 | 0.5958 ± 0.0900 | 0.6937 ± 0.0667 | 0.5539 ± 0.0441 | 0.7188 ± 0.0523 |
  123 | 0.5617 ± 0.0646 | 0.6937 ± 0.0606 | 0.5926 ± 0.0973 | 0.6531 ± 0.0784 |
  124 | 0.5748 ± 0.0566 | 0.7125 ± 0.0667 | 0.5559 ± 0.0477 | 0.7281 ± 0.0610 |
  125 | 0.5770 ± 0.1110 | 0.7063 ± 0.1212 | 0.5448 ± 0.0884 | 0.6969 ± 0.0727 |
  126 | 0.5912 ± 0.0946 | 0.6937 ± 0.1072 | 0.5669 ± 0.0370 | 0.6844 ± 0.0295 |
  127 | 0.5669 ± 0.0680 | 0.6937 ± 0.0776 | 0.5442 ± 0.0713 | 0.7375 ± 0.0841 |
  128 | 0.5790 ± 0.0482 | 0.6562 ± 0.0442 | 0.5809 ± 0.0630 | 0.6906 ± 0.0616 |
  129 | 0.5830 ± 0.0426 | 0.6937 ± 0.0364 | 0.6166 ± 0.0909 | 0.6750 ± 0.0742 |
  130 | 0.5567 ± 0.0679 | 0.6937 ± 0.0723 | 0.6370 ± 0.1017 | 0.6719 ± 0.0716 |
  131 | 0.6248 ± 0.0745 | 0.6375 ± 0.0643 | 0.6299 ± 0.1191 | 0.6312 ± 0.1125 |
  132 | 0.6040 ± 0.1084 | 0.6312 ± 0.0956 | 0.5955 ± 0.1217 | 0.6594 ± 0.1222 |
  133 | 0.6438 ± 0.0540 | 0.6562 ± 0.0791 | 0.5765 ± 0.0360 | 0.7250 ± 0.0538 |
  134 | 0.6112 ± 0.0989 | 0.6937 ± 0.1035 | 0.6043 ± 0.0673 | 0.6937 ± 0.0737 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6867 ± 0.0294 | 0.5500 ± 0.0755 | 0.6601 ± 0.0242 | 0.6000 ± 0.0682 |
    2 | 0.6934 ± 0.0109 | 0.5000 ± 0.0765 | 0.6815 ± 0.0062 | 0.6625 ± 0.0667 |
    3 | 0.6337 ± 0.0473 | 0.6500 ± 0.1159 | 0.5966 ± 0.0808 | 0.6562 ± 0.0576 |
    4 | 0.5886 ± 0.0462 | 0.7000 ± 0.0319 | 0.6210 ± 0.0606 | 0.6656 ± 0.0791 |
    5 | 0.5983 ± 0.0520 | 0.7000 ± 0.0545 | 0.6198 ± 0.0921 | 0.6625 ± 0.0925 |
    6 | 0.5990 ± 0.0748 | 0.6750 ± 0.0643 | 0.5969 ± 0.0714 | 0.6969 ± 0.0827 |
    7 | 0.5934 ± 0.0683 | 0.6750 ± 0.0919 | 0.6015 ± 0.0896 | 0.6906 ± 0.0719 |
    8 | 0.6141 ± 0.0612 | 0.6813 ± 0.0606 | 0.6195 ± 0.0428 | 0.6250 ± 0.0791 |
    9 | 0.6202 ± 0.0961 | 0.6375 ± 0.1539 | 0.6273 ± 0.0650 | 0.6625 ± 0.0813 |
   10 | 0.6303 ± 0.0679 | 0.6125 ± 0.1111 | 0.6875 ± 0.0840 | 0.6250 ± 0.1036 |
   11 | 0.6242 ± 0.0292 | 0.6312 ± 0.0696 | 0.6209 ± 0.0549 | 0.6844 ± 0.0820 |
   12 | 0.6083 ± 0.0552 | 0.6813 ± 0.0776 | 0.6091 ± 0.0840 | 0.7188 ± 0.0765 |
   13 | 0.6031 ± 0.0285 | 0.7312 ± 0.0673 | 0.5967 ± 0.0408 | 0.6969 ± 0.0685 |
   14 | 0.6477 ± 0.0949 | 0.6312 ± 0.1497 | 0.6274 ± 0.0599 | 0.6531 ± 0.0647 |
   15 | 0.5395 ± 0.0699 | 0.7438 ± 0.0723 | 0.6195 ± 0.0622 | 0.6719 ± 0.0659 |
   16 | 0.5619 ± 0.0530 | 0.7063 ± 0.0755 | 0.6013 ± 0.0508 | 0.6687 ± 0.0687 |
   17 | 0.5610 ± 0.0676 | 0.6750 ± 0.1212 | 0.6318 ± 0.0853 | 0.6500 ± 0.0813 |
   18 | 0.6177 ± 0.0254 | 0.6250 ± 0.0713 | 0.6227 ± 0.0344 | 0.6469 ± 0.0524 |
   19 | 0.6139 ± 0.0117 | 0.6875 ± 0.0395 | 0.5938 ± 0.0512 | 0.7031 ± 0.0806 |
   20 | 0.6138 ± 0.0629 | 0.6062 ± 0.1057 | 0.6015 ± 0.0529 | 0.6000 ± 0.0935 |
   21 | 0.6148 ± 0.0539 | 0.6562 ± 0.0988 | 0.6151 ± 0.0334 | 0.6469 ± 0.0713 |
   22 | 0.6230 ± 0.0657 | 0.6625 ± 0.0956 | 0.5569 ± 0.0613 | 0.7156 ± 0.0719 |
   23 | 0.5409 ± 0.0487 | 0.7250 ± 0.0573 | 0.5945 ± 0.0630 | 0.6844 ± 0.0493 |
   24 | 0.6360 ± 0.0847 | 0.6125 ± 0.0702 | 0.6187 ± 0.0670 | 0.5906 ± 0.0531 |
   25 | 0.5986 ± 0.0550 | 0.6813 ± 0.0723 | 0.5710 ± 0.0425 | 0.7094 ± 0.0420 |
   26 | 0.5521 ± 0.0514 | 0.7000 ± 0.0852 | 0.5655 ± 0.0900 | 0.6969 ± 0.0948 |
   27 | 0.5605 ± 0.0550 | 0.7188 ± 0.0765 | 0.6088 ± 0.1107 | 0.6937 ± 0.1090 |
   28 | 0.6580 ± 0.0468 | 0.6062 ± 0.0897 | 0.6088 ± 0.0312 | 0.6906 ± 0.0855 |
   29 | 0.5464 ± 0.0340 | 0.7063 ± 0.0755 | 0.6245 ± 0.0582 | 0.6281 ± 0.0855 |
   30 | 0.5849 ± 0.0617 | 0.6813 ± 0.0637 | 0.6130 ± 0.0781 | 0.6687 ± 0.0768 |
   31 | 0.5545 ± 0.0409 | 0.7250 ± 0.1176 | 0.5838 ± 0.0541 | 0.6969 ± 0.0505 |
   32 | 0.5853 ± 0.0325 | 0.7063 ± 0.0319 | 0.5567 ± 0.0893 | 0.7312 ± 0.0628 |
   33 | 0.5534 ± 0.0579 | 0.7125 ± 0.0538 | 0.5624 ± 0.0835 | 0.7125 ± 0.0750 |
   34 | 0.5569 ± 0.0812 | 0.7250 ± 0.0871 | 0.5706 ± 0.0919 | 0.6937 ± 0.0836 |
   35 | 0.5930 ± 0.0749 | 0.6937 ± 0.1035 | 0.6468 ± 0.1047 | 0.6281 ± 0.0932 |
   36 | 0.5730 ± 0.0960 | 0.7063 ± 0.1019 | 0.6218 ± 0.0592 | 0.6687 ± 0.0563 |
   37 | 0.6674 ± 0.1226 | 0.6438 ± 0.1460 | 0.6112 ± 0.0710 | 0.6594 ± 0.0691 |
   38 | 0.5601 ± 0.0333 | 0.6937 ± 0.0667 | 0.6237 ± 0.0783 | 0.6406 ± 0.0864 |
   39 | 0.5476 ± 0.0259 | 0.7125 ± 0.0234 | 0.5767 ± 0.0844 | 0.6969 ± 0.0740 |
   40 | 0.6184 ± 0.0855 | 0.6875 ± 0.0815 | 0.5957 ± 0.0625 | 0.6750 ± 0.0715 |
   41 | 0.6085 ± 0.0525 | 0.6625 ± 0.0637 | 0.5844 ± 0.0426 | 0.7156 ± 0.0549 |
   42 | 0.5979 ± 0.0951 | 0.7188 ± 0.1135 | 0.5597 ± 0.0864 | 0.7063 ± 0.0829 |
   43 | 0.5887 ± 0.0505 | 0.7312 ± 0.0319 | 0.5532 ± 0.0829 | 0.7312 ± 0.0612 |
   44 | 0.5968 ± 0.0245 | 0.6687 ± 0.0250 | 0.5988 ± 0.0864 | 0.7031 ± 0.0794 |
   45 | 0.6342 ± 0.0604 | 0.6438 ± 0.0545 | 0.5611 ± 0.0511 | 0.7156 ± 0.0549 |
   46 | 0.6768 ± 0.0526 | 0.5938 ± 0.0685 | 0.5942 ± 0.0286 | 0.7031 ± 0.0806 |
   47 | 0.5912 ± 0.0742 | 0.6937 ± 0.0776 | 0.6437 ± 0.0400 | 0.5406 ± 0.0753 |
   48 | 0.6018 ± 0.0816 | 0.6687 ± 0.0755 | 0.5554 ± 0.0744 | 0.7031 ± 0.0830 |
   49 | 0.5520 ± 0.0522 | 0.7000 ± 0.0781 | 0.6053 ± 0.0786 | 0.6438 ± 0.0864 |
   50 | 0.5803 ± 0.0583 | 0.7125 ± 0.0776 | 0.6493 ± 0.0615 | 0.6344 ± 0.0560 |
   51 | 0.5315 ± 0.0683 | 0.7500 ± 0.0442 | 0.5666 ± 0.0315 | 0.7125 ± 0.0337 |
   52 | 0.6151 ± 0.0616 | 0.6687 ± 0.0424 | 0.6080 ± 0.0447 | 0.6531 ± 0.0691 |
   53 | 0.5555 ± 0.0804 | 0.7125 ± 0.0976 | 0.6009 ± 0.0759 | 0.6813 ± 0.1192 |
   54 | 0.5896 ± 0.0521 | 0.6625 ± 0.0364 | 0.6051 ± 0.0535 | 0.6656 ± 0.0594 |
   55 | 0.5622 ± 0.0722 | 0.7000 ± 0.1163 | 0.5699 ± 0.0602 | 0.7094 ± 0.0485 |
   56 | 0.5668 ± 0.0787 | 0.7188 ± 0.0815 | 0.5521 ± 0.0501 | 0.7125 ± 0.0622 |
   57 | 0.5523 ± 0.1229 | 0.7000 ± 0.1163 | 0.5545 ± 0.0559 | 0.7250 ± 0.0573 |
   58 | 0.5485 ± 0.0236 | 0.6687 ± 0.0319 | 0.6208 ± 0.0797 | 0.6469 ± 0.0862 |
   59 | 0.6026 ± 0.0711 | 0.6813 ± 0.0750 | 0.5997 ± 0.0433 | 0.6656 ± 0.0594 |
   60 | 0.5368 ± 0.0447 | 0.6562 ± 0.1266 | 0.5971 ± 0.0757 | 0.6531 ± 0.0719 |
   61 | 0.5608 ± 0.0448 | 0.7063 ± 0.0250 | 0.6064 ± 0.0543 | 0.6687 ± 0.0468 |
   62 | 0.5618 ± 0.0433 | 0.7063 ± 0.0319 | 0.5892 ± 0.0855 | 0.6937 ± 0.0914 |
   63 | 0.5491 ± 0.0598 | 0.7312 ± 0.0424 | 0.5772 ± 0.0559 | 0.7094 ± 0.0344 |
   64 | 0.5878 ± 0.0536 | 0.6750 ± 0.0702 | 0.5943 ± 0.0870 | 0.7031 ± 0.0908 |
   65 | 0.5692 ± 0.0406 | 0.6937 ± 0.0415 | 0.5649 ± 0.0299 | 0.7219 ± 0.0632 |
   66 | 0.5951 ± 0.0805 | 0.6875 ± 0.1083 | 0.5802 ± 0.0491 | 0.6906 ± 0.0549 |
   67 | 0.5217 ± 0.0634 | 0.7500 ± 0.0559 | 0.5534 ± 0.0885 | 0.7094 ± 0.0928 |
   68 | 0.5972 ± 0.0150 | 0.6750 ± 0.0153 | 0.6160 ± 0.0567 | 0.6406 ± 0.0702 |
   69 | 0.5298 ± 0.0420 | 0.7250 ± 0.0606 | 0.5690 ± 0.0722 | 0.6937 ± 0.0813 |
   70 | 0.5732 ± 0.0939 | 0.6750 ± 0.1093 | 0.6043 ± 0.0622 | 0.7094 ± 0.0610 |
   71 | 0.5168 ± 0.0385 | 0.7000 ± 0.0545 | 0.6085 ± 0.0595 | 0.6844 ± 0.0632 |
   72 | 0.5489 ± 0.0615 | 0.6875 ± 0.0656 | 0.6032 ± 0.0996 | 0.6719 ± 0.1085 |
   73 | 0.6137 ± 0.0288 | 0.6375 ± 0.0319 | 0.6460 ± 0.0658 | 0.5687 ± 0.0696 |
   74 | 0.5878 ± 0.0777 | 0.6875 ± 0.0593 | 0.6007 ± 0.0673 | 0.6813 ± 0.0836 |
   75 | 0.5872 ± 0.0963 | 0.6625 ± 0.1159 | 0.5677 ± 0.0765 | 0.7125 ± 0.0637 |
   76 | 0.5592 ± 0.0849 | 0.7125 ± 0.1090 | 0.5989 ± 0.0547 | 0.6687 ± 0.0612 |
   77 | 0.5890 ± 0.0889 | 0.7000 ± 0.0919 | 0.5935 ± 0.0723 | 0.6813 ± 0.0667 |
   78 | 0.6187 ± 0.1366 | 0.6500 ± 0.1497 | 0.6073 ± 0.0423 | 0.6719 ± 0.0563 |
   79 | 0.5509 ± 0.0895 | 0.7000 ± 0.0852 | 0.5782 ± 0.1284 | 0.7063 ± 0.1137 |
   80 | 0.5908 ± 0.0771 | 0.7000 ± 0.0508 | 0.5900 ± 0.0366 | 0.6781 ± 0.0314 |
   81 | 0.5892 ± 0.0580 | 0.6687 ± 0.0673 | 0.5545 ± 0.0403 | 0.7000 ± 0.0817 |
   82 | 0.5536 ± 0.0601 | 0.7312 ± 0.0643 | 0.5746 ± 0.0742 | 0.6781 ± 0.0791 |
   83 | 0.6105 ± 0.0533 | 0.6438 ± 0.0829 | 0.5920 ± 0.0373 | 0.6375 ± 0.0929 |
   84 | 0.5453 ± 0.0729 | 0.7312 ± 0.0424 | 0.5750 ± 0.0561 | 0.6875 ± 0.0625 |
   85 | 0.5551 ± 0.0612 | 0.7625 ± 0.0805 | 0.5958 ± 0.0639 | 0.6643 ± 0.0640 |
   86 | 0.6298 ± 0.1285 | 0.6687 ± 0.0805 | 0.5734 ± 0.1008 | 0.6937 ± 0.0696 |
   87 | 0.5515 ± 0.0526 | 0.6937 ± 0.0606 | 0.5896 ± 0.1061 | 0.7000 ± 0.0886 |
   88 | 0.5507 ± 0.0492 | 0.7312 ± 0.0729 | 0.5372 ± 0.0639 | 0.7219 ± 0.0771 |
   89 | 0.5792 ± 0.0484 | 0.6937 ± 0.0800 | 0.5981 ± 0.0664 | 0.6625 ± 0.1006 |
   90 | 0.5841 ± 0.0338 | 0.7375 ± 0.0702 | 0.5707 ± 0.0561 | 0.7281 ± 0.0884 |
   91 | 0.5959 ± 0.0613 | 0.6687 ± 0.0508 | 0.5990 ± 0.0800 | 0.6937 ± 0.0914 |
   92 | 0.5532 ± 0.0361 | 0.7125 ± 0.0234 | 0.5574 ± 0.0490 | 0.6937 ± 0.0723 |
   93 | 0.5976 ± 0.0402 | 0.6625 ± 0.0364 | 0.5953 ± 0.0557 | 0.6687 ± 0.0886 |
   94 | 0.5737 ± 0.0774 | 0.7188 ± 0.0740 | 0.5862 ± 0.0588 | 0.6750 ± 0.0864 |
   95 | 0.5842 ± 0.0760 | 0.6312 ± 0.0637 | 0.5961 ± 0.0637 | 0.6469 ± 0.0862 |
   96 | 0.5355 ± 0.0802 | 0.7375 ± 0.0468 | 0.5875 ± 0.1125 | 0.7156 ± 0.0932 |
   97 | 0.5771 ± 0.0322 | 0.6813 ± 0.0800 | 0.5922 ± 0.0377 | 0.6875 ± 0.0699 |
   98 | 0.5853 ± 0.0418 | 0.7000 ± 0.0612 | 0.6222 ± 0.0829 | 0.6750 ± 0.0793 |
   99 | 0.6134 ± 0.1006 | 0.6750 ± 0.1259 | 0.5839 ± 0.0477 | 0.6750 ± 0.0612 |
  100 | 0.5888 ± 0.0496 | 0.6438 ± 0.0424 | 0.6065 ± 0.0524 | 0.6656 ± 0.0656 |
  101 | 0.6012 ± 0.0809 | 0.6875 ± 0.1027 | 0.5910 ± 0.0669 | 0.6781 ± 0.0685 |
  102 | 0.6524 ± 0.0840 | 0.6375 ± 0.0643 | 0.6060 ± 0.0849 | 0.6469 ± 0.1037 |
  103 | 0.5199 ± 0.0893 | 0.7312 ± 0.0960 | 0.5645 ± 0.0495 | 0.7063 ± 0.0527 |
  104 | 0.6306 ± 0.0676 | 0.6375 ± 0.0508 | 0.5557 ± 0.0625 | 0.7188 ± 0.0640 |
  105 | 0.6122 ± 0.0922 | 0.6750 ± 0.0729 | 0.5780 ± 0.0416 | 0.6813 ± 0.0750 |
  106 | 0.5496 ± 0.0688 | 0.7188 ± 0.0523 | 0.5922 ± 0.0708 | 0.6750 ± 0.0643 |
  107 | 0.5500 ± 0.0438 | 0.7063 ± 0.0643 | 0.5372 ± 0.0521 | 0.7312 ± 0.0563 |
  108 | 0.6159 ± 0.0322 | 0.6250 ± 0.0395 | 0.5948 ± 0.0436 | 0.6469 ± 0.0443 |
  109 | 0.5246 ± 0.0615 | 0.7312 ± 0.0319 | 0.5650 ± 0.0907 | 0.7031 ± 0.0730 |
  110 | 0.5550 ± 0.1180 | 0.7125 ± 0.1287 | 0.5370 ± 0.0672 | 0.7250 ± 0.0606 |
  111 | 0.5472 ± 0.0612 | 0.7500 ± 0.0484 | 0.5562 ± 0.0939 | 0.7219 ± 0.0855 |
  112 | 0.5715 ± 0.0903 | 0.7125 ± 0.0606 | 0.6060 ± 0.0767 | 0.6781 ± 0.0803 |
  113 | 0.5638 ± 0.0971 | 0.7188 ± 0.0948 | 0.5714 ± 0.0478 | 0.7094 ± 0.0420 |
  114 | 0.5809 ± 0.0642 | 0.7312 ± 0.0580 | 0.6203 ± 0.0974 | 0.6906 ± 0.0932 |
  115 | 0.5544 ± 0.0694 | 0.7063 ± 0.0755 | 0.6166 ± 0.1088 | 0.6969 ± 0.0727 |
  116 | 0.5318 ± 0.0693 | 0.7375 ± 0.1019 | 0.5377 ± 0.1032 | 0.7344 ± 0.0887 |
  117 | 0.6110 ± 0.0190 | 0.6438 ± 0.0424 | 0.5511 ± 0.0620 | 0.7031 ± 0.0613 |
  118 | 0.5575 ± 0.0708 | 0.7063 ± 0.0424 | 0.5759 ± 0.0767 | 0.6813 ± 0.0763 |
  119 | 0.4978 ± 0.0652 | 0.7937 ± 0.0319 | 0.6121 ± 0.1355 | 0.6906 ± 0.1059 |
  120 | 0.6179 ± 0.0943 | 0.6250 ± 0.1266 | 0.5939 ± 0.0810 | 0.6719 ± 0.1103 |
  121 | 0.6096 ± 0.0173 | 0.7125 ± 0.0538 | 0.6433 ± 0.0464 | 0.6281 ± 0.0662 |
  122 | 0.6400 ± 0.1052 | 0.6062 ± 0.0852 | 0.6045 ± 0.1018 | 0.6562 ± 0.0895 |
  123 | 0.5298 ± 0.0786 | 0.7562 ± 0.0606 | 0.6831 ± 0.1308 | 0.6531 ± 0.0867 |
  124 | 0.5858 ± 0.0370 | 0.6500 ± 0.0696 | 0.6151 ± 0.0341 | 0.6687 ± 0.0841 |
  125 | 0.5181 ± 0.0423 | 0.7438 ± 0.0606 | 0.5775 ± 0.0699 | 0.6906 ± 0.0719 |
  126 | 0.6429 ± 0.1193 | 0.6375 ± 0.0643 | 0.5660 ± 0.0543 | 0.7281 ± 0.0740 |
  127 | 0.5863 ± 0.0490 | 0.6625 ± 0.0234 | 0.5642 ± 0.0743 | 0.6937 ± 0.0776 |
  128 | 0.5637 ± 0.1012 | 0.7125 ± 0.1125 | 0.5609 ± 0.0759 | 0.7344 ± 0.0702 |
  129 | 0.5964 ± 0.0720 | 0.6875 ± 0.0862 | 0.5447 ± 0.0538 | 0.7094 ± 0.0594 |
  130 | 0.5679 ± 0.0299 | 0.6813 ± 0.0364 | 0.5553 ± 0.0653 | 0.6750 ± 0.0658 |
  131 | 0.5355 ± 0.0525 | 0.7188 ± 0.0656 | 0.5846 ± 0.0678 | 0.7094 ± 0.0656 |
  132 | 0.6381 ± 0.0568 | 0.6125 ± 0.0940 | 0.5987 ± 0.0525 | 0.6844 ± 0.0784 |
  133 | 0.6400 ± 0.0726 | 0.6000 ± 0.1090 | 0.5673 ± 0.0889 | 0.6750 ± 0.0990 |
  134 | 0.5572 ± 0.1103 | 0.7000 ± 0.0960 | 0.5855 ± 0.0846 | 0.6719 ± 0.0981 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6819 ± 0.0295 | 0.5250 ± 0.1035 | 0.6684 ± 0.0324 | 0.6000 ± 0.0893 |
    2 | 0.6403 ± 0.0306 | 0.6750 ± 0.0580 | 0.6540 ± 0.0391 | 0.6375 ± 0.0793 |
    3 | 0.6350 ± 0.0525 | 0.6500 ± 0.0667 | 0.6367 ± 0.0723 | 0.6594 ± 0.0705 |
    4 | 0.6030 ± 0.0376 | 0.6875 ± 0.0442 | 0.6213 ± 0.0836 | 0.7000 ± 0.0715 |
    5 | 0.5989 ± 0.0607 | 0.6687 ± 0.0375 | 0.6467 ± 0.0350 | 0.6000 ± 0.0776 |
    6 | 0.5856 ± 0.0480 | 0.7438 ± 0.0667 | 0.6318 ± 0.0462 | 0.6562 ± 0.0419 |
    7 | 0.6137 ± 0.0327 | 0.6438 ± 0.0580 | 0.6245 ± 0.0539 | 0.6562 ± 0.0884 |
    8 | 0.6289 ± 0.0261 | 0.6312 ± 0.0848 | 0.5798 ± 0.0309 | 0.7000 ± 0.0545 |
    9 | 0.5990 ± 0.0728 | 0.6813 ± 0.1272 | 0.5889 ± 0.0601 | 0.7000 ± 0.0348 |
   10 | 0.6238 ± 0.0832 | 0.6813 ± 0.1053 | 0.6973 ± 0.0938 | 0.6562 ± 0.0791 |
   11 | 0.5870 ± 0.0438 | 0.7250 ± 0.0696 | 0.5749 ± 0.1076 | 0.7156 ± 0.0963 |
   12 | 0.6636 ± 0.0807 | 0.6188 ± 0.1159 | 0.6070 ± 0.0371 | 0.6500 ± 0.0556 |
   13 | 0.6148 ± 0.0674 | 0.7063 ± 0.0755 | 0.5797 ± 0.0684 | 0.7063 ± 0.0742 |
   14 | 0.6064 ± 0.0674 | 0.6750 ± 0.0729 | 0.6152 ± 0.0583 | 0.6656 ± 0.0753 |
   15 | 0.5870 ± 0.0625 | 0.6687 ± 0.0980 | 0.5992 ± 0.0604 | 0.6781 ± 0.0766 |
   16 | 0.6582 ± 0.0912 | 0.6500 ± 0.0667 | 0.5845 ± 0.1073 | 0.7000 ± 0.1038 |
   17 | 0.6194 ± 0.0308 | 0.6562 ± 0.0442 | 0.5939 ± 0.0489 | 0.6719 ± 0.0688 |
   18 | 0.6562 ± 0.0450 | 0.6125 ± 0.0673 | 0.6025 ± 0.0868 | 0.6813 ± 0.0986 |
   19 | 0.6299 ± 0.0377 | 0.6687 ± 0.0580 | 0.5946 ± 0.0584 | 0.6844 ± 0.0921 |
   20 | 0.5682 ± 0.0615 | 0.7125 ± 0.0750 | 0.6082 ± 0.0503 | 0.7000 ± 0.0729 |
   21 | 0.5816 ± 0.0410 | 0.6937 ± 0.0606 | 0.5724 ± 0.0583 | 0.7250 ± 0.0788 |
   22 | 0.6318 ± 0.0431 | 0.6438 ± 0.0319 | 0.5785 ± 0.0471 | 0.7219 ± 0.0900 |
   23 | 0.5948 ± 0.0469 | 0.6438 ± 0.0580 | 0.6012 ± 0.0703 | 0.6875 ± 0.0593 |
   24 | 0.6446 ± 0.1464 | 0.6813 ± 0.1287 | 0.6140 ± 0.0661 | 0.6687 ± 0.0793 |
   25 | 0.5797 ± 0.0935 | 0.7188 ± 0.1118 | 0.6026 ± 0.0518 | 0.6937 ± 0.0459 |
   26 | 0.5644 ± 0.0567 | 0.6937 ± 0.0573 | 0.5929 ± 0.0410 | 0.7063 ± 0.0702 |
   27 | 0.6197 ± 0.0719 | 0.6312 ± 0.0723 | 0.6472 ± 0.0529 | 0.5563 ± 0.0519 |
   28 | 0.6096 ± 0.0509 | 0.6750 ± 0.0673 | 0.6041 ± 0.0558 | 0.6969 ± 0.0594 |
   29 | 0.6479 ± 0.0560 | 0.5625 ± 0.0280 | 0.6291 ± 0.0532 | 0.6375 ± 0.0687 |
   30 | 0.5733 ± 0.0835 | 0.7063 ± 0.0940 | 0.6184 ± 0.0378 | 0.6562 ± 0.0778 |
   31 | 0.5792 ± 0.0503 | 0.6813 ± 0.0538 | 0.5649 ± 0.0811 | 0.7188 ± 0.0850 |
   32 | 0.5923 ± 0.0444 | 0.7188 ± 0.0523 | 0.6092 ± 0.0995 | 0.6813 ± 0.0946 |
   33 | 0.6140 ± 0.0301 | 0.6687 ± 0.0319 | 0.6299 ± 0.0577 | 0.6750 ± 0.0768 |
   34 | 0.5427 ± 0.0361 | 0.7312 ± 0.0673 | 0.6174 ± 0.0521 | 0.6625 ± 0.0653 |
   35 | 0.6018 ± 0.0703 | 0.6875 ± 0.0988 | 0.6130 ± 0.0857 | 0.5938 ± 0.1118 |
   36 | 0.5957 ± 0.0339 | 0.7000 ± 0.0468 | 0.6042 ± 0.0538 | 0.6719 ± 0.0743 |
   37 | 0.6720 ± 0.0703 | 0.6062 ± 0.0755 | 0.6130 ± 0.0631 | 0.6719 ± 0.0853 |
   38 | 0.5981 ± 0.0439 | 0.6813 ± 0.0667 | 0.5919 ± 0.0714 | 0.6813 ± 0.0935 |
   39 | 0.5766 ± 0.0403 | 0.6937 ± 0.0573 | 0.5836 ± 0.0541 | 0.6969 ± 0.0803 |
   40 | 0.6490 ± 0.0515 | 0.5875 ± 0.0415 | 0.5986 ± 0.0405 | 0.6656 ± 0.0938 |
   41 | 0.6272 ± 0.0616 | 0.6438 ± 0.1057 | 0.6152 ± 0.0572 | 0.6781 ± 0.0791 |
   42 | 0.6003 ± 0.0515 | 0.6750 ± 0.0545 | 0.5887 ± 0.0517 | 0.6875 ± 0.0815 |
   43 | 0.5980 ± 0.0661 | 0.6813 ± 0.0606 | 0.6047 ± 0.0640 | 0.6875 ± 0.0827 |
   44 | 0.6100 ± 0.0466 | 0.6625 ± 0.0696 | 0.5767 ± 0.0426 | 0.7125 ± 0.0763 |
   45 | 0.6295 ± 0.0845 | 0.6687 ± 0.0702 | 0.5964 ± 0.0643 | 0.6687 ± 0.0897 |
   46 | 0.5739 ± 0.0708 | 0.7063 ± 0.0852 | 0.6328 ± 0.0393 | 0.6469 ± 0.0485 |
   47 | 0.6001 ± 0.0673 | 0.6813 ± 0.0776 | 0.6035 ± 0.0546 | 0.6750 ± 0.0742 |
   48 | 0.6188 ± 0.0485 | 0.6813 ± 0.0871 | 0.6031 ± 0.0599 | 0.6781 ± 0.0641 |
   49 | 0.5669 ± 0.0506 | 0.7500 ± 0.0442 | 0.5815 ± 0.0459 | 0.7094 ± 0.0656 |
   50 | 0.6234 ± 0.0431 | 0.6250 ± 0.0713 | 0.6297 ± 0.0651 | 0.6375 ± 0.0929 |
   51 | 0.5896 ± 0.0270 | 0.6500 ± 0.0538 | 0.5954 ± 0.0595 | 0.6375 ± 0.0805 |
   52 | 0.5857 ± 0.0413 | 0.6687 ± 0.0580 | 0.6211 ± 0.0529 | 0.6625 ± 0.0763 |
   53 | 0.6116 ± 0.0537 | 0.7000 ± 0.0805 | 0.5908 ± 0.0510 | 0.6906 ± 0.0900 |
   54 | 0.6080 ± 0.0559 | 0.6687 ± 0.0508 | 0.5972 ± 0.0529 | 0.6906 ± 0.0600 |
   55 | 0.6202 ± 0.0496 | 0.6750 ± 0.1075 | 0.6502 ± 0.0615 | 0.6406 ± 0.0702 |
   56 | 0.6112 ± 0.0475 | 0.6375 ± 0.0612 | 0.6273 ± 0.0432 | 0.6062 ± 0.0643 |
   57 | 0.5922 ± 0.0226 | 0.6500 ± 0.0538 | 0.5801 ± 0.0531 | 0.7000 ± 0.0950 |
   58 | 0.5875 ± 0.0743 | 0.6687 ± 0.0702 | 0.6095 ± 0.0503 | 0.6719 ± 0.0597 |
   59 | 0.6340 ± 0.0405 | 0.6750 ± 0.0508 | 0.6275 ± 0.0741 | 0.6562 ± 0.0699 |
   60 | 0.6292 ± 0.0289 | 0.6625 ± 0.0538 | 0.6105 ± 0.0563 | 0.6781 ± 0.0753 |
   61 | 0.6206 ± 0.0806 | 0.6500 ± 0.0667 | 0.6228 ± 0.0537 | 0.6438 ± 0.0805 |
   62 | 0.5956 ± 0.0289 | 0.6750 ± 0.0702 | 0.5856 ± 0.0330 | 0.6719 ± 0.0469 |
   63 | 0.5991 ± 0.0558 | 0.6625 ± 0.0914 | 0.6104 ± 0.0637 | 0.6625 ± 0.0904 |
   64 | 0.6206 ± 0.0518 | 0.6562 ± 0.0713 | 0.6099 ± 0.0402 | 0.6813 ± 0.0415 |
   65 | 0.5872 ± 0.0317 | 0.6813 ± 0.0723 | 0.5899 ± 0.0514 | 0.6813 ± 0.0606 |
   66 | 0.6137 ± 0.0549 | 0.6687 ± 0.0852 | 0.5967 ± 0.0830 | 0.6344 ± 0.0839 |
   67 | 0.6437 ± 0.0424 | 0.6062 ± 0.0424 | 0.5767 ± 0.0701 | 0.7094 ± 0.0959 |
   68 | 0.5627 ± 0.0339 | 0.7188 ± 0.0884 | 0.6250 ± 0.0588 | 0.6750 ± 0.0468 |
   69 | 0.5979 ± 0.0444 | 0.6312 ± 0.0459 | 0.5966 ± 0.0576 | 0.6438 ± 0.1163 |
   70 | 0.5805 ± 0.0706 | 0.7125 ± 0.0500 | 0.5820 ± 0.0579 | 0.6969 ± 0.0851 |
   71 | 0.6247 ± 0.0626 | 0.6562 ± 0.0713 | 0.6330 ± 0.0618 | 0.6625 ± 0.0800 |
   72 | 0.5932 ± 0.0544 | 0.6687 ± 0.1057 | 0.5957 ± 0.0398 | 0.6719 ± 0.0613 |
   73 | 0.5930 ± 0.0452 | 0.6250 ± 0.0523 | 0.5991 ± 0.0817 | 0.6281 ± 0.1261 |
   74 | 0.6405 ± 0.0507 | 0.6250 ± 0.0342 | 0.5914 ± 0.0726 | 0.7219 ± 0.0820 |
   75 | 0.6287 ± 0.0883 | 0.6750 ± 0.0545 | 0.5697 ± 0.0585 | 0.7219 ± 0.0647 |
   76 | 0.5929 ± 0.0490 | 0.7063 ± 0.0673 | 0.5811 ± 0.0699 | 0.7031 ± 0.0981 |
   77 | 0.6012 ± 0.0785 | 0.6188 ± 0.0637 | 0.5705 ± 0.0371 | 0.7031 ± 0.0447 |
   78 | 0.5863 ± 0.0616 | 0.6875 ± 0.0839 | 0.5876 ± 0.0620 | 0.7063 ± 0.0755 |
   79 | 0.5739 ± 0.0558 | 0.7063 ± 0.0468 | 0.6080 ± 0.0630 | 0.6687 ± 0.0545 |
   80 | 0.5734 ± 0.0289 | 0.7562 ± 0.0234 | 0.6054 ± 0.0657 | 0.6813 ± 0.0723 |
   81 | 0.6190 ± 0.0769 | 0.7000 ± 0.0612 | 0.5720 ± 0.0621 | 0.7312 ± 0.0715 |
   82 | 0.6566 ± 0.0703 | 0.6188 ± 0.0776 | 0.6111 ± 0.0537 | 0.7094 ± 0.0753 |
   83 | 0.5742 ± 0.0229 | 0.7000 ± 0.0755 | 0.6240 ± 0.1087 | 0.6438 ± 0.1228 |
   84 | 0.6723 ± 0.0355 | 0.6813 ± 0.0306 | 0.6790 ± 0.0847 | 0.6625 ± 0.0800 |
   85 | 0.5838 ± 0.0487 | 0.7000 ± 0.0729 | 0.5679 ± 0.0629 | 0.7196 ± 0.1051 |
   86 | 0.5948 ± 0.0573 | 0.6813 ± 0.0776 | 0.6244 ± 0.0647 | 0.6219 ± 0.0844 |
   87 | 0.6177 ± 0.0467 | 0.6438 ± 0.0580 | 0.5898 ± 0.0593 | 0.6750 ± 0.0768 |
   88 | 0.5669 ± 0.0983 | 0.7063 ± 0.0852 | 0.6231 ± 0.0953 | 0.6750 ± 0.0628 |
   89 | 0.6159 ± 0.0364 | 0.6500 ± 0.0750 | 0.5882 ± 0.0541 | 0.6844 ± 0.0820 |
   90 | 0.6028 ± 0.0706 | 0.7063 ± 0.0940 | 0.6350 ± 0.0399 | 0.6125 ± 0.0673 |
   91 | 0.5833 ± 0.0333 | 0.6687 ± 0.0729 | 0.6302 ± 0.0768 | 0.6469 ± 0.0766 |
   92 | 0.6172 ± 0.0381 | 0.6500 ± 0.0723 | 0.6050 ± 0.0763 | 0.6719 ± 0.0716 |
   93 | 0.5990 ± 0.0542 | 0.6937 ± 0.0500 | 0.5869 ± 0.0570 | 0.7063 ± 0.0702 |
   94 | 0.6347 ± 0.0454 | 0.6375 ± 0.0673 | 0.5829 ± 0.0704 | 0.6781 ± 0.0671 |
   95 | 0.6321 ± 0.1026 | 0.6813 ± 0.1431 | 0.6334 ± 0.0934 | 0.6625 ± 0.1006 |
   96 | 0.5555 ± 0.0406 | 0.7188 ± 0.0280 | 0.5765 ± 0.0704 | 0.7000 ± 0.0768 |
   97 | 0.6164 ± 0.0855 | 0.6687 ± 0.1196 | 0.5880 ± 0.0509 | 0.7156 ± 0.0616 |
   98 | 0.5612 ± 0.0538 | 0.7125 ± 0.0364 | 0.5784 ± 0.0623 | 0.7000 ± 0.0919 |
   99 | 0.5661 ± 0.0491 | 0.7250 ± 0.0871 | 0.5973 ± 0.0638 | 0.7000 ± 0.0805 |
  100 | 0.5855 ± 0.0593 | 0.6687 ± 0.0673 | 0.5841 ± 0.0576 | 0.6875 ± 0.0927 |
  101 | 0.6344 ± 0.0951 | 0.6687 ± 0.0580 | 0.6061 ± 0.0905 | 0.6937 ± 0.0966 |
  102 | 0.5780 ± 0.0378 | 0.7438 ± 0.0306 | 0.5854 ± 0.0522 | 0.7000 ± 0.0348 |
  103 | 0.5935 ± 0.0583 | 0.6625 ± 0.0776 | 0.5916 ± 0.0942 | 0.6781 ± 0.0740 |
  104 | 0.5891 ± 0.0590 | 0.6687 ± 0.0729 | 0.5963 ± 0.0542 | 0.7031 ± 0.0743 |
  105 | 0.6249 ± 0.0498 | 0.6813 ± 0.0306 | 0.6253 ± 0.0593 | 0.6781 ± 0.0560 |
  106 | 0.6059 ± 0.0700 | 0.6687 ± 0.0940 | 0.5668 ± 0.0567 | 0.7469 ± 0.0662 |
  107 | 0.6151 ± 0.0602 | 0.6750 ± 0.0702 | 0.5745 ± 0.0667 | 0.7219 ± 0.0771 |
  108 | 0.5742 ± 0.0567 | 0.7375 ± 0.0643 | 0.6150 ± 0.0769 | 0.6844 ± 0.0844 |
  109 | 0.6201 ± 0.0693 | 0.6312 ± 0.0800 | 0.5952 ± 0.0622 | 0.6562 ± 0.0609 |
  110 | 0.5797 ± 0.0527 | 0.7063 ± 0.0375 | 0.5976 ± 0.0548 | 0.7031 ± 0.0447 |
  111 | 0.5782 ± 0.0880 | 0.7188 ± 0.0656 | 0.6270 ± 0.0806 | 0.7063 ± 0.0908 |
  112 | 0.5846 ± 0.1009 | 0.7000 ± 0.0729 | 0.6277 ± 0.0564 | 0.6406 ± 0.0469 |
  113 | 0.6020 ± 0.0670 | 0.6562 ± 0.0862 | 0.5989 ± 0.0340 | 0.6719 ± 0.0756 |
  114 | 0.6068 ± 0.0610 | 0.6750 ± 0.0612 | 0.6246 ± 0.0672 | 0.6500 ± 0.1151 |
  115 | 0.5559 ± 0.0317 | 0.7188 ± 0.0559 | 0.5854 ± 0.0746 | 0.6750 ± 0.0875 |
  116 | 0.6435 ± 0.0694 | 0.5938 ± 0.0815 | 0.6053 ± 0.0731 | 0.6781 ± 0.1227 |
  117 | 0.5767 ± 0.0779 | 0.6937 ± 0.1035 | 0.6019 ± 0.0797 | 0.6875 ± 0.1092 |
  118 | 0.5764 ± 0.0348 | 0.7125 ± 0.0234 | 0.6189 ± 0.0535 | 0.6750 ± 0.0742 |
  119 | 0.6057 ± 0.0406 | 0.6687 ± 0.0424 | 0.6473 ± 0.0689 | 0.6531 ± 0.0844 |
  120 | 0.5736 ± 0.0494 | 0.7250 ± 0.0996 | 0.5863 ± 0.0491 | 0.7312 ± 0.0688 |
  121 | 0.5323 ± 0.0622 | 0.7438 ± 0.0871 | 0.6141 ± 0.0691 | 0.6562 ± 0.0753 |
  122 | 0.6209 ± 0.0493 | 0.6687 ± 0.0580 | 0.6017 ± 0.0886 | 0.6875 ± 0.0895 |
  123 | 0.5913 ± 0.0588 | 0.6813 ± 0.1108 | 0.5538 ± 0.0613 | 0.7375 ± 0.0805 |
  124 | 0.5860 ± 0.0682 | 0.7000 ± 0.0805 | 0.5885 ± 0.0566 | 0.6750 ± 0.0715 |
  125 | 0.5458 ± 0.0548 | 0.7312 ± 0.0424 | 0.5960 ± 0.1051 | 0.6969 ± 0.1235 |
  126 | 0.6349 ± 0.0575 | 0.6438 ± 0.0643 | 0.6049 ± 0.0456 | 0.6906 ± 0.0719 |
  127 | 0.6553 ± 0.0606 | 0.5563 ± 0.0776 | 0.6098 ± 0.0474 | 0.6156 ± 0.0862 |
  128 | 0.5760 ± 0.0485 | 0.7063 ± 0.0319 | 0.5892 ± 0.0892 | 0.7156 ± 0.0784 |
  129 | 0.5637 ± 0.0693 | 0.7125 ± 0.0956 | 0.5831 ± 0.0374 | 0.6937 ± 0.0935 |
  130 | 0.6203 ± 0.0717 | 0.6875 ± 0.0927 | 0.6041 ± 0.0689 | 0.6750 ± 0.0970 |
  131 | 0.5895 ± 0.0544 | 0.6438 ± 0.0852 | 0.6188 ± 0.0330 | 0.6500 ± 0.0500 |
  132 | 0.5958 ± 0.0508 | 0.6375 ± 0.0897 | 0.6185 ± 0.0644 | 0.6594 ± 0.0889 |
  133 | 0.5889 ± 0.0954 | 0.6875 ± 0.0815 | 0.5851 ± 0.0585 | 0.6750 ± 0.0702 |
  134 | 0.5650 ± 0.0574 | 0.7312 ± 0.0805 | 0.6068 ± 0.0502 | 0.6937 ± 0.0590 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6632 ± 0.0332 | 0.6000 ± 0.0723 | 0.6407 ± 0.0322 | 0.6375 ± 0.0628 |
    2 | 0.6200 ± 0.0527 | 0.6375 ± 0.0643 | 0.6411 ± 0.0426 | 0.6000 ± 0.0750 |
    3 | 0.6177 ± 0.0394 | 0.6375 ± 0.0250 | 0.6058 ± 0.0463 | 0.6344 ± 0.0815 |
    4 | 0.6052 ± 0.0383 | 0.6687 ± 0.0468 | 0.6334 ± 0.0892 | 0.6562 ± 0.0958 |
    5 | 0.6112 ± 0.0449 | 0.6125 ± 0.0702 | 0.5822 ± 0.0572 | 0.6844 ± 0.0719 |
    6 | 0.5980 ± 0.0825 | 0.7000 ± 0.0940 | 0.6055 ± 0.0416 | 0.6656 ± 0.0443 |
    7 | 0.6104 ± 0.0456 | 0.6562 ± 0.1135 | 0.6408 ± 0.0507 | 0.5969 ± 0.0677 |
    8 | 0.6286 ± 0.0451 | 0.6438 ± 0.0424 | 0.6042 ± 0.0549 | 0.6625 ± 0.0667 |
    9 | 0.5936 ± 0.0723 | 0.6937 ± 0.0723 | 0.5913 ± 0.0652 | 0.6844 ± 0.0796 |
   10 | 0.6266 ± 0.0218 | 0.6500 ± 0.0459 | 0.6050 ± 0.0443 | 0.6781 ± 0.0542 |
   11 | 0.5986 ± 0.0474 | 0.6875 ± 0.0593 | 0.6105 ± 0.0467 | 0.6625 ± 0.0590 |
   12 | 0.6669 ± 0.0716 | 0.6000 ± 0.0996 | 0.5810 ± 0.0625 | 0.6937 ± 0.0986 |
   13 | 0.6497 ± 0.0450 | 0.5938 ± 0.0839 | 0.6327 ± 0.0658 | 0.6188 ± 0.0848 |
   14 | 0.6084 ± 0.0592 | 0.6250 ± 0.0523 | 0.6436 ± 0.0657 | 0.6344 ± 0.0938 |
   15 | 0.4848 ± 0.0610 | 0.7875 ± 0.0871 | 0.5670 ± 0.0463 | 0.7312 ± 0.0563 |
   16 | 0.5630 ± 0.0683 | 0.7125 ± 0.0637 | 0.6286 ± 0.0794 | 0.6281 ± 0.0844 |
   17 | 0.6844 ± 0.0728 | 0.6312 ± 0.0500 | 0.6558 ± 0.0798 | 0.6375 ± 0.0580 |
   18 | 0.6083 ± 0.0383 | 0.6937 ± 0.0723 | 0.6139 ± 0.0448 | 0.6781 ± 0.0443 |
   19 | 0.6316 ± 0.0504 | 0.6375 ± 0.0508 | 0.6149 ± 0.0596 | 0.6781 ± 0.0594 |
   20 | 0.5731 ± 0.0179 | 0.6875 ± 0.0280 | 0.6023 ± 0.0905 | 0.6844 ± 0.1022 |
   21 | 0.5872 ± 0.0668 | 0.6937 ± 0.0364 | 0.6059 ± 0.0552 | 0.6781 ± 0.0443 |
   22 | 0.6182 ± 0.0513 | 0.6687 ± 0.0424 | 0.5785 ± 0.0725 | 0.7125 ± 0.0763 |
   23 | 0.6082 ± 0.0457 | 0.6750 ± 0.0755 | 0.6051 ± 0.0525 | 0.6875 ± 0.0593 |
   24 | 0.5932 ± 0.0419 | 0.6500 ± 0.0667 | 0.6237 ± 0.0588 | 0.6562 ± 0.0541 |
   25 | 0.6021 ± 0.0345 | 0.6687 ± 0.0319 | 0.5814 ± 0.0257 | 0.6875 ± 0.0370 |
   26 | 0.5566 ± 0.0338 | 0.7250 ± 0.0234 | 0.6346 ± 0.0849 | 0.6438 ± 0.1057 |
   27 | 0.5874 ± 0.0273 | 0.7063 ± 0.0424 | 0.6222 ± 0.0724 | 0.6687 ± 0.0864 |
   28 | 0.5715 ± 0.0319 | 0.6937 ± 0.0500 | 0.5991 ± 0.0418 | 0.6719 ± 0.0743 |
   29 | 0.6423 ± 0.0673 | 0.6312 ± 0.0800 | 0.5757 ± 0.0697 | 0.6750 ± 0.0612 |
   30 | 0.5816 ± 0.0765 | 0.7125 ± 0.1108 | 0.6107 ± 0.0568 | 0.6687 ± 0.0768 |
   31 | 0.5449 ± 0.0572 | 0.7500 ± 0.0656 | 0.5796 ± 0.0869 | 0.7094 ± 0.1092 |
   32 | 0.5785 ± 0.0269 | 0.6937 ± 0.0606 | 0.6225 ± 0.0658 | 0.6594 ± 0.0616 |
   33 | 0.5890 ± 0.0248 | 0.6438 ± 0.0960 | 0.6342 ± 0.0851 | 0.6062 ± 0.0729 |
   34 | 0.6289 ± 0.0589 | 0.6500 ± 0.0824 | 0.6073 ± 0.0438 | 0.6875 ± 0.0640 |
   35 | 0.5533 ± 0.0270 | 0.7375 ± 0.0319 | 0.5942 ± 0.0308 | 0.7063 ± 0.0488 |
   36 | 0.6167 ± 0.0818 | 0.6375 ± 0.0805 | 0.5691 ± 0.0574 | 0.7125 ± 0.0750 |
   37 | 0.5438 ± 0.0447 | 0.7562 ± 0.0538 | 0.6138 ± 0.0498 | 0.6719 ± 0.0876 |
   38 | 0.6299 ± 0.0597 | 0.6375 ± 0.0729 | 0.5689 ± 0.0539 | 0.7094 ± 0.0740 |
   39 | 0.6114 ± 0.0572 | 0.6312 ± 0.1192 | 0.6123 ± 0.0875 | 0.6469 ± 0.0713 |
   40 | 0.5930 ± 0.0466 | 0.7250 ± 0.0696 | 0.5941 ± 0.0989 | 0.7000 ± 0.0919 |
   41 | 0.5440 ± 0.0347 | 0.7500 ± 0.0765 | 0.6204 ± 0.0687 | 0.6656 ± 0.0862 |
   42 | 0.5813 ± 0.0362 | 0.7000 ± 0.0755 | 0.6068 ± 0.0565 | 0.6750 ± 0.0829 |
   43 | 0.6203 ± 0.0410 | 0.6625 ± 0.0637 | 0.5899 ± 0.0480 | 0.7125 ± 0.0538 |
   44 | 0.5733 ± 0.0331 | 0.6813 ± 0.0776 | 0.5860 ± 0.0689 | 0.6969 ± 0.0969 |
   45 | 0.5928 ± 0.0967 | 0.6687 ± 0.0980 | 0.5808 ± 0.0481 | 0.6594 ± 0.0732 |
   46 | 0.5741 ± 0.0925 | 0.7188 ± 0.1266 | 0.5824 ± 0.0354 | 0.7250 ± 0.0637 |
   47 | 0.6131 ± 0.0869 | 0.6750 ± 0.0897 | 0.5732 ± 0.0703 | 0.7125 ± 0.0737 |
   48 | 0.5428 ± 0.0726 | 0.7312 ± 0.0702 | 0.6017 ± 0.0746 | 0.6875 ± 0.0778 |
   49 | 0.5147 ± 0.0863 | 0.7812 ± 0.0884 | 0.6216 ± 0.0724 | 0.6531 ± 0.0921 |
   50 | 0.5671 ± 0.0581 | 0.6562 ± 0.0559 | 0.5939 ± 0.0519 | 0.6750 ± 0.0715 |
   51 | 0.6333 ± 0.0559 | 0.6562 ± 0.0559 | 0.6151 ± 0.0705 | 0.6687 ± 0.0488 |
   52 | 0.6210 ± 0.0773 | 0.6625 ± 0.1035 | 0.6003 ± 0.0737 | 0.6781 ± 0.0779 |
   53 | 0.6270 ± 0.0484 | 0.6500 ± 0.0459 | 0.6473 ± 0.0973 | 0.6687 ± 0.0658 |
   54 | 0.5672 ± 0.0359 | 0.6813 ± 0.0848 | 0.5867 ± 0.0624 | 0.6969 ± 0.0873 |
   55 | 0.5613 ± 0.0494 | 0.7125 ± 0.1035 | 0.5824 ± 0.0275 | 0.7063 ± 0.0715 |
   56 | 0.5918 ± 0.0824 | 0.6937 ± 0.0723 | 0.6416 ± 0.0682 | 0.6219 ± 0.0549 |
   57 | 0.6372 ± 0.1050 | 0.6250 ± 0.1281 | 0.5742 ± 0.0525 | 0.7312 ± 0.0702 |
   58 | 0.5965 ± 0.0685 | 0.6562 ± 0.1083 | 0.5780 ± 0.0418 | 0.7094 ± 0.0594 |
   59 | 0.6097 ± 0.0577 | 0.6937 ± 0.0667 | 0.6056 ± 0.0592 | 0.6344 ± 0.0610 |
   60 | 0.5942 ± 0.0072 | 0.6937 ± 0.0606 | 0.5684 ± 0.0652 | 0.7156 ± 0.0493 |
   61 | 0.5692 ± 0.1052 | 0.7125 ± 0.0723 | 0.5885 ± 0.0415 | 0.6844 ± 0.0745 |
   62 | 0.6054 ± 0.0120 | 0.7000 ± 0.0508 | 0.6035 ± 0.0329 | 0.6562 ± 0.0699 |
   63 | 0.5720 ± 0.0797 | 0.7063 ± 0.1000 | 0.6020 ± 0.0521 | 0.6438 ± 0.0643 |
   64 | 0.5468 ± 0.0301 | 0.7063 ± 0.0580 | 0.5653 ± 0.0466 | 0.7031 ± 0.0563 |
   65 | 0.6256 ± 0.0844 | 0.6500 ± 0.0667 | 0.6150 ± 0.0557 | 0.6625 ± 0.0763 |
   66 | 0.5852 ± 0.1062 | 0.6687 ± 0.1378 | 0.5915 ± 0.0495 | 0.6469 ± 0.0862 |
   67 | 0.6151 ± 0.0955 | 0.6750 ± 0.1019 | 0.6058 ± 0.0342 | 0.6719 ± 0.0629 |
   68 | 0.5724 ± 0.0507 | 0.7312 ± 0.0580 | 0.6150 ± 0.1010 | 0.6969 ± 0.0884 |
   69 | 0.5991 ± 0.0322 | 0.6875 ± 0.0442 | 0.6187 ± 0.1009 | 0.6813 ± 0.1256 |
   70 | 0.5688 ± 0.0404 | 0.7000 ± 0.0580 | 0.6011 ± 0.0799 | 0.6344 ± 0.0969 |
   71 | 0.6301 ± 0.0669 | 0.6250 ± 0.0559 | 0.5908 ± 0.0598 | 0.6969 ± 0.0815 |
   72 | 0.6401 ± 0.0487 | 0.6125 ± 0.0468 | 0.6061 ± 0.0516 | 0.6219 ± 0.0677 |
   73 | 0.5938 ± 0.0637 | 0.6562 ± 0.0593 | 0.5884 ± 0.0351 | 0.6937 ± 0.0573 |
   74 | 0.5622 ± 0.0916 | 0.7250 ± 0.1142 | 0.5813 ± 0.0597 | 0.7063 ± 0.0612 |
   75 | 0.5625 ± 0.0533 | 0.6625 ± 0.1287 | 0.6000 ± 0.0786 | 0.6562 ± 0.0927 |
   76 | 0.6142 ± 0.1088 | 0.6250 ± 0.0765 | 0.6115 ± 0.0586 | 0.6562 ± 0.0685 |
   77 | 0.5775 ± 0.0535 | 0.7188 ± 0.0815 | 0.5817 ± 0.0992 | 0.7125 ± 0.0871 |
   78 | 0.6175 ± 0.1419 | 0.6625 ± 0.0956 | 0.6153 ± 0.0702 | 0.6875 ± 0.0699 |
   79 | 0.5636 ± 0.0505 | 0.7250 ± 0.1159 | 0.6159 ± 0.0684 | 0.6781 ± 0.0779 |
   80 | 0.5982 ± 0.0908 | 0.6813 ± 0.0914 | 0.6005 ± 0.0625 | 0.7063 ± 0.0781 |
   81 | 0.6681 ± 0.0856 | 0.6188 ± 0.0996 | 0.6175 ± 0.1224 | 0.6687 ± 0.1163 |
   82 | 0.6072 ± 0.0783 | 0.6687 ± 0.1000 | 0.5814 ± 0.0828 | 0.7156 ± 0.0983 |
   83 | 0.5715 ± 0.0653 | 0.7000 ± 0.0755 | 0.5845 ± 0.0512 | 0.6844 ± 0.0531 |
   84 | 0.6301 ± 0.0630 | 0.6250 ± 0.0593 | 0.5745 ± 0.1009 | 0.7250 ± 0.0836 |
   85 | 0.5844 ± 0.0542 | 0.7063 ± 0.0508 | 0.5905 ± 0.0491 | 0.7028 ± 0.0581 |
   86 | 0.6521 ± 0.1263 | 0.6125 ± 0.0755 | 0.6099 ± 0.0626 | 0.6531 ± 0.0867 |
   87 | 0.5641 ± 0.0746 | 0.7250 ± 0.0848 | 0.6473 ± 0.0811 | 0.6312 ± 0.0893 |
   88 | 0.6149 ± 0.0342 | 0.6687 ± 0.0580 | 0.5832 ± 0.0573 | 0.6719 ± 0.0781 |
   89 | 0.5923 ± 0.0360 | 0.6250 ± 0.0765 | 0.6250 ± 0.0762 | 0.6719 ± 0.0597 |
   90 | 0.6687 ± 0.0488 | 0.6312 ± 0.0306 | 0.5709 ± 0.0706 | 0.6875 ± 0.0958 |
   91 | 0.5706 ± 0.0883 | 0.6750 ± 0.1128 | 0.5932 ± 0.0532 | 0.6719 ± 0.0629 |
   92 | 0.6343 ± 0.0863 | 0.6750 ± 0.0940 | 0.5970 ± 0.0773 | 0.7031 ± 0.0743 |
   93 | 0.5980 ± 0.0648 | 0.6813 ± 0.0459 | 0.6106 ± 0.0514 | 0.6687 ± 0.0715 |
   94 | 0.6350 ± 0.0655 | 0.6750 ± 0.0580 | 0.5708 ± 0.0826 | 0.7156 ± 0.1050 |
   95 | 0.5990 ± 0.0805 | 0.7063 ± 0.0897 | 0.5799 ± 0.0498 | 0.6937 ± 0.0556 |
   96 | 0.6555 ± 0.0367 | 0.6125 ± 0.0468 | 0.5902 ± 0.0391 | 0.6219 ± 0.0584 |
   97 | 0.5732 ± 0.0528 | 0.7188 ± 0.0523 | 0.5833 ± 0.0727 | 0.7219 ± 0.0832 |
   98 | 0.6115 ± 0.0353 | 0.6188 ± 0.0667 | 0.6187 ± 0.0774 | 0.6750 ± 0.0897 |
   99 | 0.5888 ± 0.0502 | 0.6750 ± 0.0612 | 0.6263 ± 0.0655 | 0.6375 ± 0.0673 |
  100 | 0.6055 ± 0.0454 | 0.6438 ± 0.0580 | 0.5707 ± 0.0543 | 0.7000 ± 0.0658 |
  101 | 0.6176 ± 0.0759 | 0.6750 ± 0.0875 | 0.6211 ± 0.0960 | 0.6500 ± 0.1006 |
  102 | 0.6340 ± 0.0210 | 0.6562 ± 0.0442 | 0.6461 ± 0.0632 | 0.6125 ± 0.0729 |
  103 | 0.5763 ± 0.0538 | 0.6687 ± 0.0940 | 0.6139 ± 0.0632 | 0.5906 ± 0.0771 |
  104 | 0.6116 ± 0.1074 | 0.6625 ± 0.0956 | 0.5865 ± 0.0653 | 0.6781 ± 0.0685 |
  105 | 0.6263 ± 0.0879 | 0.6625 ± 0.0914 | 0.6107 ± 0.0758 | 0.6750 ± 0.0886 |
  106 | 0.5388 ± 0.0751 | 0.7875 ± 0.0824 | 0.5796 ± 0.1067 | 0.7156 ± 0.0993 |
  107 | 0.6029 ± 0.0443 | 0.6687 ± 0.0897 | 0.6002 ± 0.0452 | 0.6937 ± 0.0750 |
  108 | 0.5790 ± 0.0767 | 0.6750 ± 0.0960 | 0.5970 ± 0.0645 | 0.6813 ± 0.0696 |
  109 | 0.5704 ± 0.0639 | 0.7500 ± 0.0442 | 0.5956 ± 0.0580 | 0.6750 ± 0.0596 |
  110 | 0.5665 ± 0.0653 | 0.7250 ± 0.0750 | 0.6274 ± 0.0719 | 0.6531 ± 0.0889 |
  111 | 0.6158 ± 0.0587 | 0.6438 ± 0.0468 | 0.5931 ± 0.0566 | 0.6875 ± 0.0609 |
  112 | 0.6309 ± 0.0443 | 0.6250 ± 0.0815 | 0.6119 ± 0.0673 | 0.6687 ± 0.0841 |
  113 | 0.5578 ± 0.0582 | 0.7250 ± 0.0696 | 0.5756 ± 0.0535 | 0.6906 ± 0.0632 |
  114 | 0.6174 ± 0.0952 | 0.6875 ± 0.0968 | 0.5998 ± 0.0532 | 0.6844 ± 0.0549 |
  115 | 0.5766 ± 0.0752 | 0.7250 ± 0.0696 | 0.6116 ± 0.0884 | 0.6906 ± 0.0745 |
  116 | 0.5988 ± 0.0509 | 0.6813 ± 0.0538 | 0.5941 ± 0.1009 | 0.6813 ± 0.1184 |
  117 | 0.6283 ± 0.0421 | 0.6375 ± 0.0643 | 0.5902 ± 0.0338 | 0.6937 ± 0.0519 |
  118 | 0.6178 ± 0.0601 | 0.6813 ± 0.0500 | 0.6083 ± 0.0913 | 0.7250 ± 0.0925 |
  119 | 0.5918 ± 0.0498 | 0.6937 ± 0.0364 | 0.5936 ± 0.0663 | 0.6969 ± 0.0803 |
  120 | 0.5738 ± 0.0409 | 0.6937 ± 0.0306 | 0.6023 ± 0.0479 | 0.7188 ± 0.0640 |
  121 | 0.5775 ± 0.0607 | 0.7375 ± 0.0919 | 0.5565 ± 0.0413 | 0.7281 ± 0.0577 |
  122 | 0.6136 ± 0.0378 | 0.6125 ± 0.0755 | 0.6297 ± 0.0432 | 0.5813 ± 0.0768 |
  123 | 0.5730 ± 0.0565 | 0.7250 ± 0.0824 | 0.5832 ± 0.0745 | 0.7188 ± 0.0862 |
  124 | 0.6007 ± 0.0645 | 0.6687 ± 0.0612 | 0.5716 ± 0.0894 | 0.7125 ± 0.1016 |
  125 | 0.5796 ± 0.0707 | 0.6937 ± 0.0824 | 0.5896 ± 0.0815 | 0.6906 ± 0.0745 |
  126 | 0.6550 ± 0.0661 | 0.6125 ± 0.0375 | 0.6401 ± 0.0557 | 0.6406 ± 0.0898 |
  127 | 0.5800 ± 0.0774 | 0.6875 ± 0.0523 | 0.5895 ± 0.0691 | 0.7063 ± 0.0970 |
  128 | 0.5639 ± 0.0649 | 0.7438 ± 0.0637 | 0.5829 ± 0.0549 | 0.6844 ± 0.0808 |
  129 | 0.5976 ± 0.0277 | 0.6438 ± 0.0424 | 0.5849 ± 0.0670 | 0.6906 ± 0.0855 |
  130 | 0.6184 ± 0.0434 | 0.6813 ± 0.0500 | 0.5958 ± 0.0582 | 0.6875 ± 0.0685 |
  131 | 0.6066 ± 0.0339 | 0.5875 ± 0.0696 | 0.5637 ± 0.0330 | 0.7562 ± 0.0480 |
  132 | 0.5539 ± 0.0432 | 0.7125 ± 0.0848 | 0.6090 ± 0.0844 | 0.6875 ± 0.0670 |
  133 | 0.6239 ± 0.0915 | 0.6562 ± 0.0927 | 0.5917 ± 0.0462 | 0.6531 ± 0.0889 |
  134 | 0.6647 ± 0.0407 | 0.5500 ± 0.0545 | 0.6183 ± 0.0676 | 0.6250 ± 0.0685 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6882 ± 0.0261 | 0.5500 ± 0.0673 | 0.6654 ± 0.0234 | 0.6031 ± 0.0656 |
    2 | 0.6192 ± 0.0383 | 0.6562 ± 0.0839 | 0.6238 ± 0.0446 | 0.6594 ± 0.0796 |
    3 | 0.5897 ± 0.0382 | 0.7063 ± 0.1128 | 0.6077 ± 0.0402 | 0.6781 ± 0.0577 |
    4 | 0.5871 ± 0.0417 | 0.7063 ± 0.0508 | 0.6115 ± 0.0482 | 0.6844 ± 0.0878 |
    5 | 0.6185 ± 0.0302 | 0.6875 ± 0.0625 | 0.6060 ± 0.0652 | 0.6687 ± 0.0829 |
    6 | 0.6237 ± 0.0629 | 0.6125 ± 0.0897 | 0.6353 ± 0.0389 | 0.5938 ± 0.0464 |
    7 | 0.5933 ± 0.0778 | 0.6813 ± 0.0723 | 0.5758 ± 0.0666 | 0.7063 ± 0.0768 |
    8 | 0.5866 ± 0.0622 | 0.6750 ± 0.0919 | 0.6325 ± 0.0620 | 0.6656 ± 0.0895 |
    9 | 0.6258 ± 0.0386 | 0.6687 ± 0.0729 | 0.5999 ± 0.0472 | 0.6719 ± 0.0876 |
   10 | 0.5723 ± 0.0494 | 0.6937 ± 0.0415 | 0.5749 ± 0.0455 | 0.7375 ± 0.0488 |
   11 | 0.6376 ± 0.0330 | 0.6188 ± 0.0538 | 0.5930 ± 0.0685 | 0.7031 ± 0.1103 |
   12 | 0.6765 ± 0.0666 | 0.6062 ± 0.0424 | 0.5882 ± 0.0651 | 0.6844 ± 0.1068 |
   13 | 0.5995 ± 0.0237 | 0.7000 ± 0.0424 | 0.6086 ± 0.0719 | 0.6781 ± 0.0803 |
   14 | 0.6126 ± 0.0510 | 0.6937 ± 0.0800 | 0.5756 ± 0.0464 | 0.7094 ± 0.0959 |
   15 | 0.5883 ± 0.0863 | 0.7250 ± 0.1125 | 0.6096 ± 0.0601 | 0.6687 ± 0.0864 |
   16 | 0.5757 ± 0.0476 | 0.7000 ± 0.0545 | 0.6119 ± 0.0458 | 0.6750 ± 0.0715 |
   17 | 0.6087 ± 0.0642 | 0.6813 ± 0.1142 | 0.5701 ± 0.0416 | 0.6875 ± 0.0419 |
   18 | 0.6156 ± 0.0140 | 0.6875 ± 0.0625 | 0.5726 ± 0.0610 | 0.7031 ± 0.0853 |
   19 | 0.6151 ± 0.0419 | 0.6625 ± 0.0415 | 0.6225 ± 0.0660 | 0.6500 ± 0.0637 |
   20 | 0.5750 ± 0.0580 | 0.7000 ± 0.0897 | 0.6048 ± 0.0487 | 0.6781 ± 0.0671 |
   21 | 0.6397 ± 0.1260 | 0.6438 ± 0.1019 | 0.6015 ± 0.0599 | 0.6937 ± 0.0590 |
   22 | 0.6564 ± 0.0577 | 0.5875 ± 0.0696 | 0.6302 ± 0.0614 | 0.6375 ± 0.0612 |
   23 | 0.6250 ± 0.0707 | 0.6500 ± 0.0667 | 0.6394 ± 0.0798 | 0.6438 ± 0.0805 |
   24 | 0.6077 ± 0.0486 | 0.6813 ± 0.0415 | 0.5828 ± 0.0352 | 0.7188 ± 0.0419 |
   25 | 0.6044 ± 0.0918 | 0.6625 ± 0.1302 | 0.6068 ± 0.0431 | 0.6656 ± 0.0577 |
   26 | 0.5582 ± 0.0599 | 0.7312 ± 0.0729 | 0.5838 ± 0.0512 | 0.6844 ± 0.0771 |
   27 | 0.5674 ± 0.0346 | 0.7188 ± 0.0198 | 0.6170 ± 0.0694 | 0.6656 ± 0.0884 |
   28 | 0.6076 ± 0.0363 | 0.6875 ± 0.0523 | 0.6058 ± 0.0630 | 0.7031 ± 0.0876 |
   29 | 0.5868 ± 0.0533 | 0.6937 ± 0.0750 | 0.6240 ± 0.0405 | 0.6250 ± 0.0342 |
   30 | 0.6014 ± 0.0416 | 0.6125 ± 0.0897 | 0.6310 ± 0.0762 | 0.6062 ± 0.0673 |
   31 | 0.6313 ± 0.0809 | 0.6625 ± 0.0667 | 0.6133 ± 0.0714 | 0.6531 ± 0.0493 |
   32 | 0.6166 ± 0.0872 | 0.6562 ± 0.0656 | 0.6310 ± 0.0278 | 0.6500 ± 0.0556 |
   33 | 0.6057 ± 0.0283 | 0.6125 ± 0.0319 | 0.6362 ± 0.0504 | 0.6375 ± 0.0508 |
   34 | 0.5428 ± 0.0495 | 0.7562 ± 0.0723 | 0.6166 ± 0.0797 | 0.6937 ± 0.0653 |
   35 | 0.5958 ± 0.0591 | 0.6562 ± 0.0523 | 0.6203 ± 0.0558 | 0.6625 ± 0.0737 |
   36 | 0.5727 ± 0.0623 | 0.7375 ± 0.0829 | 0.6245 ± 0.0744 | 0.6625 ± 0.0737 |
   37 | 0.6061 ± 0.0522 | 0.6562 ± 0.0593 | 0.5950 ± 0.0425 | 0.6781 ± 0.0465 |
   38 | 0.5885 ± 0.0884 | 0.7063 ± 0.0940 | 0.5906 ± 0.0611 | 0.7094 ± 0.0641 |
   39 | 0.6643 ± 0.0837 | 0.6438 ± 0.0919 | 0.5921 ± 0.0579 | 0.6906 ± 0.0647 |
   40 | 0.6637 ± 0.0295 | 0.6188 ± 0.0125 | 0.6208 ± 0.0498 | 0.6438 ± 0.0793 |
   41 | 0.6046 ± 0.0818 | 0.6875 ± 0.0862 | 0.5664 ± 0.0577 | 0.7219 ± 0.0820 |
   42 | 0.6257 ± 0.0597 | 0.6312 ± 0.0606 | 0.5690 ± 0.0598 | 0.6813 ± 0.1006 |
   43 | 0.6666 ± 0.0590 | 0.5563 ± 0.0538 | 0.5806 ± 0.0528 | 0.6875 ± 0.0523 |
   44 | 0.5765 ± 0.0998 | 0.6750 ± 0.0875 | 0.6478 ± 0.0768 | 0.6500 ± 0.0737 |
   45 | 0.6061 ± 0.0784 | 0.6500 ± 0.1108 | 0.6038 ± 0.0646 | 0.6625 ± 0.0682 |
   46 | 0.5972 ± 0.0946 | 0.6750 ± 0.1212 | 0.6126 ± 0.0470 | 0.6813 ± 0.0637 |
   47 | 0.6057 ± 0.0392 | 0.6687 ± 0.0545 | 0.6060 ± 0.0917 | 0.6844 ± 0.0921 |
   48 | 0.5730 ± 0.0344 | 0.6875 ± 0.0484 | 0.6415 ± 0.0766 | 0.6406 ± 0.1029 |
   49 | 0.5687 ± 0.0513 | 0.6687 ± 0.0545 | 0.6510 ± 0.0685 | 0.5969 ± 0.0732 |
   50 | 0.5920 ± 0.0795 | 0.7063 ± 0.0940 | 0.5882 ± 0.0792 | 0.7094 ± 0.0791 |
   51 | 0.5994 ± 0.0444 | 0.6562 ± 0.0523 | 0.6026 ± 0.0579 | 0.6906 ± 0.0677 |
   52 | 0.5756 ± 0.0634 | 0.6813 ± 0.0459 | 0.5771 ± 0.0796 | 0.7063 ± 0.1000 |
   53 | 0.5603 ± 0.0342 | 0.7063 ± 0.0580 | 0.5582 ± 0.0769 | 0.7219 ± 0.0867 |
   54 | 0.5971 ± 0.0911 | 0.6937 ± 0.0914 | 0.6014 ± 0.0508 | 0.7000 ± 0.0643 |
   55 | 0.5666 ± 0.0645 | 0.7063 ± 0.0852 | 0.5883 ± 0.0665 | 0.6844 ± 0.0932 |
   56 | 0.5288 ± 0.0112 | 0.7625 ± 0.0468 | 0.5896 ± 0.0871 | 0.7000 ± 0.0919 |
   57 | 0.6110 ± 0.0352 | 0.6438 ± 0.0375 | 0.5838 ± 0.0665 | 0.6594 ± 0.1351 |
   58 | 0.6300 ± 0.0660 | 0.6000 ± 0.0800 | 0.6054 ± 0.0547 | 0.5938 ± 0.0827 |
   59 | 0.6177 ± 0.0590 | 0.6813 ± 0.0459 | 0.5682 ± 0.0986 | 0.7000 ± 0.1146 |
   60 | 0.5797 ± 0.0925 | 0.6813 ± 0.0893 | 0.5727 ± 0.0952 | 0.7063 ± 0.0929 |
   61 | 0.5299 ± 0.0251 | 0.7438 ± 0.0364 | 0.6267 ± 0.0643 | 0.6562 ± 0.0713 |
   62 | 0.5995 ± 0.0375 | 0.7125 ± 0.0234 | 0.5847 ± 0.0548 | 0.6813 ± 0.0710 |
   63 | 0.5853 ± 0.0573 | 0.6375 ± 0.1000 | 0.5973 ± 0.0649 | 0.6312 ± 0.0364 |
   64 | 0.5724 ± 0.0087 | 0.7125 ± 0.0306 | 0.6251 ± 0.0520 | 0.6625 ± 0.0682 |
   65 | 0.5659 ± 0.0636 | 0.7125 ± 0.0500 | 0.5868 ± 0.0620 | 0.6875 ± 0.0713 |
   66 | 0.5523 ± 0.0475 | 0.7000 ± 0.0729 | 0.6161 ± 0.0861 | 0.6531 ± 0.0942 |
   67 | 0.5836 ± 0.0442 | 0.6500 ± 0.0500 | 0.6067 ± 0.0943 | 0.6531 ± 0.1206 |
   68 | 0.5918 ± 0.0410 | 0.7000 ± 0.0319 | 0.5474 ± 0.0551 | 0.7125 ± 0.0556 |
   69 | 0.6111 ± 0.0502 | 0.6813 ± 0.0500 | 0.5909 ± 0.0767 | 0.6781 ± 0.1056 |
   70 | 0.6162 ± 0.0877 | 0.6375 ± 0.0875 | 0.6067 ± 0.0512 | 0.6969 ± 0.0815 |
   71 | 0.6182 ± 0.0328 | 0.6687 ± 0.0508 | 0.5987 ± 0.0896 | 0.6719 ± 0.1129 |
   72 | 0.6032 ± 0.0848 | 0.6937 ± 0.0459 | 0.6080 ± 0.0779 | 0.6813 ± 0.0871 |
   73 | 0.6177 ± 0.0324 | 0.7063 ± 0.0508 | 0.5881 ± 0.0672 | 0.7000 ± 0.0781 |
   74 | 0.5458 ± 0.0792 | 0.7625 ± 0.0897 | 0.6153 ± 0.0776 | 0.6875 ± 0.0791 |
   75 | 0.6060 ± 0.0476 | 0.6813 ± 0.0538 | 0.5927 ± 0.0988 | 0.6875 ± 0.1152 |
   76 | 0.5454 ± 0.0461 | 0.7188 ± 0.0342 | 0.5765 ± 0.0641 | 0.7000 ± 0.0643 |
   77 | 0.6544 ± 0.0879 | 0.6562 ± 0.0948 | 0.6462 ± 0.0807 | 0.6406 ± 0.1212 |
   78 | 0.6136 ± 0.0856 | 0.6813 ± 0.0956 | 0.6139 ± 0.0657 | 0.6687 ± 0.0628 |
   79 | 0.5541 ± 0.0738 | 0.7250 ± 0.1090 | 0.5913 ± 0.0705 | 0.7375 ± 0.0908 |
   80 | 0.5905 ± 0.0709 | 0.6750 ± 0.0805 | 0.5611 ± 0.0739 | 0.7031 ± 0.0898 |
   81 | 0.5753 ± 0.0468 | 0.7000 ± 0.1000 | 0.6185 ± 0.0451 | 0.6375 ± 0.0250 |
   82 | 0.5745 ± 0.0768 | 0.6875 ± 0.0927 | 0.6325 ± 0.0656 | 0.6594 ± 0.0784 |
   83 | 0.5502 ± 0.0474 | 0.7250 ± 0.0723 | 0.5899 ± 0.0505 | 0.6844 ± 0.0531 |
   84 | 0.5984 ± 0.0789 | 0.6562 ± 0.0685 | 0.6162 ± 0.0747 | 0.6625 ± 0.1035 |
   85 | 0.5827 ± 0.0648 | 0.7125 ± 0.0723 | 0.5549 ± 0.0537 | 0.7383 ± 0.0603 |
   86 | 0.5657 ± 0.0606 | 0.7812 ± 0.0815 | 0.5981 ± 0.0701 | 0.6969 ± 0.0641 |
   87 | 0.5827 ± 0.0498 | 0.6438 ± 0.1128 | 0.5835 ± 0.0379 | 0.7250 ± 0.0480 |
   88 | 0.5934 ± 0.0438 | 0.7000 ± 0.0643 | 0.5836 ± 0.0619 | 0.7156 ± 0.0600 |
   89 | 0.5978 ± 0.0814 | 0.6562 ± 0.0713 | 0.5913 ± 0.0713 | 0.7125 ± 0.0904 |
   90 | 0.6098 ± 0.0543 | 0.6687 ± 0.0319 | 0.6368 ± 0.0992 | 0.6562 ± 0.0850 |
   91 | 0.6389 ± 0.0336 | 0.6312 ± 0.0606 | 0.6070 ± 0.0462 | 0.6594 ± 0.0513 |
   92 | 0.6188 ± 0.0538 | 0.6687 ± 0.0805 | 0.5862 ± 0.0791 | 0.7031 ± 0.0688 |
   93 | 0.5768 ± 0.0835 | 0.6750 ± 0.0852 | 0.5965 ± 0.0582 | 0.6969 ± 0.0560 |
   94 | 0.5873 ± 0.0581 | 0.6937 ± 0.0459 | 0.5911 ± 0.0653 | 0.6687 ± 0.0793 |
   95 | 0.5495 ± 0.0589 | 0.6687 ± 0.0702 | 0.5859 ± 0.0436 | 0.7094 ± 0.0443 |
   96 | 0.5887 ± 0.0457 | 0.6875 ± 0.0523 | 0.5843 ± 0.0712 | 0.7125 ± 0.1006 |
   97 | 0.6022 ± 0.0528 | 0.6937 ± 0.0637 | 0.5967 ± 0.0885 | 0.7000 ± 0.1029 |
   98 | 0.5723 ± 0.0392 | 0.7125 ± 0.0364 | 0.5797 ± 0.0682 | 0.6875 ± 0.0753 |
   99 | 0.5521 ± 0.0442 | 0.6750 ± 0.0829 | 0.5876 ± 0.0669 | 0.6844 ± 0.0952 |
  100 | 0.6295 ± 0.0506 | 0.6687 ± 0.0319 | 0.6118 ± 0.0549 | 0.6813 ± 0.0622 |
  101 | 0.5977 ± 0.0642 | 0.6937 ± 0.0415 | 0.5896 ± 0.0634 | 0.6969 ± 0.0791 |
  102 | 0.6132 ± 0.0825 | 0.6813 ± 0.0500 | 0.5653 ± 0.0341 | 0.6813 ± 0.0337 |
  103 | 0.5981 ± 0.0671 | 0.6937 ± 0.0364 | 0.5467 ± 0.0622 | 0.7250 ± 0.0590 |
  104 | 0.5900 ± 0.0757 | 0.6750 ± 0.1128 | 0.6274 ± 0.0522 | 0.6531 ± 0.0616 |
  105 | 0.5949 ± 0.0416 | 0.6937 ± 0.0364 | 0.6066 ± 0.0592 | 0.6719 ± 0.0730 |
  106 | 0.5863 ± 0.0565 | 0.7125 ± 0.0500 | 0.5828 ± 0.0712 | 0.7125 ± 0.0438 |
  107 | 0.5953 ± 0.0479 | 0.6875 ± 0.0523 | 0.6374 ± 0.0692 | 0.6687 ± 0.0628 |
  108 | 0.6416 ± 0.0376 | 0.6250 ± 0.0839 | 0.6247 ± 0.0615 | 0.6438 ± 0.0829 |
  109 | 0.6393 ± 0.0426 | 0.6625 ± 0.0800 | 0.5678 ± 0.0301 | 0.7250 ± 0.0519 |
  110 | 0.5947 ± 0.0995 | 0.6937 ± 0.0364 | 0.5903 ± 0.0576 | 0.6781 ± 0.0485 |
  111 | 0.5940 ± 0.0203 | 0.6875 ± 0.0442 | 0.6006 ± 0.0533 | 0.6719 ± 0.0546 |
  112 | 0.6107 ± 0.0731 | 0.6687 ± 0.1212 | 0.5943 ± 0.0717 | 0.6906 ± 0.0952 |
  113 | 0.6384 ± 0.0618 | 0.6375 ± 0.0673 | 0.5966 ± 0.0735 | 0.7000 ± 0.0688 |
  114 | 0.5984 ± 0.0269 | 0.6750 ± 0.0508 | 0.6127 ± 0.0541 | 0.6969 ± 0.0641 |
  115 | 0.5764 ± 0.0421 | 0.7125 ± 0.0538 | 0.6006 ± 0.0651 | 0.6781 ± 0.0713 |
  116 | 0.5553 ± 0.0239 | 0.7188 ± 0.0593 | 0.6146 ± 0.0668 | 0.6562 ± 0.0504 |
  117 | 0.6266 ± 0.0485 | 0.6000 ± 0.0606 | 0.6344 ± 0.0374 | 0.6219 ± 0.0168 |
  118 | 0.5547 ± 0.0496 | 0.7063 ± 0.0755 | 0.6214 ± 0.0600 | 0.6562 ± 0.0862 |
  119 | 0.6538 ± 0.0903 | 0.6500 ± 0.0573 | 0.5917 ± 0.0514 | 0.6937 ± 0.0573 |
  120 | 0.5686 ± 0.0640 | 0.7125 ± 0.0415 | 0.6033 ± 0.0676 | 0.6937 ± 0.0606 |
  121 | 0.5846 ± 0.0580 | 0.7250 ± 0.0459 | 0.6302 ± 0.0754 | 0.6562 ± 0.0895 |
  122 | 0.6034 ± 0.0908 | 0.6625 ± 0.1302 | 0.5754 ± 0.0571 | 0.7063 ± 0.0768 |
  123 | 0.5672 ± 0.0331 | 0.7250 ± 0.0538 | 0.5893 ± 0.0671 | 0.6719 ± 0.0930 |
  124 | 0.6343 ± 0.0608 | 0.6500 ± 0.0459 | 0.5976 ± 0.0543 | 0.6875 ± 0.0713 |
  125 | 0.5983 ± 0.0280 | 0.6813 ± 0.0364 | 0.6005 ± 0.0428 | 0.6687 ± 0.0805 |
  126 | 0.6448 ± 0.0557 | 0.6188 ± 0.0606 | 0.5529 ± 0.0682 | 0.7156 ± 0.0889 |
  127 | 0.5709 ± 0.0249 | 0.7000 ± 0.0319 | 0.5568 ± 0.0644 | 0.6844 ± 0.1096 |
  128 | 0.5757 ± 0.0566 | 0.7375 ± 0.0508 | 0.5923 ± 0.0743 | 0.6625 ± 0.0996 |
  129 | 0.5674 ± 0.0491 | 0.7125 ± 0.0538 | 0.6307 ± 0.0826 | 0.6719 ± 0.0688 |
  130 | 0.6441 ± 0.0719 | 0.6375 ± 0.0673 | 0.5897 ± 0.0639 | 0.6750 ± 0.0687 |
  131 | 0.6425 ± 0.0267 | 0.6500 ± 0.0459 | 0.6045 ± 0.0439 | 0.6719 ± 0.0688 |
  132 | 0.5400 ± 0.0419 | 0.7750 ± 0.0459 | 0.6042 ± 0.0928 | 0.7063 ± 0.0897 |
  133 | 0.5921 ± 0.0741 | 0.6875 ± 0.0791 | 0.6317 ± 0.0877 | 0.6781 ± 0.0766 |
  134 | 0.5812 ± 0.0611 | 0.7312 ± 0.0424 | 0.6502 ± 0.0547 | 0.6219 ± 0.0549 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6466 ± 0.0526 | 0.6250 ± 0.0815 | 0.6406 ± 0.0398 | 0.6094 ± 0.0950 |
    2 | 0.6697 ± 0.0377 | 0.5563 ± 0.0500 | 0.6471 ± 0.0286 | 0.6813 ± 0.0946 |
    3 | 0.6320 ± 0.0200 | 0.6937 ± 0.0723 | 0.6230 ± 0.0339 | 0.6719 ± 0.0401 |
    4 | 0.6679 ± 0.0694 | 0.6813 ± 0.0667 | 0.6292 ± 0.0563 | 0.5844 ± 0.0713 |
    5 | 0.5547 ± 0.0607 | 0.7750 ± 0.0800 | 0.6535 ± 0.0717 | 0.6781 ± 0.0685 |
    6 | 0.6458 ± 0.0265 | 0.6438 ± 0.0424 | 0.6039 ± 0.0370 | 0.6875 ± 0.0625 |
    7 | 0.6473 ± 0.0567 | 0.6312 ± 0.0776 | 0.6283 ± 0.0251 | 0.6219 ± 0.0632 |
    8 | 0.6064 ± 0.0347 | 0.7000 ± 0.0852 | 0.6574 ± 0.0386 | 0.6219 ± 0.0878 |
    9 | 0.6512 ± 0.0405 | 0.5750 ± 0.0829 | 0.6143 ± 0.0542 | 0.7000 ± 0.0864 |
   10 | 0.6687 ± 0.0389 | 0.5813 ± 0.0424 | 0.6856 ± 0.1113 | 0.6750 ± 0.0980 |
   11 | 0.6180 ± 0.1175 | 0.7063 ± 0.1057 | 0.5653 ± 0.0497 | 0.7312 ± 0.0545 |
   12 | 0.6018 ± 0.0287 | 0.7188 ± 0.0523 | 0.6047 ± 0.0354 | 0.6969 ± 0.0656 |
   13 | 0.5660 ± 0.0331 | 0.6875 ± 0.0395 | 0.6130 ± 0.0475 | 0.6719 ± 0.0597 |
   14 | 0.6883 ± 0.0359 | 0.5625 ± 0.0685 | 0.6266 ± 0.0518 | 0.6062 ± 0.0687 |
   15 | 0.6021 ± 0.0828 | 0.6750 ± 0.0980 | 0.6395 ± 0.0721 | 0.6406 ± 0.0769 |
   16 | 0.6667 ± 0.0691 | 0.6188 ± 0.0667 | 0.6361 ± 0.0967 | 0.6781 ± 0.0895 |
   17 | 0.6133 ± 0.0632 | 0.6875 ± 0.0927 | 0.6803 ± 0.0559 | 0.6344 ± 0.0505 |
   18 | 0.5753 ± 0.0497 | 0.7000 ± 0.0250 | 0.5641 ± 0.0610 | 0.7031 ± 0.0830 |
   19 | 0.6736 ± 0.0782 | 0.5938 ± 0.0884 | 0.5847 ± 0.0397 | 0.6937 ± 0.0682 |
   20 | 0.6578 ± 0.0189 | 0.5813 ± 0.0319 | 0.5924 ± 0.0433 | 0.6531 ± 0.0784 |
   21 | 0.6169 ± 0.0647 | 0.6750 ± 0.0468 | 0.5921 ± 0.0393 | 0.6750 ± 0.0468 |
   22 | 0.5780 ± 0.0396 | 0.7125 ± 0.0364 | 0.6222 ± 0.0382 | 0.6813 ± 0.0750 |
   23 | 0.5766 ± 0.0226 | 0.6625 ± 0.0306 | 0.6407 ± 0.0742 | 0.6000 ± 0.0946 |
   24 | 0.6628 ± 0.0723 | 0.6250 ± 0.0948 | 0.5983 ± 0.0681 | 0.6937 ± 0.0986 |
   25 | 0.6487 ± 0.1073 | 0.7125 ± 0.1072 | 0.6039 ± 0.0427 | 0.5625 ± 0.0873 |
   26 | 0.6445 ± 0.0781 | 0.6687 ± 0.1406 | 0.6344 ± 0.0424 | 0.6094 ± 0.0950 |
   27 | 0.6573 ± 0.0890 | 0.6500 ± 0.0976 | 0.6083 ± 0.0703 | 0.6625 ± 0.0848 |
   28 | 0.5761 ± 0.0254 | 0.7438 ± 0.0234 | 0.6264 ± 0.0510 | 0.6687 ± 0.0875 |
   29 | 0.5668 ± 0.0345 | 0.7000 ± 0.0424 | 0.6403 ± 0.0389 | 0.6219 ± 0.0705 |
   30 | 0.5967 ± 0.0303 | 0.7000 ± 0.0580 | 0.6183 ± 0.0473 | 0.6844 ± 0.0567 |
   31 | 0.6240 ± 0.0692 | 0.6687 ± 0.0612 | 0.6629 ± 0.0597 | 0.6562 ± 0.0740 |
   32 | 0.6075 ± 0.0314 | 0.6562 ± 0.0523 | 0.6280 ± 0.0419 | 0.5844 ± 0.0839 |
   33 | 0.6013 ± 0.0648 | 0.7063 ± 0.0612 | 0.5748 ± 0.0350 | 0.7188 ± 0.0523 |
   34 | 0.6485 ± 0.0799 | 0.5813 ± 0.0755 | 0.6268 ± 0.1127 | 0.7063 ± 0.1111 |
   35 | 0.6060 ± 0.0577 | 0.6813 ± 0.0606 | 0.6276 ± 0.0576 | 0.6687 ± 0.0897 |
   36 | 0.6103 ± 0.0541 | 0.6562 ± 0.0656 | 0.6058 ± 0.0598 | 0.6813 ± 0.0871 |
   37 | 0.5741 ± 0.0822 | 0.7250 ± 0.0893 | 0.6297 ± 0.0903 | 0.6594 ± 0.0808 |
   38 | 0.6269 ± 0.0742 | 0.6562 ± 0.0968 | 0.6450 ± 0.0473 | 0.5531 ± 0.0524 |
   39 | 0.6483 ± 0.1170 | 0.6375 ± 0.1163 | 0.6132 ± 0.0460 | 0.6375 ± 0.0673 |
   40 | 0.5764 ± 0.0444 | 0.7250 ± 0.0538 | 0.5972 ± 0.0542 | 0.6500 ± 0.0737 |
   41 | 0.5853 ± 0.0460 | 0.6937 ± 0.0871 | 0.5959 ± 0.0766 | 0.7063 ± 0.0688 |
   42 | 0.5960 ± 0.0384 | 0.6625 ± 0.0459 | 0.6126 ± 0.0728 | 0.6625 ± 0.0966 |
   43 | 0.5861 ± 0.0892 | 0.7063 ± 0.1057 | 0.6194 ± 0.0700 | 0.6531 ± 0.0855 |
   44 | 0.6332 ± 0.0432 | 0.6188 ± 0.0538 | 0.5830 ± 0.0445 | 0.6906 ± 0.0705 |
   45 | 0.5769 ± 0.0694 | 0.7250 ± 0.0871 | 0.6413 ± 0.0651 | 0.6438 ± 0.0742 |
   46 | 0.5605 ± 0.0493 | 0.7312 ± 0.0508 | 0.5920 ± 0.0814 | 0.7031 ± 0.0702 |
   47 | 0.7194 ± 0.1378 | 0.5875 ± 0.0824 | 0.5906 ± 0.0536 | 0.6875 ± 0.0640 |
   48 | 0.6515 ± 0.0453 | 0.6250 ± 0.0559 | 0.6782 ± 0.0653 | 0.5156 ± 0.0769 |
   49 | 0.6053 ± 0.0396 | 0.6813 ± 0.0723 | 0.6029 ± 0.0651 | 0.6781 ± 0.0839 |
   50 | 0.6384 ± 0.0505 | 0.6375 ± 0.0580 | 0.6163 ± 0.0552 | 0.6813 ± 0.0590 |
   51 | 0.6196 ± 0.0707 | 0.6813 ± 0.0637 | 0.6030 ± 0.0497 | 0.6875 ± 0.0713 |
   52 | 0.5604 ± 0.0648 | 0.7375 ± 0.0319 | 0.5865 ± 0.0606 | 0.6937 ± 0.0696 |
   53 | 0.5908 ± 0.0608 | 0.7000 ± 0.0702 | 0.5621 ± 0.0779 | 0.7219 ± 0.0662 |
   54 | 0.5907 ± 0.0587 | 0.7000 ± 0.0729 | 0.5993 ± 0.0559 | 0.7063 ± 0.0781 |
   55 | 0.5761 ± 0.0339 | 0.7063 ± 0.0805 | 0.6072 ± 0.0930 | 0.6781 ± 0.0948 |
   56 | 0.6304 ± 0.0644 | 0.6500 ± 0.0459 | 0.5941 ± 0.0539 | 0.6656 ± 0.0485 |
   57 | 0.6593 ± 0.0552 | 0.6062 ± 0.0829 | 0.6031 ± 0.0536 | 0.6625 ± 0.1025 |
   58 | 0.5565 ± 0.0449 | 0.7312 ± 0.0545 | 0.6313 ± 0.0547 | 0.6281 ± 0.0381 |
   59 | 0.5719 ± 0.0806 | 0.7312 ± 0.0580 | 0.5851 ± 0.0507 | 0.6906 ± 0.0513 |
   60 | 0.5452 ± 0.0430 | 0.7688 ± 0.0508 | 0.6506 ± 0.0906 | 0.6562 ± 0.0998 |
   61 | 0.6111 ± 0.0387 | 0.6813 ± 0.0606 | 0.5684 ± 0.0490 | 0.7375 ± 0.0563 |
   62 | 0.5709 ± 0.0453 | 0.7063 ± 0.0643 | 0.6108 ± 0.0624 | 0.6594 ± 0.0719 |
   63 | 0.6603 ± 0.0496 | 0.6562 ± 0.0927 | 0.5925 ± 0.0416 | 0.7156 ± 0.0647 |
   64 | 0.5851 ± 0.0651 | 0.7000 ± 0.1111 | 0.5729 ± 0.0441 | 0.7188 ± 0.0523 |
   65 | 0.6077 ± 0.0575 | 0.6562 ± 0.0559 | 0.5945 ± 0.0606 | 0.6781 ± 0.0753 |
   66 | 0.5956 ± 0.0404 | 0.6375 ± 0.0643 | 0.6378 ± 0.0451 | 0.6500 ± 0.0667 |
   67 | 0.5996 ± 0.0675 | 0.7125 ± 0.0606 | 0.6225 ± 0.0640 | 0.6562 ± 0.0740 |
   68 | 0.6447 ± 0.0488 | 0.6312 ± 0.0459 | 0.5817 ± 0.0683 | 0.7094 ± 0.0851 |
   69 | 0.6079 ± 0.0499 | 0.7250 ± 0.0606 | 0.5923 ± 0.0539 | 0.6687 ± 0.0596 |
   70 | 0.5898 ± 0.0303 | 0.6438 ± 0.0852 | 0.5978 ± 0.0723 | 0.6781 ± 0.0779 |
   71 | 0.6510 ± 0.1118 | 0.6250 ± 0.1046 | 0.5917 ± 0.0778 | 0.7031 ± 0.1244 |
   72 | 0.5524 ± 0.0386 | 0.7375 ± 0.0250 | 0.6606 ± 0.1032 | 0.6594 ± 0.0784 |
   73 | 0.5821 ± 0.0836 | 0.6312 ± 0.0871 | 0.7567 ± 0.1350 | 0.5062 ± 0.0925 |
   74 | 0.5697 ± 0.0263 | 0.6937 ± 0.0234 | 0.6060 ± 0.0727 | 0.6719 ± 0.0830 |
   75 | 0.5972 ± 0.0985 | 0.6625 ± 0.0800 | 0.5591 ± 0.0665 | 0.7094 ± 0.0779 |
   76 | 0.6448 ± 0.0645 | 0.6562 ± 0.1064 | 0.6022 ± 0.0370 | 0.6656 ± 0.0641 |
   77 | 0.5680 ± 0.0469 | 0.7125 ± 0.0750 | 0.5991 ± 0.0920 | 0.7000 ± 0.0864 |
   78 | 0.6020 ± 0.0543 | 0.6813 ± 0.0824 | 0.5973 ± 0.0488 | 0.7031 ± 0.0702 |
   79 | 0.5655 ± 0.0714 | 0.6625 ± 0.1016 | 0.6445 ± 0.0499 | 0.6344 ± 0.0779 |
   80 | 0.6355 ± 0.0653 | 0.6438 ± 0.0940 | 0.5719 ± 0.0439 | 0.6969 ± 0.0727 |
   81 | 0.6275 ± 0.0422 | 0.6687 ± 0.0424 | 0.6075 ± 0.0861 | 0.6719 ± 0.1039 |
   82 | 0.6400 ± 0.0595 | 0.6312 ± 0.0538 | 0.5920 ± 0.0663 | 0.7000 ± 0.0755 |
   83 | 0.5058 ± 0.0798 | 0.7500 ± 0.0839 | 0.5781 ± 0.0605 | 0.7219 ± 0.0855 |
   84 | 0.5511 ± 0.0229 | 0.7063 ± 0.0545 | 0.9710 ± 0.1852 | 0.5625 ± 0.1144 |
   85 | 0.5909 ± 0.0265 | 0.6250 ± 0.0765 | 0.6061 ± 0.0502 | 0.6829 ± 0.0790 |
   86 | 0.5778 ± 0.0453 | 0.7000 ± 0.0319 | 0.5713 ± 0.0506 | 0.7063 ± 0.0612 |
   87 | 0.5453 ± 0.0482 | 0.7562 ± 0.0500 | 0.5915 ± 0.0829 | 0.7000 ± 0.0929 |
   88 | 0.5677 ± 0.0681 | 0.6312 ± 0.0824 | 0.6140 ± 0.0509 | 0.6469 ± 0.0560 |
   89 | 0.6132 ± 0.0631 | 0.6438 ± 0.0852 | 0.5646 ± 0.0595 | 0.7156 ± 0.0647 |
   90 | 0.5518 ± 0.0514 | 0.7312 ± 0.0702 | 0.6006 ± 0.0870 | 0.6656 ± 0.1046 |
   91 | 0.5408 ± 0.0532 | 0.7250 ± 0.0976 | 0.5844 ± 0.0552 | 0.6813 ± 0.0893 |
   92 | 0.5886 ± 0.0883 | 0.7250 ± 0.0723 | 0.6391 ± 0.0515 | 0.6219 ± 0.0796 |
   93 | 0.5936 ± 0.0203 | 0.6813 ± 0.0538 | 0.6237 ± 0.0431 | 0.6500 ± 0.0710 |
   94 | 0.6053 ± 0.0575 | 0.6625 ± 0.0364 | 0.5891 ± 0.0622 | 0.7031 ± 0.0830 |
   95 | 0.6031 ± 0.0219 | 0.6500 ± 0.0538 | 0.6566 ± 0.0914 | 0.6219 ± 0.0973 |
   96 | 0.5699 ± 0.0558 | 0.7000 ± 0.0960 | 0.5590 ± 0.0447 | 0.7094 ± 0.0542 |
   97 | 0.6056 ± 0.0816 | 0.6875 ± 0.0791 | 0.5736 ± 0.0542 | 0.6937 ± 0.0556 |
   98 | 0.6251 ± 0.0886 | 0.6687 ± 0.0580 | 0.6133 ± 0.0916 | 0.6750 ± 0.0658 |
   99 | 0.5814 ± 0.0578 | 0.6375 ± 0.0919 | 0.5867 ± 0.0631 | 0.6844 ± 0.1022 |
  100 | 0.5830 ± 0.0603 | 0.6937 ± 0.0956 | 0.6492 ± 0.0931 | 0.6438 ± 0.0715 |
  101 | 0.5539 ± 0.0318 | 0.7188 ± 0.0395 | 0.6097 ± 0.1024 | 0.6687 ± 0.1000 |
  102 | 0.5956 ± 0.0577 | 0.6438 ± 0.1163 | 0.5983 ± 0.0528 | 0.6531 ± 0.0600 |
  103 | 0.5522 ± 0.0433 | 0.6625 ± 0.0234 | 0.5460 ± 0.0350 | 0.7156 ± 0.0531 |
  104 | 0.6174 ± 0.0414 | 0.6813 ± 0.0306 | 0.5716 ± 0.0469 | 0.7031 ± 0.0425 |
  105 | 0.5602 ± 0.0661 | 0.7312 ± 0.0729 | 0.6223 ± 0.0563 | 0.6531 ± 0.0784 |
  106 | 0.6086 ± 0.0445 | 0.6687 ± 0.0729 | 0.5921 ± 0.0451 | 0.7031 ± 0.0806 |
  107 | 0.6077 ± 0.0656 | 0.6937 ± 0.0415 | 0.5986 ± 0.0600 | 0.7219 ± 0.0796 |
  108 | 0.6255 ± 0.0280 | 0.6000 ± 0.0637 | 0.6175 ± 0.0725 | 0.6969 ± 0.0740 |
  109 | 0.6025 ± 0.0737 | 0.6562 ± 0.0523 | 0.5931 ± 0.0827 | 0.6813 ± 0.1006 |
  110 | 0.5797 ± 0.0606 | 0.7125 ± 0.1072 | 0.5660 ± 0.0584 | 0.7188 ± 0.0713 |
  111 | 0.6319 ± 0.0539 | 0.6750 ± 0.0781 | 0.6214 ± 0.0615 | 0.6500 ± 0.0667 |
  112 | 0.6043 ± 0.0775 | 0.6750 ± 0.0829 | 0.6045 ± 0.0614 | 0.6781 ± 0.0928 |
  113 | 0.5890 ± 0.0420 | 0.7000 ± 0.0508 | 0.6218 ± 0.0698 | 0.6687 ± 0.0702 |
  114 | 0.5568 ± 0.0748 | 0.7125 ± 0.0776 | 0.6320 ± 0.0539 | 0.6531 ± 0.0600 |
  115 | 0.6304 ± 0.0104 | 0.6813 ± 0.0500 | 0.5968 ± 0.0465 | 0.6969 ± 0.0420 |
  116 | 0.6553 ± 0.0820 | 0.6125 ± 0.1075 | 0.6205 ± 0.0440 | 0.6562 ± 0.0593 |
  117 | 0.6713 ± 0.2067 | 0.7125 ± 0.1176 | 0.6069 ± 0.0434 | 0.6656 ± 0.0753 |
  118 | 0.6459 ± 0.0837 | 0.6125 ± 0.1406 | 0.5637 ± 0.0493 | 0.7188 ± 0.0609 |
  119 | 0.5722 ± 0.0465 | 0.7125 ± 0.0667 | 0.5543 ± 0.0806 | 0.7188 ± 0.1017 |
  120 | 0.5634 ± 0.0866 | 0.6937 ± 0.1142 | 0.6180 ± 0.0646 | 0.6594 ± 0.0889 |
  121 | 0.5748 ± 0.0882 | 0.7375 ± 0.0897 | 0.5783 ± 0.0694 | 0.7000 ± 0.0970 |
  122 | 0.5457 ± 0.0489 | 0.7063 ± 0.0729 | 0.6382 ± 0.1069 | 0.5969 ± 0.0973 |
  123 | 0.5910 ± 0.0968 | 0.6813 ± 0.0606 | 0.5859 ± 0.0525 | 0.7188 ± 0.0827 |
  124 | 0.5696 ± 0.0698 | 0.6937 ± 0.0538 | 0.6028 ± 0.0599 | 0.6625 ± 0.0653 |
  125 | 0.5964 ± 0.0916 | 0.6813 ± 0.1035 | 0.5878 ± 0.0543 | 0.6969 ± 0.0766 |
  126 | 0.5790 ± 0.0923 | 0.7188 ± 0.0765 | 0.5811 ± 0.0729 | 0.7031 ± 0.0830 |
  127 | 0.5567 ± 0.0529 | 0.7625 ± 0.0580 | 0.5829 ± 0.0752 | 0.6969 ± 0.0656 |
  128 | 0.6073 ± 0.0530 | 0.6813 ± 0.0364 | 0.6407 ± 0.0657 | 0.6375 ± 0.0729 |
  129 | 0.6380 ± 0.0466 | 0.6250 ± 0.1064 | 0.6170 ± 0.0760 | 0.6719 ± 0.0769 |
  130 | 0.5669 ± 0.0272 | 0.7188 ± 0.0342 | 0.6465 ± 0.0862 | 0.6281 ± 0.0832 |
  131 | 0.5775 ± 0.0553 | 0.7063 ± 0.0508 | 0.5479 ± 0.0554 | 0.7406 ± 0.0641 |
  132 | 0.6153 ± 0.0928 | 0.6813 ± 0.0750 | 0.5679 ± 0.0651 | 0.7281 ± 0.0815 |
  133 | 0.5869 ± 0.0688 | 0.6813 ± 0.0364 | 0.6029 ± 0.0687 | 0.6875 ± 0.0827 |
  134 | 0.5357 ± 0.0446 | 0.7312 ± 0.0468 | 0.5877 ± 0.0594 | 0.6937 ± 0.0750 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6025 ± 0.0552 | 0.7063 ± 0.0468 | 0.5942 ± 0.0905 | 0.6781 ± 0.0979 |
    2 | 0.6424 ± 0.0562 | 0.6375 ± 0.0580 | 0.6109 ± 0.0621 | 0.6562 ± 0.0948 |
    3 | 0.6196 ± 0.0362 | 0.6125 ± 0.0468 | 0.6229 ± 0.0602 | 0.6031 ± 0.0641 |
    4 | 0.5923 ± 0.0523 | 0.7500 ± 0.0968 | 0.6071 ± 0.0460 | 0.6781 ± 0.0560 |
    5 | 0.6684 ± 0.0422 | 0.6062 ± 0.1228 | 0.6304 ± 0.0296 | 0.6687 ± 0.0580 |
    6 | 0.5646 ± 0.0435 | 0.6937 ± 0.0500 | 0.5778 ± 0.0853 | 0.6906 ± 0.0719 |
    7 | 0.6585 ± 0.0460 | 0.6062 ± 0.0805 | 0.6135 ± 0.0311 | 0.6594 ± 0.0567 |
    8 | 0.6241 ± 0.1175 | 0.6250 ± 0.0815 | 0.6622 ± 0.0905 | 0.5906 ± 0.0844 |
    9 | 0.5792 ± 0.0515 | 0.7375 ± 0.0643 | 0.6343 ± 0.0814 | 0.6594 ± 0.0705 |
   10 | 0.6270 ± 0.0963 | 0.6813 ± 0.0956 | 0.6639 ± 0.1239 | 0.7094 ± 0.0685 |
   11 | 0.6057 ± 0.1027 | 0.6687 ± 0.1259 | 0.6065 ± 0.0434 | 0.6781 ± 0.0560 |
   12 | 0.5966 ± 0.0315 | 0.6875 ± 0.0523 | 0.5907 ± 0.0489 | 0.6844 ± 0.0745 |
   13 | 0.6182 ± 0.0680 | 0.6750 ± 0.0897 | 0.5886 ± 0.0648 | 0.7094 ± 0.0779 |
   14 | 0.5712 ± 0.0958 | 0.6937 ± 0.0871 | 0.5759 ± 0.0559 | 0.6969 ± 0.0577 |
   15 | 0.5787 ± 0.0722 | 0.7125 ± 0.0696 | 0.6492 ± 0.0990 | 0.6438 ± 0.0768 |
   16 | 0.5548 ± 0.0651 | 0.7250 ± 0.0800 | 0.6061 ± 0.0726 | 0.6844 ± 0.0647 |
   17 | 0.6224 ± 0.1228 | 0.6813 ± 0.0606 | 0.6479 ± 0.0421 | 0.6250 ± 0.0484 |
   18 | 0.5937 ± 0.0749 | 0.6625 ± 0.0306 | 0.6076 ± 0.0506 | 0.6469 ± 0.0839 |
   19 | 0.5643 ± 0.0598 | 0.7063 ± 0.0729 | 0.6032 ± 0.0833 | 0.6875 ± 0.0873 |
   20 | 0.6567 ± 0.0515 | 0.6000 ± 0.0234 | 0.6030 ± 0.0548 | 0.6750 ± 0.0864 |
   21 | 0.5885 ± 0.0490 | 0.7188 ± 0.0395 | 0.5933 ± 0.0407 | 0.6438 ± 0.0829 |
   22 | 0.6034 ± 0.0576 | 0.6813 ± 0.0723 | 0.6039 ± 0.0380 | 0.6781 ± 0.0851 |
   23 | 0.6119 ± 0.0120 | 0.6375 ± 0.0424 | 0.6365 ± 0.0790 | 0.6344 ± 0.0873 |
   24 | 0.6266 ± 0.0316 | 0.6312 ± 0.0573 | 0.6901 ± 0.0874 | 0.6188 ± 0.0788 |
   25 | 0.5495 ± 0.0808 | 0.7375 ± 0.0612 | 0.6505 ± 0.0705 | 0.6625 ± 0.0500 |
   26 | 0.5668 ± 0.0585 | 0.7312 ± 0.0852 | 0.6311 ± 0.0734 | 0.6562 ± 0.0958 |
   27 | 0.5434 ± 0.0852 | 0.7312 ± 0.0829 | 0.6157 ± 0.0766 | 0.6656 ± 0.0766 |
   28 | 0.6162 ± 0.0803 | 0.6750 ± 0.0702 | 0.6179 ± 0.0699 | 0.6687 ± 0.0702 |
   29 | 0.6629 ± 0.0331 | 0.6188 ± 0.0538 | 0.5901 ± 0.0645 | 0.7094 ± 0.1018 |
   30 | 0.6079 ± 0.0670 | 0.6687 ± 0.0897 | 0.5999 ± 0.0615 | 0.6937 ± 0.1062 |
   31 | 0.6506 ± 0.0558 | 0.6562 ± 0.0280 | 0.6756 ± 0.1077 | 0.5563 ± 0.1072 |
   32 | 0.5932 ± 0.0819 | 0.6813 ± 0.1523 | 0.8076 ± 0.1912 | 0.6687 ± 0.0829 |
   33 | 0.6746 ± 0.0534 | 0.6438 ± 0.0673 | 0.6105 ± 0.0584 | 0.6781 ± 0.0610 |
   34 | 0.5964 ± 0.0407 | 0.6750 ± 0.0153 | 0.6089 ± 0.0810 | 0.6719 ± 0.0841 |
   35 | 0.5875 ± 0.0518 | 0.7063 ± 0.0643 | 0.5977 ± 0.1007 | 0.6750 ± 0.0960 |
   36 | 0.5635 ± 0.0309 | 0.6875 ± 0.0342 | 0.5659 ± 0.0853 | 0.7250 ± 0.0871 |
   37 | 0.6158 ± 0.0522 | 0.7188 ± 0.0884 | 0.5939 ± 0.0594 | 0.6875 ± 0.0778 |
   38 | 0.5994 ± 0.0604 | 0.6875 ± 0.0656 | 0.5870 ± 0.0669 | 0.7063 ± 0.0852 |
   39 | 0.6041 ± 0.0889 | 0.7250 ± 0.0848 | 0.6263 ± 0.0956 | 0.6781 ± 0.0999 |
   40 | 0.5799 ± 0.0394 | 0.6750 ± 0.0375 | 0.5753 ± 0.0724 | 0.6875 ± 0.0988 |
   41 | 0.6019 ± 0.1166 | 0.6687 ± 0.1000 | 0.5995 ± 0.0483 | 0.6750 ± 0.0658 |
   42 | 0.6340 ± 0.0819 | 0.6750 ± 0.0755 | 0.6280 ± 0.0527 | 0.6406 ± 0.0769 |
   43 | 0.5862 ± 0.0940 | 0.6500 ± 0.0893 | 0.5981 ± 0.0404 | 0.6687 ± 0.0841 |
   44 | 0.5708 ± 0.1448 | 0.6937 ± 0.1471 | 0.5810 ± 0.0791 | 0.6813 ± 0.0935 |
   45 | 0.5744 ± 0.0551 | 0.6813 ± 0.0538 | 0.5958 ± 0.0576 | 0.6844 ± 0.0632 |
   46 | 0.5573 ± 0.0556 | 0.7063 ± 0.0673 | 0.5568 ± 0.0698 | 0.7156 ± 0.0796 |
   47 | 0.6130 ± 0.0545 | 0.6438 ± 0.0580 | 0.6288 ± 0.0780 | 0.6875 ± 0.0916 |
   48 | 0.5823 ± 0.0594 | 0.6750 ± 0.0673 | 0.5841 ± 0.0568 | 0.6781 ± 0.0727 |
   49 | 0.6270 ± 0.0294 | 0.6625 ± 0.0500 | 0.6108 ± 0.0989 | 0.6625 ± 0.0986 |
   50 | 0.5981 ± 0.0474 | 0.6750 ± 0.0580 | 0.6005 ± 0.0618 | 0.6531 ± 0.0942 |
   51 | 0.5881 ± 0.0766 | 0.6562 ± 0.0839 | 0.5419 ± 0.0713 | 0.7312 ± 0.0897 |
   52 | 0.5876 ± 0.0308 | 0.7250 ± 0.0606 | 0.6141 ± 0.0315 | 0.6813 ± 0.0480 |
   53 | 0.5980 ± 0.0464 | 0.6625 ± 0.0696 | 0.5887 ± 0.0699 | 0.6719 ± 0.0743 |
   54 | 0.5906 ± 0.1096 | 0.6813 ± 0.0956 | 0.5704 ± 0.0814 | 0.7281 ± 0.0641 |
   55 | 0.5908 ± 0.0428 | 0.6625 ± 0.0538 | 0.5911 ± 0.0579 | 0.7063 ± 0.0702 |
   56 | 0.6249 ± 0.1164 | 0.6813 ± 0.1035 | 0.6018 ± 0.1235 | 0.6906 ± 0.0993 |
   57 | 0.5371 ± 0.0439 | 0.6937 ± 0.0667 | 0.6225 ± 0.0633 | 0.6906 ± 0.0855 |
   58 | 0.5928 ± 0.0222 | 0.6438 ± 0.0424 | 0.5695 ± 0.0310 | 0.7094 ± 0.0344 |
   59 | 0.5936 ± 0.0567 | 0.6687 ± 0.0643 | 0.6090 ± 0.0764 | 0.6844 ± 0.0745 |
   60 | 0.5810 ± 0.0768 | 0.6813 ± 0.1256 | 0.5954 ± 0.0716 | 0.6906 ± 0.0900 |
   61 | 0.6162 ± 0.0608 | 0.6687 ± 0.0919 | 0.5878 ± 0.0541 | 0.7000 ± 0.0805 |
   62 | 0.5965 ± 0.0823 | 0.6813 ± 0.0996 | 0.6157 ± 0.0383 | 0.6656 ± 0.0505 |
   63 | 0.6531 ± 0.0263 | 0.6188 ± 0.0459 | 0.6136 ± 0.0449 | 0.6625 ± 0.0538 |
   64 | 0.6198 ± 0.0445 | 0.6687 ± 0.0580 | 0.6016 ± 0.0551 | 0.6750 ± 0.0805 |
   65 | 0.6673 ± 0.1116 | 0.6000 ± 0.1332 | 0.6046 ± 0.0746 | 0.6719 ± 0.0716 |
   66 | 0.6759 ± 0.2096 | 0.6625 ± 0.0696 | 0.6381 ± 0.0637 | 0.5625 ± 0.0791 |
   67 | 0.6718 ± 0.1338 | 0.6188 ± 0.1053 | 0.6332 ± 0.1150 | 0.6531 ± 0.1068 |
   68 | 0.5237 ± 0.0419 | 0.7625 ± 0.0424 | 0.5533 ± 0.0975 | 0.7344 ± 0.0743 |
   69 | 0.5505 ± 0.0483 | 0.7438 ± 0.0234 | 0.6644 ± 0.0837 | 0.6312 ± 0.0750 |
   70 | 0.5735 ± 0.0355 | 0.7000 ± 0.0424 | 0.5896 ± 0.0627 | 0.6844 ± 0.0796 |
   71 | 0.5513 ± 0.0651 | 0.7000 ± 0.0643 | 0.6147 ± 0.0404 | 0.6656 ± 0.0465 |
   72 | 0.5907 ± 0.0427 | 0.6687 ± 0.0153 | 0.5386 ± 0.0650 | 0.7312 ± 0.0970 |
   73 | 0.5792 ± 0.0898 | 0.6937 ± 0.0824 | 0.6175 ± 0.0658 | 0.5563 ± 0.0859 |
   74 | 0.6533 ± 0.0398 | 0.6250 ± 0.0815 | 0.5868 ± 0.0498 | 0.7063 ± 0.0580 |
   75 | 0.6127 ± 0.0616 | 0.6625 ± 0.0500 | 0.5971 ± 0.0738 | 0.6531 ± 0.0855 |
   76 | 0.5396 ± 0.0883 | 0.7125 ± 0.1090 | 0.5306 ± 0.0479 | 0.7281 ± 0.0626 |
   77 | 0.6242 ± 0.0636 | 0.6375 ± 0.0673 | 0.6140 ± 0.0802 | 0.6625 ± 0.0848 |
   78 | 0.5918 ± 0.0486 | 0.6875 ± 0.0765 | 0.5795 ± 0.0512 | 0.6906 ± 0.0295 |
   79 | 0.6244 ± 0.0913 | 0.6438 ± 0.0829 | 0.5716 ± 0.0811 | 0.7000 ± 0.0729 |
   80 | 0.5793 ± 0.0649 | 0.6937 ± 0.0800 | 0.6112 ± 0.0550 | 0.6687 ± 0.0596 |
   81 | 0.6511 ± 0.0934 | 0.6250 ± 0.0791 | 0.5936 ± 0.0768 | 0.6781 ± 0.0766 |
   82 | 0.6017 ± 0.0779 | 0.6813 ± 0.1332 | 0.5809 ± 0.0650 | 0.6906 ± 0.0647 |
   83 | 0.5760 ± 0.0581 | 0.7312 ± 0.0375 | 0.6009 ± 0.0712 | 0.6687 ± 0.0768 |
   84 | 0.6229 ± 0.0531 | 0.5938 ± 0.0656 | 0.5617 ± 0.1034 | 0.6969 ± 0.0917 |
   85 | 0.6180 ± 0.0647 | 0.6750 ± 0.0508 | 0.5848 ± 0.0646 | 0.6558 ± 0.0846 |
   86 | 0.6315 ± 0.0797 | 0.6687 ± 0.0702 | 0.5896 ± 0.0712 | 0.6969 ± 0.0959 |
   87 | 0.6207 ± 0.0188 | 0.6500 ± 0.0538 | 0.5798 ± 0.0293 | 0.7094 ± 0.0485 |
   88 | 0.5982 ± 0.0717 | 0.7063 ± 0.0897 | 0.5667 ± 0.0454 | 0.7000 ± 0.0673 |
   89 | 0.6086 ± 0.0688 | 0.6937 ± 0.0776 | 0.5681 ± 0.0461 | 0.6969 ± 0.0779 |
   90 | 0.6059 ± 0.0563 | 0.6000 ± 0.0606 | 0.5827 ± 0.0652 | 0.6875 ± 0.0778 |
   91 | 0.6772 ± 0.1062 | 0.6312 ± 0.0667 | 0.5864 ± 0.0754 | 0.6406 ± 0.0629 |
   92 | 0.5723 ± 0.1048 | 0.6375 ± 0.1378 | 0.5956 ± 0.0766 | 0.6750 ± 0.0658 |
   93 | 0.5849 ± 0.0819 | 0.6562 ± 0.0839 | 0.6149 ± 0.0985 | 0.6719 ± 0.1094 |
   94 | 0.5935 ± 0.0565 | 0.6500 ± 0.0637 | 0.6222 ± 0.0847 | 0.6531 ± 0.0963 |
   95 | 0.5757 ± 0.0603 | 0.7000 ± 0.0980 | 0.6779 ± 0.0505 | 0.5281 ± 0.0647 |
   96 | 0.6135 ± 0.0757 | 0.6438 ± 0.0919 | 0.6944 ± 0.1050 | 0.6719 ± 0.0629 |
   97 | 0.5428 ± 0.0835 | 0.6937 ± 0.0976 | 0.5789 ± 0.0487 | 0.7063 ± 0.0446 |
   98 | 0.6016 ± 0.1246 | 0.7125 ± 0.1090 | 0.5716 ± 0.0751 | 0.7063 ± 0.0715 |
   99 | 0.6789 ± 0.1044 | 0.5938 ± 0.1064 | 0.7170 ± 0.0721 | 0.6969 ± 0.0524 |
  100 | 0.6281 ± 0.0447 | 0.5625 ± 0.0839 | 0.6071 ± 0.0667 | 0.6594 ± 0.0844 |
  101 | 0.5971 ± 0.0933 | 0.6937 ± 0.0696 | 0.5922 ± 0.0576 | 0.6687 ± 0.0508 |
  102 | 0.5498 ± 0.0265 | 0.7063 ± 0.0319 | 0.6151 ± 0.0636 | 0.6625 ± 0.0653 |
  103 | 0.5574 ± 0.0500 | 0.7063 ± 0.0580 | 0.5801 ± 0.0469 | 0.6531 ± 0.0808 |
  104 | 0.6264 ± 0.1209 | 0.6625 ± 0.0848 | 0.5811 ± 0.0897 | 0.6875 ± 0.0958 |
  105 | 0.5976 ± 0.0375 | 0.6562 ± 0.0656 | 0.6028 ± 0.0555 | 0.6594 ± 0.0832 |
  106 | 0.4992 ± 0.0482 | 0.7750 ± 0.0606 | 0.5845 ± 0.0630 | 0.6531 ± 0.0796 |
  107 | 0.5756 ± 0.0520 | 0.7063 ± 0.0673 | 0.5810 ± 0.0660 | 0.6969 ± 0.0851 |
  108 | 0.6053 ± 0.0860 | 0.6250 ± 0.1398 | 0.6146 ± 0.0935 | 0.6375 ± 0.0829 |
  109 | 0.5573 ± 0.0502 | 0.7438 ± 0.0871 | 0.6116 ± 0.0680 | 0.6625 ± 0.0871 |
  110 | 0.5779 ± 0.0468 | 0.6875 ± 0.0988 | 0.5865 ± 0.0594 | 0.6844 ± 0.0758 |
  111 | 0.6260 ± 0.0763 | 0.7063 ± 0.0755 | 0.6328 ± 0.0547 | 0.6406 ± 0.0597 |
  112 | 0.5630 ± 0.0373 | 0.6875 ± 0.0198 | 0.5873 ± 0.0651 | 0.6687 ± 0.0508 |
  113 | 0.5460 ± 0.0650 | 0.7188 ± 0.0523 | 0.5673 ± 0.0824 | 0.6969 ± 0.0948 |
  114 | 0.5541 ± 0.0296 | 0.7125 ± 0.0696 | 0.5930 ± 0.0615 | 0.6937 ± 0.0871 |
  115 | 0.6615 ± 0.0227 | 0.6375 ± 0.0580 | 0.5804 ± 0.0677 | 0.7156 ± 0.1139 |
  116 | 0.6145 ± 0.0427 | 0.6312 ± 0.0538 | 0.5914 ± 0.0655 | 0.6937 ± 0.1116 |
  117 | 0.5579 ± 0.0801 | 0.7188 ± 0.1027 | 0.6044 ± 0.0484 | 0.6656 ± 0.0610 |
  118 | 0.5875 ± 0.0966 | 0.6562 ± 0.0988 | 0.6017 ± 0.0875 | 0.6625 ± 0.0590 |
  119 | 0.5969 ± 0.0537 | 0.6875 ± 0.0523 | 0.6182 ± 0.0787 | 0.6625 ± 0.0836 |
  120 | 0.6282 ± 0.0473 | 0.7125 ± 0.0500 | 0.5573 ± 0.0951 | 0.7250 ± 0.0893 |
  121 | 0.5704 ± 0.1184 | 0.6875 ± 0.1531 | 0.5821 ± 0.0948 | 0.7000 ± 0.0755 |
  122 | 0.5903 ± 0.0883 | 0.6500 ± 0.0956 | 0.6002 ± 0.0517 | 0.6500 ± 0.0904 |
  123 | 1.0311 ± 0.1010 | 0.5437 ± 0.0468 | 1.2507 ± 0.2268 | 0.5500 ± 0.0864 |
  124 | 0.7442 ± 0.0526 | 0.4313 ± 0.0996 | 0.6963 ± 0.0071 | 0.4906 ± 0.0465 |
  125 | 0.6948 ± 0.0252 | 0.5563 ± 0.0606 | 0.7640 ± 0.0449 | 0.4531 ± 0.0702 |
  126 | 0.7346 ± 0.1138 | 0.5312 ± 0.1152 | 0.8456 ± 0.0957 | 0.5344 ± 0.0719 |
  127 | 0.8473 ± 0.1541 | 0.5250 ± 0.0723 | 0.7227 ± 0.0426 | 0.5188 ± 0.0643 |
  128 | 0.6918 ± 0.0211 | 0.5000 ± 0.0656 | 0.6777 ± 0.0087 | 0.6219 ± 0.0632 |
  129 | 0.6361 ± 0.0570 | 0.6250 ± 0.0559 | 0.6662 ± 0.0430 | 0.6344 ± 0.1074 |
  130 | 0.6813 ± 0.0877 | 0.6062 ± 0.0805 | 0.7282 ± 0.0897 | 0.6094 ± 0.0887 |
  131 | 0.7081 ± 0.0913 | 0.5750 ± 0.0781 | 0.7206 ± 0.0682 | 0.4250 ± 0.0755 |
  132 | 0.6848 ± 0.0471 | 0.5875 ± 0.0996 | 0.6213 ± 0.0661 | 0.6906 ± 0.0549 |
  133 | 0.6296 ± 0.0501 | 0.6188 ± 0.0538 | 0.6181 ± 0.0492 | 0.6875 ± 0.0699 |
  134 | 0.6376 ± 0.0287 | 0.6250 ± 0.0593 | 0.6584 ± 0.0870 | 0.6281 ± 0.0889 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6688 ± 0.0797 | 0.5250 ± 0.0935 | 0.6114 ± 0.0332 | 0.6719 ± 0.0644 |
    2 | 0.6607 ± 0.0069 | 0.6875 ± 0.1046 | 0.6678 ± 0.0383 | 0.5969 ± 0.0878 |
    3 | 0.6802 ± 0.0998 | 0.6438 ± 0.0702 | 0.6095 ± 0.0407 | 0.6375 ± 0.1163 |
    4 | 0.6156 ± 0.0380 | 0.6562 ± 0.0395 | 0.6625 ± 0.0969 | 0.6312 ± 0.0836 |
    5 | 0.6828 ± 0.0136 | 0.5312 ± 0.1027 | 0.6269 ± 0.0367 | 0.5969 ± 0.0632 |
    6 | 0.6692 ± 0.0312 | 0.6188 ± 0.0500 | 0.6069 ± 0.0245 | 0.7156 ± 0.0677 |
    7 | 0.6217 ± 0.0668 | 0.6438 ± 0.0940 | 0.6172 ± 0.0572 | 0.6656 ± 0.0906 |
    8 | 0.6679 ± 0.0472 | 0.5687 ± 0.0824 | 0.5943 ± 0.0852 | 0.7000 ± 0.0940 |
    9 | 0.7121 ± 0.1517 | 0.6375 ± 0.0829 | 0.6282 ± 0.0482 | 0.6687 ± 0.0929 |
   10 | 0.5936 ± 0.0319 | 0.7312 ± 0.0424 | 0.6369 ± 0.0474 | 0.6000 ± 0.0710 |
   11 | 0.6236 ± 0.0282 | 0.6687 ± 0.0643 | 0.6268 ± 0.0411 | 0.6438 ± 0.0897 |
   12 | 0.6782 ± 0.1135 | 0.6375 ± 0.0755 | 0.6161 ± 0.0342 | 0.6594 ± 0.0844 |
   13 | 0.6116 ± 0.0380 | 0.6375 ± 0.0852 | 0.6047 ± 0.0532 | 0.7250 ± 0.0556 |
   14 | 0.6436 ± 0.0808 | 0.6375 ± 0.0729 | 0.6509 ± 0.0781 | 0.5687 ± 0.1225 |
   15 | 0.6223 ± 0.0964 | 0.7063 ± 0.0545 | 0.6428 ± 0.0641 | 0.5813 ± 0.0715 |
   16 | 0.6382 ± 0.0691 | 0.6500 ± 0.0800 | 0.5851 ± 0.0564 | 0.7063 ± 0.0980 |
   17 | 0.5666 ± 0.0550 | 0.7312 ± 0.0729 | 0.6326 ± 0.0890 | 0.6312 ± 0.0935 |
   18 | 0.5528 ± 0.0266 | 0.7500 ± 0.0342 | 0.6105 ± 0.0535 | 0.6687 ± 0.0673 |
   19 | 0.5892 ± 0.0356 | 0.6875 ± 0.0395 | 0.5936 ± 0.0579 | 0.6781 ± 0.0641 |
   20 | 0.5856 ± 0.0582 | 0.6937 ± 0.0914 | 0.5721 ± 0.0745 | 0.7031 ± 0.0563 |
   21 | 0.5877 ± 0.0474 | 0.6687 ± 0.0424 | 0.5987 ± 0.1202 | 0.7125 ± 0.0800 |
   22 | 0.5829 ± 0.0826 | 0.6813 ± 0.0637 | 0.5851 ± 0.0714 | 0.7219 ± 0.0732 |
   23 | 0.5536 ± 0.1115 | 0.7750 ± 0.1035 | 0.5911 ± 0.0795 | 0.7156 ± 0.0616 |
   24 | 0.6415 ± 0.0866 | 0.6250 ± 0.1083 | 0.6361 ± 0.0989 | 0.6531 ± 0.0973 |
   25 | 0.5786 ± 0.0503 | 0.7063 ± 0.0424 | 0.6644 ± 0.0440 | 0.5906 ± 0.0600 |
   26 | 0.5673 ± 0.0612 | 0.7312 ± 0.0755 | 0.5981 ± 0.0771 | 0.6781 ± 0.0766 |
   27 | 0.5963 ± 0.0863 | 0.6750 ± 0.0729 | 0.5874 ± 0.0626 | 0.7125 ± 0.0667 |
   28 | 0.5923 ± 0.0565 | 0.6500 ± 0.0848 | 0.6913 ± 0.1232 | 0.6469 ± 0.1065 |
   29 | 0.6044 ± 0.0514 | 0.6625 ± 0.0750 | 0.5950 ± 0.0510 | 0.7188 ± 0.0791 |
   30 | 0.5766 ± 0.0598 | 0.6750 ± 0.0643 | 0.6832 ± 0.1181 | 0.5531 ± 0.0928 |
   31 | 0.5900 ± 0.0495 | 0.6750 ± 0.1000 | 0.6628 ± 0.0614 | 0.5750 ± 0.0715 |
   32 | 0.5566 ± 0.0392 | 0.6937 ± 0.0125 | 0.5988 ± 0.1025 | 0.6906 ± 0.1131 |
   33 | 0.6047 ± 0.0912 | 0.6937 ± 0.0956 | 0.5840 ± 0.0543 | 0.7031 ± 0.0702 |
   34 | 0.5703 ± 0.0559 | 0.7438 ± 0.0459 | 0.6011 ± 0.0624 | 0.6937 ± 0.1248 |
   35 | 0.5809 ± 0.0579 | 0.6750 ± 0.0781 | 0.5970 ± 0.0655 | 0.6750 ± 0.0817 |
   36 | 0.5916 ± 0.0631 | 0.7000 ± 0.0805 | 0.6329 ± 0.1136 | 0.6750 ± 0.0886 |
   37 | 0.5223 ± 0.0247 | 0.7500 ± 0.0342 | 0.6668 ± 0.1909 | 0.6906 ± 0.1246 |
   38 | 0.5774 ± 0.0390 | 0.6875 ± 0.0442 | 0.5833 ± 0.0675 | 0.6937 ± 0.0848 |
   39 | 0.6186 ± 0.0454 | 0.6687 ± 0.0424 | 0.6163 ± 0.0608 | 0.6531 ± 0.1002 |
   40 | 0.6146 ± 0.0422 | 0.7250 ± 0.0364 | 0.5611 ± 0.0454 | 0.7250 ± 0.0723 |
   41 | 0.6058 ± 0.0991 | 0.6625 ± 0.0935 | 0.5671 ± 0.0451 | 0.7188 ± 0.0464 |
   42 | 0.5956 ± 0.0817 | 0.6750 ± 0.0940 | 0.6035 ± 0.0698 | 0.6719 ± 0.0930 |
   43 | 0.5920 ± 0.0747 | 0.6562 ± 0.0948 | 0.5595 ± 0.0408 | 0.7031 ± 0.0597 |
   44 | 0.5816 ± 0.0785 | 0.6937 ± 0.0893 | 0.5777 ± 0.0457 | 0.7219 ± 0.0758 |
   45 | 0.6204 ± 0.0604 | 0.5750 ± 0.0919 | 0.6041 ± 0.0635 | 0.6281 ± 0.0600 |
   46 | 0.6164 ± 0.0884 | 0.6562 ± 0.0656 | 0.5774 ± 0.0546 | 0.6844 ± 0.0662 |
   47 | 0.6495 ± 0.0851 | 0.6813 ± 0.0538 | 0.6323 ± 0.0906 | 0.6469 ± 0.0839 |
   48 | 0.5985 ± 0.0636 | 0.7000 ± 0.0580 | 0.6151 ± 0.0480 | 0.6656 ± 0.0685 |
   49 | 0.6753 ± 0.0347 | 0.5938 ± 0.0948 | 0.5595 ± 0.0514 | 0.7219 ± 0.0677 |
   50 | 0.6340 ± 0.0953 | 0.6937 ± 0.0459 | 0.7229 ± 0.0566 | 0.5406 ± 0.0727 |
   51 | 0.5767 ± 0.0570 | 0.7125 ± 0.0956 | 0.6411 ± 0.0748 | 0.6406 ± 0.0908 |
   52 | 0.5486 ± 0.0843 | 0.7000 ± 0.0805 | 0.5584 ± 0.0442 | 0.7156 ± 0.0771 |
   53 | 0.6258 ± 0.0746 | 0.6750 ± 0.0829 | 0.5733 ± 0.0740 | 0.6781 ± 0.0641 |
   54 | 0.6139 ± 0.0969 | 0.6687 ± 0.0940 | 0.6351 ± 0.0956 | 0.6813 ± 0.0788 |
   55 | 0.6421 ± 0.0230 | 0.6125 ± 0.0805 | 0.6040 ± 0.0535 | 0.7406 ± 0.0753 |
   56 | 0.6178 ± 0.1298 | 0.6875 ± 0.1064 | 0.6228 ± 0.0828 | 0.6656 ± 0.0815 |
   57 | 0.5303 ± 0.0372 | 0.7750 ± 0.0415 | 0.5202 ± 0.0657 | 0.7594 ± 0.0791 |
   58 | 0.5951 ± 0.0278 | 0.6875 ± 0.0656 | 0.6649 ± 0.0627 | 0.6719 ± 0.0702 |
   59 | 0.5699 ± 0.0771 | 0.6500 ± 0.0824 | 0.5654 ± 0.0434 | 0.6937 ± 0.0788 |
   60 | 0.6635 ± 0.0852 | 0.6687 ± 0.0424 | 0.5859 ± 0.0564 | 0.6875 ± 0.0803 |
   61 | 0.5377 ± 0.0708 | 0.7250 ± 0.0776 | 0.5631 ± 0.0605 | 0.7063 ± 0.0673 |
   62 | 0.5450 ± 0.0387 | 0.7500 ± 0.0395 | 0.6138 ± 0.1046 | 0.6906 ± 0.0691 |
   63 | 0.6583 ± 0.0606 | 0.6188 ± 0.0824 | 0.5836 ± 0.0492 | 0.6594 ± 0.0900 |
   64 | 0.5710 ± 0.0394 | 0.6562 ± 0.0559 | 0.6258 ± 0.0789 | 0.6094 ± 0.0908 |
   65 | 0.5722 ± 0.0875 | 0.6937 ± 0.1272 | 0.6042 ± 0.0453 | 0.6219 ± 0.0355 |
   66 | 0.6097 ± 0.0711 | 0.6687 ± 0.0702 | 0.6086 ± 0.0391 | 0.6500 ± 0.0653 |
   67 | 0.5537 ± 0.0697 | 0.7500 ± 0.0765 | 0.5979 ± 0.0406 | 0.6969 ± 0.0397 |
   68 | 0.6679 ± 0.1452 | 0.6125 ± 0.1163 | 0.6440 ± 0.0731 | 0.5594 ± 0.0820 |
   69 | 0.5498 ± 0.0373 | 0.7438 ± 0.0364 | 0.6273 ± 0.0885 | 0.6906 ± 0.0867 |
   70 | 0.5368 ± 0.0280 | 0.7250 ± 0.0500 | 0.5740 ± 0.0871 | 0.7156 ± 0.0493 |
   71 | 0.5405 ± 0.0339 | 0.7063 ± 0.0153 | 0.5773 ± 0.0793 | 0.6844 ± 0.0796 |
   72 | 0.5509 ± 0.0709 | 0.7125 ± 0.0637 | 0.5431 ± 0.0829 | 0.7375 ± 0.0781 |
   73 | 0.5853 ± 0.0779 | 0.6937 ± 0.0914 | 0.5760 ± 0.0537 | 0.6937 ± 0.0750 |
   74 | 0.6428 ± 0.0589 | 0.6750 ± 0.0673 | 0.6127 ± 0.0603 | 0.5938 ± 0.0895 |
   75 | 0.7050 ± 0.1339 | 0.6687 ± 0.0805 | 0.6866 ± 0.1375 | 0.6344 ± 0.0928 |
   76 | 0.6879 ± 0.0812 | 0.6250 ± 0.0884 | 0.7328 ± 0.1036 | 0.5625 ± 0.0916 |
   77 | 0.6635 ± 0.0769 | 0.6188 ± 0.0364 | 0.5804 ± 0.0756 | 0.7406 ± 0.0917 |
   78 | 0.6881 ± 0.1584 | 0.6500 ± 0.0776 | 0.7402 ± 0.1610 | 0.5844 ± 0.0959 |
   79 | 0.6280 ± 0.1001 | 0.7000 ± 0.0729 | 0.5879 ± 0.0968 | 0.6844 ± 0.0784 |
   80 | 0.6279 ± 0.0433 | 0.6750 ± 0.0319 | 0.5997 ± 0.0592 | 0.6250 ± 0.0504 |
   81 | 0.5394 ± 0.0641 | 0.7250 ± 0.0637 | 0.6217 ± 0.0765 | 0.6813 ± 0.0573 |
   82 | 0.6387 ± 0.0961 | 0.7000 ± 0.0643 | 0.5924 ± 0.0884 | 0.7094 ± 0.0803 |
   83 | 0.5847 ± 0.0633 | 0.7250 ± 0.0538 | 0.5835 ± 0.0628 | 0.6875 ± 0.0765 |
   84 | 0.5898 ± 0.0896 | 0.7000 ± 0.0643 | 0.5748 ± 0.0601 | 0.7000 ± 0.0673 |
   85 | 0.6667 ± 0.0880 | 0.6500 ± 0.0935 | 0.5919 ± 0.0478 | 0.6903 ± 0.0452 |
   86 | 0.5717 ± 0.0872 | 0.7000 ± 0.0875 | 0.5641 ± 0.0679 | 0.6937 ± 0.0750 |
   87 | 0.5481 ± 0.0630 | 0.7188 ± 0.0559 | 0.6197 ± 0.0939 | 0.6531 ± 0.0771 |
   88 | 0.5263 ± 0.0835 | 0.7562 ± 0.0776 | 0.5583 ± 0.0716 | 0.6969 ± 0.0740 |
   89 | 0.5563 ± 0.0649 | 0.7125 ± 0.0637 | 0.6395 ± 0.1120 | 0.6438 ± 0.1075 |
   90 | 0.6219 ± 0.0990 | 0.7063 ± 0.0580 | 0.5617 ± 0.0707 | 0.7188 ± 0.0625 |
   91 | 0.5779 ± 0.0812 | 0.7125 ± 0.1209 | 0.5656 ± 0.0525 | 0.7000 ± 0.0596 |
   92 | 0.6285 ± 0.0840 | 0.6188 ± 0.0723 | 0.6348 ± 0.0657 | 0.6469 ± 0.0713 |
   93 | 0.6356 ± 0.0866 | 0.6562 ± 0.0740 | 0.5760 ± 0.0728 | 0.7063 ± 0.0897 |
   94 | 0.5560 ± 0.1678 | 0.6750 ± 0.1932 | 0.5893 ± 0.0608 | 0.5969 ± 0.0771 |
   95 | 0.5718 ± 0.0741 | 0.6813 ± 0.1035 | 0.5858 ± 0.0790 | 0.6813 ± 0.0976 |
   96 | 0.5841 ± 0.0724 | 0.7188 ± 0.0862 | 0.6423 ± 0.1400 | 0.6562 ± 0.1194 |
   97 | 0.5951 ± 0.0398 | 0.6750 ± 0.0468 | 0.5840 ± 0.0518 | 0.6906 ± 0.0832 |
   98 | 0.5773 ± 0.0652 | 0.7188 ± 0.0927 | 0.5586 ± 0.0593 | 0.7125 ± 0.0622 |
   99 | 0.5424 ± 0.0500 | 0.7188 ± 0.0442 | 0.5920 ± 0.0559 | 0.6875 ± 0.0726 |
  100 | 0.5482 ± 0.0704 | 0.7250 ± 0.0935 | 0.6017 ± 0.0949 | 0.6625 ± 0.0813 |
  101 | 0.5533 ± 0.0910 | 0.6813 ± 0.0637 | 0.5970 ± 0.0952 | 0.5844 ± 0.0884 |
  102 | 0.6403 ± 0.0241 | 0.6562 ± 0.0593 | 0.6485 ± 0.0747 | 0.6156 ± 0.0948 |
  103 | 0.6853 ± 0.0953 | 0.5875 ± 0.1302 | 0.6088 ± 0.0427 | 0.6594 ± 0.0473 |
  104 | 0.5866 ± 0.0824 | 0.6813 ± 0.0723 | 0.5461 ± 0.0556 | 0.7188 ± 0.0656 |
  105 | 0.6062 ± 0.0535 | 0.6125 ± 0.1128 | 0.5840 ± 0.0828 | 0.6656 ± 0.0699 |
  106 | 0.6430 ± 0.0792 | 0.6062 ± 0.0643 | 0.6126 ± 0.0761 | 0.6813 ± 0.0904 |
  107 | 0.6065 ± 0.0376 | 0.5875 ± 0.0500 | 0.6211 ± 0.0667 | 0.5375 ± 0.0836 |
  108 | 0.5928 ± 0.0879 | 0.6937 ± 0.1016 | 0.5623 ± 0.0571 | 0.7031 ± 0.0659 |
  109 | 0.5981 ± 0.1532 | 0.6937 ± 0.1072 | 0.5904 ± 0.0878 | 0.6937 ± 0.0996 |
  110 | 0.5748 ± 0.1138 | 0.6875 ± 0.0442 | 0.7006 ± 0.0741 | 0.5938 ± 0.0541 |
  111 | 0.5400 ± 0.0531 | 0.7250 ± 0.0696 | 0.5713 ± 0.0560 | 0.6969 ± 0.0594 |
  112 | 0.5747 ± 0.0628 | 0.7000 ± 0.0545 | 0.5911 ± 0.0917 | 0.7031 ± 0.0794 |
  113 | 0.6039 ± 0.1328 | 0.6687 ± 0.1447 | 0.6001 ± 0.0733 | 0.7063 ± 0.0864 |
  114 | 0.5458 ± 0.0386 | 0.7125 ± 0.0306 | 0.5998 ± 0.0593 | 0.7094 ± 0.0641 |
  115 | 0.5805 ± 0.0714 | 0.6875 ± 0.0685 | 0.6029 ± 0.1032 | 0.6750 ± 0.1019 |
  116 | 0.5269 ± 0.0716 | 0.7750 ± 0.0415 | 0.6697 ± 0.1174 | 0.6500 ± 0.0750 |
  117 | 0.5030 ± 0.0456 | 0.7688 ± 0.0508 | 0.5680 ± 0.1293 | 0.7344 ± 0.1103 |
  118 | 0.5918 ± 0.1282 | 0.6625 ± 0.0935 | 0.5619 ± 0.0847 | 0.7312 ± 0.0829 |
  119 | 0.6735 ± 0.1141 | 0.6687 ± 0.0829 | 0.6061 ± 0.1414 | 0.6687 ± 0.1093 |
  120 | 0.6418 ± 0.0852 | 0.6188 ± 0.0459 | 0.5980 ± 0.1008 | 0.5844 ± 0.1056 |
  121 | 0.6182 ± 0.1152 | 0.7375 ± 0.0805 | 0.5655 ± 0.0656 | 0.6969 ± 0.1027 |
  122 | 0.5936 ± 0.0198 | 0.5938 ± 0.0656 | 0.5927 ± 0.0932 | 0.6781 ± 0.0895 |
  123 | 0.5880 ± 0.0658 | 0.6250 ± 0.0523 | 0.5834 ± 0.0776 | 0.6094 ± 0.1283 |
  124 | 0.5471 ± 0.0437 | 0.7000 ± 0.0545 | 0.6029 ± 0.0718 | 0.6062 ± 0.0817 |
  125 | 0.6198 ± 0.0620 | 0.6625 ± 0.0776 | 0.5530 ± 0.0503 | 0.7312 ± 0.0527 |
  126 | 0.5822 ± 0.0915 | 0.7000 ± 0.0755 | 0.5735 ± 0.0650 | 0.6875 ± 0.0593 |
  127 | 0.5960 ± 0.0858 | 0.6875 ± 0.0559 | 0.5446 ± 0.0281 | 0.7156 ± 0.0632 |
  128 | 0.6570 ± 0.0334 | 0.5375 ± 0.0776 | 0.5342 ± 0.0458 | 0.7188 ± 0.0625 |
  129 | 0.5839 ± 0.0562 | 0.6312 ± 0.1142 | 0.5849 ± 0.0644 | 0.7219 ± 0.0867 |
  130 | 0.5485 ± 0.0425 | 0.7125 ± 0.0538 | 0.5855 ± 0.0654 | 0.6844 ± 0.0832 |
  131 | 0.5769 ± 0.0472 | 0.6813 ± 0.0500 | 0.6001 ± 0.0880 | 0.6750 ± 0.0886 |
  132 | 0.5324 ± 0.0504 | 0.7500 ± 0.0342 | 0.5814 ± 0.0545 | 0.6937 ± 0.0653 |
  133 | 0.5938 ± 0.0679 | 0.7188 ± 0.0442 | 0.6014 ± 0.0661 | 0.6594 ± 0.0732 |
  134 | 0.6141 ± 0.0776 | 0.6875 ± 0.0713 | 0.5122 ± 0.0829 | 0.7312 ± 0.0673 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6458 ± 0.0111 | 0.6188 ± 0.0459 | 0.6512 ± 0.0229 | 0.6406 ± 0.0716 |
    2 | 0.5870 ± 0.0525 | 0.6875 ± 0.0988 | 0.6557 ± 0.0590 | 0.5750 ± 0.0628 |
    3 | 0.6353 ± 0.0230 | 0.6125 ± 0.0319 | 0.6288 ± 0.0716 | 0.5781 ± 0.0769 |
    4 | 0.6282 ± 0.0519 | 0.6000 ± 0.0306 | 0.6241 ± 0.0562 | 0.6687 ± 0.0715 |
    5 | 0.5862 ± 0.0342 | 0.6687 ± 0.0319 | 0.5974 ± 0.0328 | 0.6969 ± 0.0656 |
    6 | 0.6177 ± 0.0311 | 0.6062 ± 0.0545 | 0.5972 ± 0.0394 | 0.6188 ± 0.0538 |
    7 | 0.6170 ± 0.0488 | 0.6500 ± 0.0364 | 0.6239 ± 0.0389 | 0.6562 ± 0.0609 |
    8 | 0.6159 ± 0.0725 | 0.6500 ± 0.0800 | 0.6363 ± 0.0807 | 0.6406 ± 0.0908 |
    9 | 0.5712 ± 0.0171 | 0.7063 ± 0.0319 | 0.6036 ± 0.0360 | 0.7063 ± 0.0400 |
   10 | 0.6465 ± 0.0344 | 0.6312 ± 0.0723 | 0.6403 ± 0.0645 | 0.5938 ± 0.0862 |
   11 | 0.6600 ± 0.0270 | 0.6125 ± 0.0508 | 0.6176 ± 0.0533 | 0.6531 ± 0.0616 |
   12 | 0.6119 ± 0.0516 | 0.6750 ± 0.0729 | 0.5931 ± 0.0632 | 0.6969 ± 0.0626 |
   13 | 0.5679 ± 0.0484 | 0.6750 ± 0.0897 | 0.5975 ± 0.0606 | 0.7000 ± 0.0875 |
   14 | 0.6046 ± 0.0643 | 0.7000 ± 0.0673 | 0.6247 ± 0.0870 | 0.6562 ± 0.0850 |
   15 | 0.5809 ± 0.0398 | 0.7063 ± 0.0468 | 0.5948 ± 0.0488 | 0.6750 ± 0.0673 |
   16 | 0.6492 ± 0.0285 | 0.6188 ± 0.0364 | 0.5999 ± 0.0612 | 0.6875 ± 0.0753 |
   17 | 0.6040 ± 0.0795 | 0.6438 ± 0.1128 | 0.6617 ± 0.0692 | 0.5656 ± 0.1012 |
   18 | 0.5518 ± 0.0153 | 0.7188 ± 0.0442 | 0.5735 ± 0.0860 | 0.6969 ± 0.0989 |
   19 | 0.5719 ± 0.0452 | 0.7063 ± 0.0545 | 0.6212 ± 0.0929 | 0.6438 ± 0.1102 |
   20 | 0.6085 ± 0.0269 | 0.6562 ± 0.0442 | 0.5905 ± 0.0675 | 0.7125 ± 0.0800 |
   21 | 0.6105 ± 0.0352 | 0.6750 ± 0.0702 | 0.6049 ± 0.0697 | 0.6937 ± 0.0788 |
   22 | 0.5891 ± 0.0354 | 0.7188 ± 0.0342 | 0.6034 ± 0.0749 | 0.6562 ± 0.0593 |
   23 | 0.5833 ± 0.0669 | 0.6875 ± 0.1266 | 0.6527 ± 0.0856 | 0.6500 ± 0.1090 |
   24 | 0.5636 ± 0.0464 | 0.7125 ± 0.1053 | 0.5999 ± 0.0440 | 0.6656 ± 0.0505 |
   25 | 0.6188 ± 0.0842 | 0.6625 ± 0.1072 | 0.5637 ± 0.0438 | 0.7281 ± 0.0610 |
   26 | 0.6415 ± 0.0621 | 0.5813 ± 0.0805 | 0.5974 ± 0.0519 | 0.6969 ± 0.0727 |
   27 | 0.6441 ± 0.0337 | 0.6062 ± 0.0643 | 0.5965 ± 0.0561 | 0.6844 ± 0.0921 |
   28 | 0.6738 ± 0.0563 | 0.6438 ± 0.0580 | 0.6354 ± 0.0578 | 0.6687 ± 0.0424 |
   29 | 0.5634 ± 0.0373 | 0.7312 ± 0.0250 | 0.6077 ± 0.0679 | 0.6875 ± 0.0895 |
   30 | 0.5908 ± 0.0714 | 0.6875 ± 0.0839 | 0.6126 ± 0.0640 | 0.6625 ± 0.0556 |
   31 | 0.6112 ± 0.0601 | 0.6000 ± 0.0871 | 0.5938 ± 0.0652 | 0.6562 ± 0.0895 |
   32 | 0.6058 ± 0.0427 | 0.6937 ± 0.0935 | 0.5921 ± 0.0520 | 0.7188 ± 0.0656 |
   33 | 0.6729 ± 0.0377 | 0.5563 ± 0.0606 | 0.6613 ± 0.0589 | 0.5844 ± 0.0524 |
   34 | 0.5983 ± 0.0601 | 0.6937 ± 0.0871 | 0.5718 ± 0.0547 | 0.7250 ± 0.0737 |
   35 | 0.6480 ± 0.0909 | 0.6250 ± 0.0862 | 0.6138 ± 0.0532 | 0.6531 ± 0.0632 |
   36 | 0.6193 ± 0.0757 | 0.5938 ± 0.1046 | 0.5904 ± 0.0557 | 0.6531 ± 0.0616 |
   37 | 0.5511 ± 0.0454 | 0.7312 ± 0.0673 | 0.6351 ± 0.0944 | 0.6562 ± 0.0791 |
   38 | 0.6090 ± 0.0789 | 0.6750 ± 0.0508 | 0.5707 ± 0.0485 | 0.7125 ± 0.0904 |
   39 | 0.6150 ± 0.0481 | 0.7063 ± 0.0673 | 0.5901 ± 0.0423 | 0.7063 ± 0.0375 |
   40 | 0.6689 ± 0.0699 | 0.6500 ± 0.1035 | 0.5857 ± 0.0582 | 0.6937 ± 0.1035 |
   41 | 0.5928 ± 0.0527 | 0.6937 ± 0.0364 | 0.6375 ± 0.0752 | 0.6281 ± 0.1068 |
   42 | 0.6085 ± 0.0408 | 0.6813 ± 0.0667 | 0.5697 ± 0.0530 | 0.7219 ± 0.0784 |
   43 | 0.5977 ± 0.0262 | 0.6875 ± 0.0656 | 0.6543 ± 0.0538 | 0.6125 ± 0.0375 |
   44 | 0.5904 ± 0.0450 | 0.6687 ± 0.0424 | 0.5824 ± 0.0471 | 0.7063 ± 0.0596 |
   45 | 0.5356 ± 0.0205 | 0.7688 ± 0.0424 | 0.6267 ± 0.0728 | 0.6781 ± 0.0699 |
   46 | 0.5533 ± 0.0655 | 0.7125 ± 0.1317 | 0.6196 ± 0.0570 | 0.6813 ± 0.0848 |
   47 | 0.5686 ± 0.0927 | 0.7063 ± 0.0919 | 0.6058 ± 0.0601 | 0.6813 ± 0.0500 |
   48 | 0.5484 ± 0.0275 | 0.7375 ± 0.0319 | 0.5845 ± 0.0863 | 0.7281 ± 0.1046 |
   49 | 0.5943 ± 0.0769 | 0.6937 ± 0.0776 | 0.6139 ± 0.0590 | 0.6906 ± 0.0784 |
   50 | 0.5828 ± 0.0589 | 0.7250 ± 0.0500 | 0.6085 ± 0.0784 | 0.6781 ± 0.0884 |
   51 | 0.6020 ± 0.0770 | 0.6625 ± 0.0996 | 0.6053 ± 0.0409 | 0.6906 ± 0.0732 |
   52 | 0.6067 ± 0.0521 | 0.6750 ± 0.0897 | 0.6184 ± 0.0606 | 0.6750 ± 0.0628 |
   53 | 0.6134 ± 0.0466 | 0.7063 ± 0.0424 | 0.6122 ± 0.0709 | 0.6937 ± 0.0667 |
   54 | 0.5589 ± 0.0509 | 0.7625 ± 0.0580 | 0.5859 ± 0.0802 | 0.6813 ± 0.0996 |
   55 | 0.6335 ± 0.0354 | 0.6562 ± 0.0685 | 0.6384 ± 0.0721 | 0.6625 ± 0.1062 |
   56 | 0.6005 ± 0.0232 | 0.6687 ± 0.0375 | 0.6253 ± 0.0496 | 0.6719 ± 0.0546 |
   57 | 0.5979 ± 0.0310 | 0.6562 ± 0.0442 | 0.6238 ± 0.0484 | 0.6156 ± 0.0505 |
   58 | 0.6019 ± 0.0694 | 0.6750 ± 0.0729 | 0.6034 ± 0.0593 | 0.6656 ± 0.0766 |
   59 | 0.5761 ± 0.0812 | 0.6875 ± 0.0713 | 0.6048 ± 0.0674 | 0.6531 ± 0.1122 |
   60 | 0.5940 ± 0.0636 | 0.6750 ± 0.0673 | 0.5763 ± 0.0640 | 0.7094 ± 0.0895 |
   61 | 0.5834 ± 0.0718 | 0.7000 ± 0.0852 | 0.6068 ± 0.0677 | 0.6719 ± 0.0730 |
   62 | 0.6074 ± 0.0491 | 0.6562 ± 0.0713 | 0.6193 ± 0.0699 | 0.6656 ± 0.0873 |
   63 | 0.5937 ± 0.0447 | 0.6813 ± 0.0234 | 0.5718 ± 0.0770 | 0.7063 ± 0.0908 |
   64 | 0.5799 ± 0.0297 | 0.6813 ± 0.0538 | 0.5917 ± 0.0685 | 0.6594 ± 0.0844 |
   65 | 0.6265 ± 0.0773 | 0.6875 ± 0.0593 | 0.5944 ± 0.0620 | 0.6750 ± 0.1066 |
   66 | 0.5571 ± 0.0223 | 0.7312 ± 0.0424 | 0.5937 ± 0.0501 | 0.6594 ± 0.0677 |
   67 | 0.5908 ± 0.0322 | 0.6750 ± 0.0643 | 0.6067 ± 0.0535 | 0.6750 ± 0.0755 |
   68 | 0.6080 ± 0.0794 | 0.5687 ± 0.1035 | 0.6032 ± 0.0566 | 0.6813 ± 0.0956 |
   69 | 0.6241 ± 0.1083 | 0.7063 ± 0.1057 | 0.5988 ± 0.0785 | 0.6844 ± 0.0867 |
   70 | 0.5809 ± 0.0508 | 0.6813 ± 0.0500 | 0.6064 ± 0.0743 | 0.6781 ± 0.0791 |
   71 | 0.5940 ± 0.0354 | 0.6750 ± 0.0468 | 0.5966 ± 0.0512 | 0.6750 ± 0.0643 |
   72 | 0.6080 ± 0.0620 | 0.6312 ± 0.0824 | 0.6176 ± 0.0460 | 0.6562 ± 0.0670 |
   73 | 0.5808 ± 0.0538 | 0.7188 ± 0.0523 | 0.5491 ± 0.0489 | 0.7156 ± 0.0632 |
   74 | 0.5755 ± 0.0748 | 0.7188 ± 0.0815 | 0.6329 ± 0.0586 | 0.6344 ± 0.0938 |
   75 | 0.5495 ± 0.0878 | 0.7562 ± 0.0800 | 0.5747 ± 0.0436 | 0.7031 ± 0.0580 |
   76 | 0.6234 ± 0.0349 | 0.6687 ± 0.0319 | 0.5887 ± 0.0647 | 0.6813 ± 0.0682 |
   77 | 0.5763 ± 0.0650 | 0.6937 ± 0.0914 | 0.6872 ± 0.0394 | 0.5000 ± 0.0609 |
   78 | 0.6870 ± 0.0657 | 0.6375 ± 0.0755 | 0.6108 ± 0.0654 | 0.6719 ± 0.0794 |
   79 | 0.6017 ± 0.0629 | 0.6375 ± 0.0805 | 0.6055 ± 0.0511 | 0.6562 ± 0.0850 |
   80 | 0.6410 ± 0.0447 | 0.6312 ± 0.0776 | 0.5948 ± 0.0468 | 0.6969 ± 0.0577 |
   81 | 0.6276 ± 0.1451 | 0.6875 ± 0.1340 | 0.5837 ± 0.0576 | 0.7156 ± 0.0808 |
   82 | 0.5652 ± 0.0449 | 0.7000 ± 0.0468 | 0.5952 ± 0.0692 | 0.6813 ± 0.0776 |
   83 | 0.6114 ± 0.0498 | 0.6687 ± 0.0319 | 0.5733 ± 0.0437 | 0.7125 ± 0.0653 |
   84 | 0.6524 ± 0.0446 | 0.5938 ± 0.0988 | 0.6025 ± 0.0398 | 0.6406 ± 0.0950 |
   85 | 0.6077 ± 0.0504 | 0.6750 ± 0.0805 | 0.6188 ± 0.0854 | 0.6788 ± 0.0913 |
   86 | 0.6578 ± 0.0754 | 0.5938 ± 0.0906 | 0.6279 ± 0.0391 | 0.6375 ± 0.0612 |
   87 | 0.6004 ± 0.0386 | 0.6312 ± 0.0606 | 0.6241 ± 0.0614 | 0.6188 ± 0.0776 |
   88 | 0.5859 ± 0.0785 | 0.6937 ± 0.0637 | 0.5862 ± 0.0730 | 0.6906 ± 0.1041 |
   89 | 0.6292 ± 0.0454 | 0.6750 ± 0.0805 | 0.5941 ± 0.0402 | 0.6875 ± 0.0815 |
   90 | 0.5781 ± 0.0882 | 0.7125 ± 0.1090 | 0.5624 ± 0.0493 | 0.7312 ± 0.0817 |
   91 | 0.5869 ± 0.0718 | 0.6937 ± 0.1108 | 0.5999 ± 0.0560 | 0.6813 ± 0.0788 |
   92 | 0.5816 ± 0.0629 | 0.7063 ± 0.1128 | 0.6121 ± 0.0736 | 0.6594 ± 0.0942 |
   93 | 0.5670 ± 0.0337 | 0.7312 ± 0.0755 | 0.6246 ± 0.0605 | 0.6625 ± 0.0653 |
   94 | 0.5792 ± 0.0677 | 0.6875 ± 0.1135 | 0.6128 ± 0.0709 | 0.6656 ± 0.0626 |
   95 | 0.5990 ± 0.1232 | 0.6438 ± 0.1259 | 0.6117 ± 0.0603 | 0.6937 ± 0.0763 |
   96 | 0.6011 ± 0.0974 | 0.6937 ± 0.0696 | 0.6195 ± 0.0516 | 0.6719 ± 0.0659 |
   97 | 0.6052 ± 0.0356 | 0.6312 ± 0.0538 | 0.5756 ± 0.0742 | 0.6937 ± 0.0925 |
   98 | 0.6510 ± 0.0527 | 0.6500 ± 0.0500 | 0.6360 ± 0.0411 | 0.6375 ± 0.0829 |
   99 | 0.6054 ± 0.0632 | 0.6750 ± 0.0673 | 0.6185 ± 0.1084 | 0.6687 ± 0.0875 |
  100 | 0.5725 ± 0.0569 | 0.7250 ± 0.0606 | 0.5800 ± 0.0402 | 0.7094 ± 0.0397 |
  101 | 0.5662 ± 0.0483 | 0.6500 ± 0.0800 | 0.6334 ± 0.0649 | 0.5906 ± 0.0855 |
  102 | 0.5878 ± 0.0443 | 0.6312 ± 0.0848 | 0.6134 ± 0.0979 | 0.6625 ± 0.1324 |
  103 | 0.6168 ± 0.0614 | 0.6625 ± 0.0848 | 0.5898 ± 0.0528 | 0.7031 ± 0.0961 |
  104 | 0.6029 ± 0.0386 | 0.6250 ± 0.0625 | 0.6073 ± 0.0705 | 0.6344 ± 0.1127 |
  105 | 0.5022 ± 0.0582 | 0.8000 ± 0.0508 | 0.5900 ± 0.0657 | 0.7125 ± 0.0710 |
  106 | 0.6086 ± 0.0931 | 0.6500 ± 0.1142 | 0.5758 ± 0.0534 | 0.6687 ± 0.0805 |
  107 | 0.6211 ± 0.0689 | 0.6438 ± 0.0875 | 0.6228 ± 0.0529 | 0.6438 ± 0.0468 |
  108 | 0.5633 ± 0.0345 | 0.6625 ± 0.0893 | 0.6199 ± 0.1094 | 0.6250 ± 0.1092 |
  109 | 0.5641 ± 0.0355 | 0.6937 ± 0.0606 | 0.5820 ± 0.0321 | 0.6875 ± 0.0625 |
  110 | 0.6078 ± 0.0535 | 0.6687 ± 0.0545 | 0.5553 ± 0.0498 | 0.7312 ± 0.0468 |
  111 | 0.6024 ± 0.0687 | 0.6500 ± 0.0696 | 0.5967 ± 0.0557 | 0.6844 ± 0.0632 |
  112 | 0.6108 ± 0.0474 | 0.6750 ± 0.0580 | 0.6087 ± 0.0646 | 0.6719 ± 0.0908 |
  113 | 0.5799 ± 0.0431 | 0.6875 ± 0.0713 | 0.6189 ± 0.0418 | 0.6344 ± 0.0671 |
  114 | 0.5880 ± 0.0662 | 0.6937 ± 0.0750 | 0.6086 ± 0.0754 | 0.6813 ± 0.0776 |
  115 | 0.6254 ± 0.0736 | 0.6312 ± 0.1035 | 0.5827 ± 0.0567 | 0.6438 ± 0.0929 |
  116 | 0.6013 ± 0.0671 | 0.6687 ± 0.0980 | 0.5846 ± 0.0539 | 0.6781 ± 0.0766 |
  117 | 0.5950 ± 0.0611 | 0.6500 ± 0.0667 | 0.6056 ± 0.0719 | 0.6875 ± 0.0753 |
  118 | 0.5722 ± 0.0504 | 0.6438 ± 0.0702 | 0.5797 ± 0.0559 | 0.6937 ± 0.0935 |
  119 | 0.6441 ± 0.0962 | 0.6562 ± 0.1425 | 0.5740 ± 0.0582 | 0.6750 ± 0.0852 |
  120 | 0.6038 ± 0.0775 | 0.6562 ± 0.0884 | 0.6212 ± 0.0575 | 0.6469 ± 0.0594 |
  121 | 0.6044 ± 0.0417 | 0.6250 ± 0.0523 | 0.5595 ± 0.0732 | 0.6813 ± 0.0696 |
  122 | 0.5685 ± 0.0675 | 0.7438 ± 0.0459 | 0.6899 ± 0.1152 | 0.6531 ± 0.0878 |
  123 | 0.6019 ± 0.0763 | 0.6625 ± 0.0996 | 0.5845 ± 0.0547 | 0.7125 ± 0.0637 |
  124 | 0.5544 ± 0.0490 | 0.7500 ± 0.0280 | 0.5806 ± 0.0339 | 0.7063 ± 0.0348 |
  125 | 0.6520 ± 0.0418 | 0.6062 ± 0.0980 | 0.5866 ± 0.0676 | 0.7000 ± 0.0990 |
  126 | 0.5877 ± 0.0491 | 0.7250 ± 0.0459 | 0.6230 ± 0.0822 | 0.6719 ± 0.1085 |
  127 | 0.6239 ± 0.0659 | 0.6438 ± 0.0755 | 0.5588 ± 0.0690 | 0.7281 ± 0.0626 |
  128 | 0.5427 ± 0.0496 | 0.7188 ± 0.0523 | 0.6269 ± 0.0589 | 0.6469 ± 0.0626 |
  129 | 0.6004 ± 0.0611 | 0.6750 ± 0.0643 | 0.5697 ± 0.0319 | 0.6937 ± 0.0500 |
  130 | 0.5445 ± 0.0449 | 0.7500 ± 0.0484 | 0.6400 ± 0.0685 | 0.6562 ± 0.0753 |
  131 | 0.6237 ± 0.0617 | 0.6625 ± 0.0459 | 0.5856 ± 0.0333 | 0.7250 ± 0.0459 |
  132 | 0.6079 ± 0.0651 | 0.6375 ± 0.0673 | 0.5968 ± 0.0655 | 0.6750 ± 0.0829 |
  133 | 0.5944 ± 0.0744 | 0.7000 ± 0.0755 | 0.5752 ± 0.0960 | 0.7125 ± 0.0824 |
  134 | 0.6295 ± 0.0491 | 0.6062 ± 0.0468 | 0.5885 ± 0.0717 | 0.7188 ± 0.0791 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6266 ± 0.0141 | 0.7000 ± 0.0643 | 0.6177 ± 0.0513 | 0.6781 ± 0.0641 |
    2 | 0.6439 ± 0.1022 | 0.6375 ± 0.1146 | 0.6337 ± 0.0476 | 0.6562 ± 0.0670 |
    3 | 0.6294 ± 0.0706 | 0.6312 ± 0.0871 | 0.6191 ± 0.0443 | 0.5969 ± 0.0832 |
    4 | 0.6496 ± 0.0885 | 0.5813 ± 0.1075 | 0.6066 ± 0.0578 | 0.6937 ± 0.0723 |
    5 | 0.5622 ± 0.0362 | 0.7063 ± 0.0580 | 0.6265 ± 0.0633 | 0.6531 ± 0.0732 |
    6 | 0.5963 ± 0.0336 | 0.7250 ± 0.0606 | 0.6091 ± 0.0396 | 0.6969 ± 0.0443 |
    7 | 0.6084 ± 0.0626 | 0.6562 ± 0.0740 | 0.5929 ± 0.0626 | 0.6750 ± 0.1019 |
    8 | 0.5651 ± 0.0789 | 0.7562 ± 0.0723 | 0.6072 ± 0.0679 | 0.6656 ± 0.0727 |
    9 | 0.6433 ± 0.1015 | 0.6813 ± 0.0996 | 0.6137 ± 0.0654 | 0.6594 ± 0.0771 |
   10 | 0.6423 ± 0.0661 | 0.6500 ± 0.0306 | 0.6225 ± 0.0730 | 0.6625 ± 0.0606 |
   11 | 0.6145 ± 0.0882 | 0.6250 ± 0.0625 | 0.6183 ± 0.0802 | 0.6281 ± 0.0820 |
   12 | 0.5770 ± 0.0398 | 0.6875 ± 0.0656 | 0.6298 ± 0.0755 | 0.6062 ± 0.1075 |
   13 | 0.5936 ± 0.0522 | 0.6937 ± 0.0606 | 0.6057 ± 0.0598 | 0.6687 ± 0.0817 |
   14 | 0.6018 ± 0.0571 | 0.6312 ± 0.0723 | 0.5871 ± 0.0392 | 0.6750 ± 0.0596 |
   15 | 0.5362 ± 0.0561 | 0.7562 ± 0.0956 | 0.5982 ± 0.0899 | 0.7094 ± 0.0938 |
   16 | 0.5539 ± 0.0707 | 0.7250 ± 0.0723 | 0.5930 ± 0.0540 | 0.7031 ± 0.0425 |
   17 | 0.6146 ± 0.0533 | 0.6562 ± 0.0395 | 0.6127 ± 0.0682 | 0.6719 ± 0.0806 |
   18 | 0.6202 ± 0.0505 | 0.6750 ± 0.0580 | 0.6199 ± 0.0354 | 0.6562 ± 0.0541 |
   19 | 0.5920 ± 0.0613 | 0.6687 ± 0.0875 | 0.5816 ± 0.0350 | 0.6219 ± 0.0406 |
   20 | 0.5818 ± 0.0311 | 0.7125 ± 0.0364 | 0.6066 ± 0.0385 | 0.6906 ± 0.0771 |
   21 | 0.6094 ± 0.0682 | 0.6750 ± 0.0319 | 0.5908 ± 0.0372 | 0.6750 ± 0.0348 |
   22 | 0.5615 ± 0.0813 | 0.6813 ± 0.0776 | 0.6235 ± 0.0672 | 0.6562 ± 0.0884 |
   23 | 0.6136 ± 0.0483 | 0.6937 ± 0.0606 | 0.6251 ± 0.0622 | 0.6156 ± 0.0827 |
   24 | 0.5354 ± 0.0589 | 0.7500 ± 0.0523 | 0.6045 ± 0.0947 | 0.7031 ± 0.0830 |
   25 | 0.6033 ± 0.0817 | 0.6937 ± 0.1035 | 0.5985 ± 0.0819 | 0.6750 ± 0.1093 |
   26 | 0.6358 ± 0.0557 | 0.6125 ± 0.0852 | 0.6340 ± 0.0412 | 0.5938 ± 0.0419 |
   27 | 0.6353 ± 0.0363 | 0.6750 ± 0.0468 | 0.5987 ± 0.1047 | 0.7000 ± 0.0908 |
   28 | 0.6002 ± 0.0387 | 0.6625 ± 0.0723 | 0.5976 ± 0.0424 | 0.6344 ± 0.0577 |
   29 | 0.6044 ± 0.0444 | 0.6937 ± 0.0606 | 0.6092 ± 0.0522 | 0.7000 ± 0.0596 |
   30 | 0.5859 ± 0.0592 | 0.7000 ± 0.0875 | 0.5784 ± 0.0498 | 0.7156 ± 0.0855 |
   31 | 0.6056 ± 0.0543 | 0.6813 ± 0.0667 | 0.6465 ± 0.0721 | 0.6312 ± 0.0893 |
   32 | 0.5472 ± 0.0548 | 0.7688 ± 0.0755 | 0.5886 ± 0.0693 | 0.6781 ± 0.0873 |
   33 | 0.5459 ± 0.0519 | 0.7250 ± 0.0538 | 0.6244 ± 0.0913 | 0.6625 ± 0.0800 |
   34 | 0.5830 ± 0.0367 | 0.6687 ± 0.0980 | 0.6152 ± 0.0466 | 0.7156 ± 0.0796 |
   35 | 0.5990 ± 0.0457 | 0.6875 ± 0.0395 | 0.6067 ± 0.0620 | 0.6781 ± 0.0641 |
   36 | 0.6167 ± 0.1013 | 0.7063 ± 0.1019 | 0.6679 ± 0.0881 | 0.6281 ± 0.0952 |
   37 | 0.6859 ± 0.0896 | 0.6438 ± 0.0875 | 0.6060 ± 0.0501 | 0.6719 ± 0.0546 |
   38 | 0.5705 ± 0.0759 | 0.7250 ± 0.0750 | 0.5819 ± 0.0588 | 0.7063 ± 0.0688 |
   39 | 0.6626 ± 0.0611 | 0.5875 ± 0.0573 | 0.5969 ± 0.0716 | 0.6375 ± 0.0980 |
   40 | 0.6019 ± 0.0514 | 0.6875 ± 0.0713 | 0.6066 ± 0.0821 | 0.6781 ± 0.0815 |
   41 | 0.6159 ± 0.1187 | 0.6875 ± 0.1169 | 0.6531 ± 0.0663 | 0.6219 ± 0.0549 |
   42 | 0.5790 ± 0.0648 | 0.7125 ± 0.1053 | 0.6311 ± 0.0941 | 0.6344 ± 0.1018 |
   43 | 0.5571 ± 0.0776 | 0.7438 ± 0.0606 | 0.6350 ± 0.0438 | 0.6594 ± 0.0758 |
   44 | 0.6708 ± 0.0616 | 0.6125 ± 0.0580 | 0.5872 ± 0.0718 | 0.7031 ± 0.0981 |
   45 | 0.6061 ± 0.0156 | 0.6562 ± 0.0559 | 0.5995 ± 0.0540 | 0.6813 ± 0.0573 |
   46 | 0.6022 ± 0.0356 | 0.6500 ± 0.0776 | 0.6036 ± 0.0543 | 0.6281 ± 0.0584 |
   47 | 0.5969 ± 0.0804 | 0.7000 ± 0.0897 | 0.5545 ± 0.0588 | 0.7156 ± 0.0719 |
   48 | 0.5890 ± 0.0611 | 0.6375 ± 0.0643 | 0.5922 ± 0.0351 | 0.6469 ± 0.0884 |
   49 | 0.5951 ± 0.0178 | 0.7000 ± 0.0319 | 0.6171 ± 0.0591 | 0.6625 ± 0.0637 |
   50 | 0.6739 ± 0.0299 | 0.6438 ± 0.0319 | 0.6523 ± 0.0817 | 0.6500 ± 0.0893 |
   51 | 0.5869 ± 0.0619 | 0.7250 ± 0.0500 | 0.5697 ± 0.0685 | 0.7219 ± 0.0820 |
   52 | 0.5999 ± 0.0579 | 0.6750 ± 0.0424 | 0.5828 ± 0.0392 | 0.6750 ± 0.0580 |
   53 | 0.6055 ± 0.0788 | 0.6687 ± 0.0580 | 0.5928 ± 0.0719 | 0.6906 ± 0.0796 |
   54 | 0.6570 ± 0.0673 | 0.6625 ± 0.0776 | 0.6058 ± 0.0624 | 0.6719 ± 0.0716 |
   55 | 0.6180 ± 0.0592 | 0.6562 ± 0.0713 | 0.5861 ± 0.0817 | 0.7125 ± 0.0836 |
   56 | 0.6570 ± 0.0481 | 0.6062 ± 0.0612 | 0.6239 ± 0.0516 | 0.6750 ± 0.0658 |
   57 | 0.5502 ± 0.0117 | 0.7188 ± 0.0342 | 0.6026 ± 0.0772 | 0.7063 ± 0.0755 |
   58 | 0.5950 ± 0.1046 | 0.6750 ± 0.1000 | 0.5803 ± 0.0542 | 0.6937 ± 0.0800 |
   59 | 0.5625 ± 0.0723 | 0.7312 ± 0.0702 | 0.5933 ± 0.0734 | 0.7000 ± 0.0742 |
   60 | 0.5795 ± 0.0727 | 0.6687 ± 0.1075 | 0.6098 ± 0.0583 | 0.6719 ± 0.0794 |
   61 | 0.6043 ± 0.0249 | 0.7000 ± 0.0319 | 0.6240 ± 0.0560 | 0.6625 ± 0.0390 |
   62 | 0.6050 ± 0.0623 | 0.6500 ± 0.0637 | 0.5809 ± 0.0468 | 0.7063 ± 0.0446 |
   63 | 0.5769 ± 0.0617 | 0.6750 ± 0.0755 | 0.5616 ± 0.0537 | 0.6813 ± 0.0653 |
   64 | 0.6124 ± 0.0570 | 0.6813 ± 0.0538 | 0.6207 ± 0.0726 | 0.6687 ± 0.0897 |
   65 | 0.6181 ± 0.0284 | 0.6188 ± 0.0848 | 0.5881 ± 0.0634 | 0.6750 ± 0.0742 |
   66 | 0.5861 ± 0.0916 | 0.7438 ± 0.0750 | 0.5589 ± 0.0542 | 0.7469 ± 0.0784 |
   67 | 0.5789 ± 0.0484 | 0.6750 ± 0.0729 | 0.5827 ± 0.0681 | 0.6875 ± 0.0850 |
   68 | 0.5896 ± 0.0460 | 0.7063 ± 0.0468 | 0.5749 ± 0.0704 | 0.7031 ± 0.0806 |
   69 | 0.5707 ± 0.0929 | 0.6937 ± 0.1090 | 0.6018 ± 0.0405 | 0.6406 ± 0.0743 |
   70 | 0.6050 ± 0.0790 | 0.6312 ± 0.1035 | 0.5936 ± 0.0545 | 0.6625 ± 0.0750 |
   71 | 0.5914 ± 0.0570 | 0.6625 ± 0.0538 | 0.5857 ± 0.0676 | 0.7250 ± 0.0606 |
   72 | 0.5816 ± 0.0734 | 0.6750 ± 0.1019 | 0.5660 ± 0.0877 | 0.6750 ± 0.1038 |
   73 | 0.5807 ± 0.0423 | 0.6875 ± 0.0484 | 0.5729 ± 0.0656 | 0.7125 ± 0.0813 |
   74 | 0.5757 ± 0.0401 | 0.7063 ± 0.0375 | 0.5914 ± 0.0709 | 0.6844 ± 0.0832 |
   75 | 0.6224 ± 0.0553 | 0.6813 ± 0.0606 | 0.5608 ± 0.0582 | 0.7281 ± 0.0713 |
   76 | 0.5774 ± 0.0477 | 0.7438 ± 0.0500 | 0.5948 ± 0.1184 | 0.7063 ± 0.0990 |
   77 | 0.5857 ± 0.0711 | 0.6750 ± 0.0852 | 0.6168 ± 0.0460 | 0.6656 ± 0.0713 |
   78 | 0.6465 ± 0.0391 | 0.6438 ± 0.0580 | 0.5970 ± 0.0641 | 0.6875 ± 0.0753 |
   79 | 0.5773 ± 0.0502 | 0.6500 ± 0.0306 | 0.5973 ± 0.0488 | 0.6719 ± 0.0756 |
   80 | 0.5931 ± 0.0507 | 0.6625 ± 0.0776 | 0.6094 ± 0.0580 | 0.6687 ± 0.0875 |
   81 | 0.6013 ± 0.0572 | 0.6687 ± 0.0580 | 0.5999 ± 0.0574 | 0.6625 ± 0.0750 |
   82 | 0.5792 ± 0.0224 | 0.6500 ± 0.0415 | 0.6064 ± 0.0453 | 0.5906 ± 0.0719 |
   83 | 0.5950 ± 0.0642 | 0.6625 ± 0.0538 | 0.6064 ± 0.0605 | 0.6438 ± 0.0886 |
   84 | 0.5684 ± 0.0663 | 0.7250 ± 0.0696 | 0.6018 ± 0.0849 | 0.7000 ± 0.0755 |
   85 | 0.6157 ± 0.0594 | 0.6375 ± 0.0612 | 0.5877 ± 0.0563 | 0.6663 ± 0.0812 |
   86 | 0.5441 ± 0.0509 | 0.7125 ± 0.0500 | 0.5908 ± 0.0637 | 0.7031 ± 0.0489 |
   87 | 0.5383 ± 0.0329 | 0.7312 ± 0.0643 | 0.5768 ± 0.0464 | 0.6844 ± 0.0719 |
   88 | 0.5642 ± 0.0767 | 0.7312 ± 0.0980 | 0.5620 ± 0.0517 | 0.7063 ± 0.0580 |
   89 | 0.5700 ± 0.0356 | 0.7250 ± 0.0606 | 0.6140 ± 0.0442 | 0.6875 ± 0.0640 |
   90 | 0.5471 ± 0.0606 | 0.7562 ± 0.0415 | 0.6246 ± 0.0768 | 0.6594 ± 0.0784 |
   91 | 0.5788 ± 0.0250 | 0.6438 ± 0.0468 | 0.6099 ± 0.0374 | 0.6438 ± 0.0488 |
   92 | 0.5845 ± 0.0384 | 0.7000 ± 0.0545 | 0.5839 ± 0.0904 | 0.6781 ± 0.0884 |
   93 | 0.6021 ± 0.0584 | 0.6937 ± 0.0750 | 0.5800 ± 0.0609 | 0.6969 ± 0.0827 |
   94 | 0.5878 ± 0.0964 | 0.6937 ± 0.1072 | 0.6292 ± 0.0422 | 0.5813 ± 0.0488 |
   95 | 0.6124 ± 0.0488 | 0.6687 ± 0.0508 | 0.6145 ± 0.0552 | 0.6750 ± 0.0400 |
   96 | 0.5713 ± 0.0490 | 0.7188 ± 0.0342 | 0.6622 ± 0.1258 | 0.6188 ± 0.1295 |
   97 | 0.5983 ± 0.0355 | 0.6875 ± 0.0625 | 0.6575 ± 0.0921 | 0.6406 ± 0.0887 |
   98 | 0.6209 ± 0.0411 | 0.6625 ± 0.0364 | 0.5882 ± 0.0653 | 0.6906 ± 0.0705 |
   99 | 0.5780 ± 0.0707 | 0.7562 ± 0.0776 | 0.5702 ± 0.0610 | 0.7250 ± 0.0696 |
  100 | 0.5448 ± 0.0228 | 0.7438 ± 0.0364 | 0.5918 ± 0.0840 | 0.6906 ± 0.0745 |
  101 | 0.5759 ± 0.0542 | 0.6750 ± 0.0755 | 0.6233 ± 0.0602 | 0.6969 ± 0.0465 |
  102 | 0.6034 ± 0.0651 | 0.6813 ± 0.0848 | 0.6023 ± 0.0710 | 0.6656 ± 0.0815 |
  103 | 0.5906 ± 0.0634 | 0.6937 ± 0.0637 | 0.6075 ± 0.0332 | 0.6531 ± 0.0260 |
  104 | 0.5931 ± 0.0636 | 0.6750 ± 0.0673 | 0.5702 ± 0.0438 | 0.7156 ± 0.0691 |
  105 | 0.6382 ± 0.0764 | 0.6687 ± 0.0468 | 0.5677 ± 0.0483 | 0.7281 ± 0.0397 |
  106 | 0.6099 ± 0.0956 | 0.6312 ± 0.0573 | 0.5801 ± 0.0461 | 0.6844 ± 0.0531 |
  107 | 0.5610 ± 0.0615 | 0.7438 ± 0.0306 | 0.6127 ± 0.0468 | 0.6562 ± 0.0559 |
  108 | 0.5816 ± 0.0805 | 0.7125 ± 0.0848 | 0.6168 ± 0.0809 | 0.6750 ± 0.0793 |
  109 | 0.6320 ± 0.0774 | 0.6438 ± 0.0897 | 0.5815 ± 0.0475 | 0.6875 ± 0.0576 |
  110 | 0.5425 ± 0.0754 | 0.7562 ± 0.0871 | 0.6344 ± 0.0657 | 0.6594 ± 0.0493 |
  111 | 0.5910 ± 0.0299 | 0.6438 ± 0.0643 | 0.6061 ± 0.0599 | 0.6844 ± 0.0932 |
  112 | 0.6012 ± 0.0533 | 0.6500 ± 0.0573 | 0.5910 ± 0.1088 | 0.6844 ± 0.0796 |
  113 | 0.5607 ± 0.0396 | 0.7375 ± 0.1163 | 0.5933 ± 0.0314 | 0.6562 ± 0.0640 |
  114 | 0.5862 ± 0.0846 | 0.6687 ± 0.0980 | 0.6008 ± 0.0821 | 0.7031 ± 0.0629 |
  115 | 0.6051 ± 0.0629 | 0.6750 ± 0.0508 | 0.5817 ± 0.0848 | 0.7063 ± 0.0781 |
  116 | 0.6553 ± 0.0475 | 0.6250 ± 0.0862 | 0.6458 ± 0.0542 | 0.5969 ± 0.0771 |
  117 | 0.5912 ± 0.0447 | 0.6625 ± 0.0776 | 0.5809 ± 0.0877 | 0.6969 ± 0.1008 |
  118 | 0.6102 ± 0.0512 | 0.6500 ± 0.0500 | 0.6226 ± 0.0838 | 0.6594 ± 0.0719 |
  119 | 0.5693 ± 0.0702 | 0.7188 ± 0.0988 | 0.6036 ± 0.0583 | 0.6875 ± 0.0640 |
  120 | 0.6059 ± 0.0583 | 0.6750 ± 0.0919 | 0.6103 ± 0.0683 | 0.6875 ± 0.0670 |
  121 | 0.6817 ± 0.0782 | 0.5625 ± 0.0523 | 0.6029 ± 0.0556 | 0.6406 ± 0.0629 |
  122 | 0.6136 ± 0.0753 | 0.6375 ± 0.1179 | 0.6155 ± 0.0326 | 0.6562 ± 0.0342 |
  123 | 0.5939 ± 0.0871 | 0.7063 ± 0.0643 | 0.5624 ± 0.0728 | 0.7250 ± 0.0776 |
  124 | 0.5896 ± 0.0872 | 0.6813 ± 0.0667 | 0.5894 ± 0.0929 | 0.6906 ± 0.0878 |
  125 | 0.6146 ± 0.0411 | 0.6500 ± 0.0459 | 0.6096 ± 0.0572 | 0.6781 ± 0.0862 |
  126 | 0.5816 ± 0.0845 | 0.7188 ± 0.0656 | 0.6278 ± 0.0616 | 0.6469 ± 0.0791 |
  127 | 0.6187 ± 0.0241 | 0.6750 ± 0.0153 | 0.6486 ± 0.0885 | 0.6312 ± 0.1241 |
  128 | 0.5865 ± 0.0447 | 0.6375 ± 0.0508 | 0.6099 ± 0.0621 | 0.6562 ± 0.0803 |
  129 | 0.6052 ± 0.0573 | 0.6875 ± 0.0523 | 0.5711 ± 0.0853 | 0.7188 ± 0.0593 |
  130 | 0.5999 ± 0.0599 | 0.6750 ± 0.0508 | 0.5825 ± 0.0545 | 0.7094 ± 0.0873 |
  131 | 0.5633 ± 0.1228 | 0.7188 ± 0.1202 | 0.6083 ± 0.0910 | 0.6594 ± 0.0867 |
  132 | 0.5626 ± 0.0411 | 0.7063 ± 0.0612 | 0.5331 ± 0.0393 | 0.7469 ± 0.0584 |
  133 | 0.6341 ± 0.0714 | 0.6000 ± 0.0824 | 0.5928 ± 0.0472 | 0.6406 ± 0.0769 |
  134 | 0.5606 ± 0.0736 | 0.7312 ± 0.0829 | 0.6349 ± 0.0646 | 0.6562 ± 0.0726 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6571 ± 0.0172 | 0.6062 ± 0.0580 | 0.6359 ± 0.0322 | 0.6687 ± 0.0702 |
    2 | 0.6234 ± 0.0529 | 0.6438 ± 0.0580 | 0.6247 ± 0.0333 | 0.6594 ± 0.0567 |
    3 | 0.5962 ± 0.0571 | 0.7000 ± 0.0852 | 0.6411 ± 0.0678 | 0.6531 ± 0.0732 |
    4 | 0.6377 ± 0.0300 | 0.6125 ± 0.0612 | 0.6220 ± 0.0422 | 0.6250 ± 0.0576 |
    5 | 0.6110 ± 0.0454 | 0.6875 ± 0.0765 | 0.5752 ± 0.0479 | 0.7281 ± 0.0727 |
    6 | 0.6420 ± 0.0233 | 0.6750 ± 0.0673 | 0.6176 ± 0.0727 | 0.6531 ± 0.1104 |
    7 | 0.6276 ± 0.0309 | 0.5875 ± 0.1176 | 0.6353 ± 0.0528 | 0.6219 ± 0.0513 |
    8 | 0.6428 ± 0.0378 | 0.6687 ± 0.0424 | 0.6201 ± 0.0648 | 0.6375 ± 0.0596 |
    9 | 0.5757 ± 0.0487 | 0.7063 ± 0.0250 | 0.6097 ± 0.0583 | 0.7094 ± 0.0542 |
   10 | 0.5879 ± 0.0340 | 0.7063 ± 0.0580 | 0.6526 ± 0.0238 | 0.6031 ± 0.0713 |
   11 | 0.6100 ± 0.0344 | 0.6000 ± 0.0606 | 0.6025 ± 0.0465 | 0.6500 ± 0.0871 |
   12 | 0.5508 ± 0.0203 | 0.7000 ± 0.0643 | 0.6154 ± 0.0820 | 0.6531 ± 0.0632 |
   13 | 0.5910 ± 0.0796 | 0.7125 ± 0.0956 | 0.5880 ± 0.0944 | 0.7250 ± 0.0836 |
   14 | 0.6269 ± 0.0666 | 0.6687 ± 0.0755 | 0.5932 ± 0.0386 | 0.6937 ± 0.0682 |
   15 | 0.6345 ± 0.0831 | 0.6312 ± 0.0800 | 0.5610 ± 0.0683 | 0.7250 ± 0.1044 |
   16 | 0.6180 ± 0.0614 | 0.6687 ± 0.0729 | 0.5765 ± 0.0511 | 0.7125 ± 0.0667 |
   17 | 0.5735 ± 0.0557 | 0.7188 ± 0.0927 | 0.5990 ± 0.0655 | 0.6844 ± 0.0784 |
   18 | 0.5912 ± 0.0588 | 0.7000 ± 0.0643 | 0.5851 ± 0.0567 | 0.7188 ± 0.0593 |
   19 | 0.6161 ± 0.0621 | 0.7188 ± 0.0559 | 0.6556 ± 0.0571 | 0.6156 ± 0.0577 |
   20 | 0.6540 ± 0.1441 | 0.6625 ± 0.1444 | 0.6079 ± 0.0857 | 0.6906 ± 0.0932 |
   21 | 0.6203 ± 0.0878 | 0.6500 ± 0.0824 | 0.6151 ± 0.0987 | 0.6937 ± 0.0859 |
   22 | 0.5593 ± 0.0395 | 0.7125 ± 0.0667 | 0.6104 ± 0.0443 | 0.6531 ± 0.0705 |
   23 | 0.5914 ± 0.0476 | 0.6750 ± 0.0375 | 0.6150 ± 0.0688 | 0.6062 ± 0.0960 |
   24 | 0.6009 ± 0.0292 | 0.6250 ± 0.0442 | 0.5795 ± 0.0324 | 0.6969 ± 0.0610 |
   25 | 0.5547 ± 0.0624 | 0.6937 ± 0.0234 | 0.5825 ± 0.0764 | 0.7344 ± 0.0613 |
   26 | 0.6092 ± 0.0775 | 0.6562 ± 0.0906 | 0.6063 ± 0.0654 | 0.6625 ± 0.0893 |
   27 | 0.5932 ± 0.0641 | 0.6750 ± 0.0580 | 0.6189 ± 0.0685 | 0.6344 ± 0.0656 |
   28 | 0.6328 ± 0.0518 | 0.5625 ± 0.0791 | 0.6180 ± 0.0650 | 0.5969 ± 0.0878 |
   29 | 0.5704 ± 0.0302 | 0.6937 ± 0.0637 | 0.6099 ± 0.0328 | 0.6656 ± 0.0485 |
   30 | 0.6223 ± 0.0508 | 0.6562 ± 0.0988 | 0.6280 ± 0.0564 | 0.6406 ± 0.0597 |
   31 | 0.6028 ± 0.0319 | 0.6750 ± 0.0643 | 0.5801 ± 0.0518 | 0.6562 ± 0.0803 |
   32 | 0.6025 ± 0.1003 | 0.7000 ± 0.1163 | 0.6286 ± 0.0770 | 0.6750 ± 0.0596 |
   33 | 0.5894 ± 0.0813 | 0.6687 ± 0.0612 | 0.6040 ± 0.0580 | 0.6813 ± 0.0836 |
   34 | 0.6691 ± 0.0342 | 0.6000 ± 0.0500 | 0.6118 ± 0.0691 | 0.6562 ± 0.1017 |
   35 | 0.6049 ± 0.0308 | 0.7250 ± 0.0573 | 0.6217 ± 0.0889 | 0.6656 ± 0.1037 |
   36 | 0.6247 ± 0.0518 | 0.6562 ± 0.0862 | 0.5939 ± 0.0850 | 0.6656 ± 0.0815 |
   37 | 0.5859 ± 0.0286 | 0.6750 ± 0.0702 | 0.6053 ± 0.0800 | 0.6562 ± 0.0803 |
   38 | 0.5875 ± 0.1163 | 0.7063 ± 0.0729 | 0.6020 ± 0.0723 | 0.6844 ± 0.0647 |
   39 | 0.6339 ± 0.0447 | 0.6062 ± 0.0643 | 0.6511 ± 0.0587 | 0.6719 ± 0.0716 |
   40 | 0.6012 ± 0.0205 | 0.6813 ± 0.0415 | 0.5954 ± 0.0455 | 0.6750 ± 0.0424 |
   41 | 0.6317 ± 0.0664 | 0.6438 ± 0.0729 | 0.5923 ± 0.0681 | 0.6844 ± 0.0796 |
   42 | 0.6588 ± 0.0299 | 0.6500 ± 0.0538 | 0.5986 ± 0.0474 | 0.6781 ± 0.0626 |
   43 | 0.5995 ± 0.0544 | 0.6562 ± 0.0791 | 0.5809 ± 0.0728 | 0.7031 ± 0.1103 |
   44 | 0.5891 ± 0.0469 | 0.7000 ± 0.0468 | 0.6100 ± 0.0576 | 0.6781 ± 0.0815 |
   45 | 0.5955 ± 0.0442 | 0.6937 ± 0.0667 | 0.6005 ± 0.0427 | 0.6937 ± 0.0893 |
   46 | 0.5722 ± 0.0364 | 0.7312 ± 0.0319 | 0.5914 ± 0.0909 | 0.6969 ± 0.0979 |
   47 | 0.6091 ± 0.0649 | 0.7188 ± 0.0280 | 0.6233 ± 0.0685 | 0.6875 ± 0.0753 |
   48 | 0.5931 ± 0.0707 | 0.7000 ± 0.0702 | 0.6134 ± 0.0400 | 0.6813 ± 0.0904 |
   49 | 0.6310 ± 0.0713 | 0.6250 ± 0.1266 | 0.5711 ± 0.0505 | 0.7125 ± 0.0813 |
   50 | 0.5555 ± 0.0311 | 0.7562 ± 0.0459 | 0.5860 ± 0.0971 | 0.6844 ± 0.0952 |
   51 | 0.5799 ± 0.0610 | 0.7000 ± 0.0829 | 0.6187 ± 0.0854 | 0.6594 ± 0.0832 |
   52 | 0.5849 ± 0.0369 | 0.7000 ± 0.0468 | 0.6078 ± 0.0453 | 0.6344 ± 0.0397 |
   53 | 0.5825 ± 0.0526 | 0.6750 ± 0.0580 | 0.5371 ± 0.0622 | 0.7562 ± 0.0653 |
   54 | 0.6371 ± 0.0964 | 0.6438 ± 0.0805 | 0.5968 ± 0.0675 | 0.6719 ± 0.0781 |
   55 | 0.5981 ± 0.0950 | 0.6188 ± 0.1209 | 0.6086 ± 0.0578 | 0.6406 ± 0.0876 |
   56 | 0.5890 ± 0.1013 | 0.7000 ± 0.0852 | 0.6162 ± 0.0584 | 0.6875 ± 0.0765 |
   57 | 0.5553 ± 0.0879 | 0.7312 ± 0.0897 | 0.6359 ± 0.0584 | 0.6687 ± 0.0628 |
   58 | 0.6057 ± 0.0537 | 0.6875 ± 0.0656 | 0.6208 ± 0.0411 | 0.6625 ± 0.0824 |
   59 | 0.6142 ± 0.0534 | 0.6937 ± 0.0538 | 0.5908 ± 0.0603 | 0.6750 ± 0.0612 |
   60 | 0.5850 ± 0.0591 | 0.6438 ± 0.0643 | 0.5718 ± 0.0377 | 0.6375 ± 0.0817 |
   61 | 0.6674 ± 0.0777 | 0.5625 ± 0.0559 | 0.5955 ± 0.0675 | 0.6312 ± 0.0622 |
   62 | 0.6522 ± 0.0532 | 0.6250 ± 0.0656 | 0.5372 ± 0.0677 | 0.7469 ± 0.0745 |
   63 | 0.5666 ± 0.0503 | 0.7125 ± 0.0538 | 0.5983 ± 0.0819 | 0.7000 ± 0.1075 |
   64 | 0.6203 ± 0.0715 | 0.6750 ± 0.0643 | 0.5941 ± 0.0472 | 0.7000 ± 0.0793 |
   65 | 0.6203 ± 0.0286 | 0.6438 ± 0.0319 | 0.5981 ± 0.0624 | 0.6844 ± 0.0911 |
   66 | 0.5769 ± 0.0625 | 0.6375 ± 0.0729 | 0.6217 ± 0.0539 | 0.6250 ± 0.0850 |
   67 | 0.5833 ± 0.0751 | 0.7312 ± 0.0940 | 0.5623 ± 0.0579 | 0.7250 ± 0.0480 |
   68 | 0.5665 ± 0.0517 | 0.7312 ± 0.0852 | 0.5815 ± 0.0799 | 0.7188 ± 0.0778 |
   69 | 0.5165 ± 0.0402 | 0.7438 ± 0.0500 | 0.5808 ± 0.0748 | 0.7219 ± 0.0531 |
   70 | 0.6222 ± 0.0570 | 0.6687 ± 0.0508 | 0.5904 ± 0.0542 | 0.7031 ± 0.0756 |
   71 | 0.5795 ± 0.0259 | 0.7312 ± 0.0545 | 0.5825 ± 0.0450 | 0.7188 ± 0.0699 |
   72 | 0.6038 ± 0.0498 | 0.6625 ± 0.0776 | 0.6054 ± 0.0874 | 0.6937 ± 0.0859 |
   73 | 0.5820 ± 0.0504 | 0.6625 ± 0.0415 | 0.6239 ± 0.0731 | 0.6594 ± 0.0889 |
   74 | 0.6152 ± 0.0497 | 0.6562 ± 0.0523 | 0.6294 ± 0.0472 | 0.6438 ± 0.0580 |
   75 | 0.6306 ± 0.0338 | 0.6687 ± 0.0829 | 0.6091 ± 0.0938 | 0.6562 ± 0.1202 |
   76 | 0.5324 ± 0.0497 | 0.7125 ± 0.0824 | 0.5683 ± 0.0461 | 0.7188 ± 0.0395 |
   77 | 0.5625 ± 0.0469 | 0.7063 ± 0.0580 | 0.5983 ± 0.0380 | 0.6687 ± 0.1120 |
   78 | 0.5895 ± 0.0607 | 0.7250 ± 0.0723 | 0.6239 ± 0.0850 | 0.6813 ± 0.0904 |
   79 | 0.5570 ± 0.0703 | 0.7312 ± 0.0805 | 0.6070 ± 0.0836 | 0.6813 ± 0.0859 |
   80 | 0.6065 ± 0.0626 | 0.6375 ± 0.0852 | 0.6107 ± 0.0439 | 0.6125 ± 0.0658 |
   81 | 0.5862 ± 0.0419 | 0.6813 ± 0.0500 | 0.5809 ± 0.0332 | 0.7406 ± 0.0641 |
   82 | 0.5999 ± 0.0327 | 0.6062 ± 0.0755 | 0.6104 ± 0.0586 | 0.5813 ± 0.0829 |
   83 | 0.6471 ± 0.0663 | 0.6312 ± 0.0364 | 0.5717 ± 0.0621 | 0.6844 ± 0.0616 |
   84 | 0.5637 ± 0.0494 | 0.7312 ± 0.0580 | 0.6036 ± 0.0795 | 0.6687 ± 0.1029 |
   85 | 0.5630 ± 0.0404 | 0.6875 ± 0.0593 | 0.5886 ± 0.0771 | 0.7133 ± 0.0631 |
   86 | 0.6037 ± 0.0842 | 0.6937 ± 0.0914 | 0.5777 ± 0.0549 | 0.7188 ± 0.0713 |
   87 | 0.6137 ± 0.0677 | 0.6937 ± 0.0914 | 0.6386 ± 0.0760 | 0.6562 ± 0.0670 |
   88 | 0.5637 ± 0.0792 | 0.6687 ± 0.1075 | 0.6297 ± 0.0800 | 0.6469 ± 0.0727 |
   89 | 0.5798 ± 0.0665 | 0.6937 ± 0.0538 | 0.6195 ± 0.0651 | 0.6531 ± 0.0932 |
   90 | 0.5540 ± 0.0320 | 0.7438 ± 0.0306 | 0.5729 ± 0.0474 | 0.7312 ± 0.0715 |
   91 | 0.5685 ± 0.0536 | 0.7375 ± 0.0702 | 0.5718 ± 0.0883 | 0.7156 ± 0.0820 |
   92 | 0.5447 ± 0.0383 | 0.7000 ± 0.0468 | 0.6367 ± 0.0692 | 0.6031 ± 0.0873 |
   93 | 0.6016 ± 0.0750 | 0.6750 ± 0.0940 | 0.5897 ± 0.0647 | 0.6781 ± 0.0641 |
   94 | 0.5737 ± 0.0840 | 0.7312 ± 0.0755 | 0.6217 ± 0.0553 | 0.6687 ± 0.0829 |
   95 | 0.5429 ± 0.0393 | 0.7625 ± 0.0805 | 0.6386 ± 0.0737 | 0.6500 ± 0.0904 |
   96 | 0.6044 ± 0.0867 | 0.6687 ± 0.0755 | 0.5810 ± 0.0637 | 0.6875 ± 0.0958 |
   97 | 0.6242 ± 0.0553 | 0.6438 ± 0.0508 | 0.5867 ± 0.0790 | 0.6875 ± 0.0778 |
   98 | 0.5709 ± 0.0845 | 0.7000 ± 0.1057 | 0.6039 ± 0.0612 | 0.6813 ± 0.0682 |
   99 | 0.6483 ± 0.0309 | 0.6438 ± 0.0375 | 0.5602 ± 0.0533 | 0.7125 ± 0.0788 |
  100 | 0.5958 ± 0.0618 | 0.6750 ± 0.0805 | 0.5693 ± 0.0440 | 0.7250 ± 0.0776 |
  101 | 0.5791 ± 0.0562 | 0.6937 ± 0.0776 | 0.5856 ± 0.0396 | 0.6906 ± 0.0600 |
  102 | 0.5397 ± 0.0591 | 0.6750 ± 0.0940 | 0.5643 ± 0.0708 | 0.6969 ± 0.0740 |
  103 | 0.5669 ± 0.0565 | 0.7063 ± 0.0805 | 0.5886 ± 0.0469 | 0.7031 ± 0.0489 |
  104 | 0.5582 ± 0.0752 | 0.6375 ± 0.0702 | 0.6384 ± 0.0529 | 0.5906 ± 0.0567 |
  105 | 0.5615 ± 0.0329 | 0.7188 ± 0.0523 | 0.6018 ± 0.0874 | 0.6937 ± 0.0848 |
  106 | 0.6039 ± 0.0924 | 0.7188 ± 0.0862 | 0.6223 ± 0.1100 | 0.6656 ± 0.1074 |
  107 | 0.6019 ± 0.1083 | 0.6875 ± 0.1169 | 0.6155 ± 0.0599 | 0.6594 ± 0.0493 |
  108 | 0.6549 ± 0.0314 | 0.6375 ± 0.0468 | 0.5975 ± 0.0675 | 0.6813 ± 0.0788 |
  109 | 0.5515 ± 0.0709 | 0.7312 ± 0.0919 | 0.5898 ± 0.0481 | 0.6906 ± 0.0600 |
  110 | 0.5298 ± 0.0344 | 0.7688 ± 0.0424 | 0.6113 ± 0.0821 | 0.7031 ± 0.1048 |
  111 | 0.6321 ± 0.1023 | 0.6500 ± 0.1142 | 0.5945 ± 0.0721 | 0.6875 ± 0.0765 |
  112 | 0.6222 ± 0.0938 | 0.6625 ± 0.0996 | 0.5889 ± 0.0842 | 0.6781 ± 0.0873 |
  113 | 0.6040 ± 0.0593 | 0.6937 ± 0.0848 | 0.6214 ± 0.0549 | 0.6531 ± 0.0452 |
  114 | 0.5529 ± 0.0125 | 0.6875 ± 0.0442 | 0.6074 ± 0.0673 | 0.6000 ± 0.1035 |
  115 | 0.5880 ± 0.0350 | 0.7000 ± 0.0612 | 0.6216 ± 0.0560 | 0.6687 ± 0.0715 |
  116 | 0.6251 ± 0.0697 | 0.6750 ± 0.0702 | 0.5685 ± 0.0708 | 0.7281 ± 0.0656 |
  117 | 0.6157 ± 0.0749 | 0.5813 ± 0.0580 | 0.6054 ± 0.0686 | 0.6219 ± 0.0952 |
  118 | 0.5439 ± 0.0369 | 0.7562 ± 0.0234 | 0.6141 ± 0.0692 | 0.6844 ± 0.0745 |
  119 | 0.5768 ± 0.0552 | 0.7125 ± 0.0800 | 0.6102 ± 0.0781 | 0.6687 ± 0.0755 |
  120 | 0.5757 ± 0.0372 | 0.6687 ± 0.0643 | 0.5760 ± 0.0474 | 0.6906 ± 0.0844 |
  121 | 0.5481 ± 0.0412 | 0.7438 ± 0.0573 | 0.6026 ± 0.1021 | 0.6656 ± 0.1046 |
  122 | 0.6116 ± 0.0326 | 0.6937 ± 0.0459 | 0.6322 ± 0.0925 | 0.6469 ± 0.0815 |
  123 | 0.6348 ± 0.0836 | 0.6312 ± 0.0776 | 0.6090 ± 0.0745 | 0.6750 ± 0.0715 |
  124 | 0.6129 ± 0.0401 | 0.6438 ± 0.0755 | 0.6032 ± 0.0554 | 0.7063 ± 0.0908 |
  125 | 0.5848 ± 0.0454 | 0.7125 ± 0.0500 | 0.6331 ± 0.0597 | 0.6531 ± 0.0732 |
  126 | 0.6040 ± 0.0502 | 0.6500 ± 0.0871 | 0.5863 ± 0.0624 | 0.6969 ± 0.0791 |
  127 | 0.6156 ± 0.0788 | 0.6937 ± 0.0776 | 0.6234 ± 0.0748 | 0.6813 ± 0.0737 |
  128 | 0.5842 ± 0.0580 | 0.6875 ± 0.0198 | 0.5628 ± 0.0541 | 0.7344 ± 0.0794 |
  129 | 0.5086 ± 0.0371 | 0.7937 ± 0.0702 | 0.6198 ± 0.0720 | 0.6562 ± 0.0873 |
  130 | 0.6257 ± 0.0545 | 0.6687 ± 0.0729 | 0.5934 ± 0.0640 | 0.6937 ± 0.0882 |
  131 | 0.5613 ± 0.0454 | 0.7438 ± 0.0723 | 0.6013 ± 0.0938 | 0.7125 ± 0.0914 |
  132 | 0.5738 ± 0.0791 | 0.6375 ± 0.0940 | 0.6357 ± 0.0508 | 0.6438 ± 0.0970 |
  133 | 0.6384 ± 0.0470 | 0.6625 ± 0.0459 | 0.6026 ± 0.0475 | 0.6813 ± 0.0622 |
  134 | 0.6374 ± 0.0490 | 0.6687 ± 0.0897 | 0.5577 ± 0.0641 | 0.7281 ± 0.0928 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.7017 ± 0.0274 | 0.5437 ± 0.0702 | 0.6957 ± 0.0058 | 0.4938 ± 0.0710 |
    2 | 0.6354 ± 0.0512 | 0.6250 ± 0.1169 | 0.6552 ± 0.0641 | 0.6781 ± 0.0594 |
    3 | 0.6443 ± 0.0407 | 0.6687 ± 0.0424 | 0.6519 ± 0.0609 | 0.5406 ± 0.1074 |
    4 | 0.6076 ± 0.0961 | 0.6875 ± 0.1581 | 0.7332 ± 0.0761 | 0.6531 ± 0.0567 |
    5 | 0.6013 ± 0.0314 | 0.6750 ± 0.0250 | 0.6023 ± 0.0546 | 0.6906 ± 0.0745 |
    6 | 0.5995 ± 0.0853 | 0.6375 ± 0.1179 | 0.6044 ± 0.0539 | 0.6875 ± 0.0464 |
    7 | 0.6300 ± 0.0480 | 0.6375 ± 0.0580 | 0.6254 ± 0.0557 | 0.6438 ± 0.0742 |
    8 | 0.6598 ± 0.0287 | 0.6188 ± 0.0667 | 0.6383 ± 0.0270 | 0.6687 ± 0.0673 |
    9 | 0.6141 ± 0.0608 | 0.6438 ± 0.0829 | 0.6112 ± 0.0608 | 0.6594 ± 0.0942 |
   10 | 0.5859 ± 0.0373 | 0.7500 ± 0.0523 | 0.6347 ± 0.0535 | 0.6438 ± 0.0829 |
   11 | 0.6334 ± 0.0267 | 0.6500 ± 0.0306 | 0.6204 ± 0.0321 | 0.6813 ± 0.0667 |
   12 | 0.6110 ± 0.0505 | 0.7063 ± 0.0375 | 0.6069 ± 0.0510 | 0.7094 ± 0.0779 |
   13 | 0.6242 ± 0.0530 | 0.6438 ± 0.0580 | 0.6363 ± 0.0810 | 0.6750 ± 0.0742 |
   14 | 0.6446 ± 0.0480 | 0.6500 ± 0.0459 | 0.6311 ± 0.0777 | 0.6531 ± 0.0844 |
   15 | 0.5538 ± 0.0843 | 0.7375 ± 0.0805 | 0.5906 ± 0.0529 | 0.7063 ± 0.0580 |
   16 | 0.6226 ± 0.0776 | 0.6813 ± 0.0956 | 0.6437 ± 0.0531 | 0.6250 ± 0.0685 |
   17 | 0.6397 ± 0.0434 | 0.6500 ± 0.0606 | 0.6045 ± 0.0643 | 0.7031 ± 0.1129 |
   18 | 0.5737 ± 0.0543 | 0.7562 ± 0.0776 | 0.6120 ± 0.0722 | 0.6844 ± 0.0771 |
   19 | 0.5969 ± 0.0688 | 0.7500 ± 0.0656 | 0.6229 ± 0.1109 | 0.6781 ± 0.1092 |
   20 | 0.6483 ± 0.0479 | 0.6188 ± 0.0848 | 0.6170 ± 0.0332 | 0.6844 ± 0.0647 |
   21 | 0.5637 ± 0.0516 | 0.7438 ± 0.0606 | 0.6259 ± 0.0673 | 0.6656 ± 0.0862 |
   22 | 0.6914 ± 0.0462 | 0.6125 ± 0.0424 | 0.6368 ± 0.0754 | 0.6406 ± 0.0794 |
   23 | 0.6106 ± 0.0785 | 0.6750 ± 0.0805 | 0.6351 ± 0.0473 | 0.6625 ± 0.0519 |
   24 | 0.6011 ± 0.0583 | 0.6813 ± 0.1090 | 0.6059 ± 0.0416 | 0.6844 ± 0.0616 |
   25 | 0.5774 ± 0.0257 | 0.7063 ± 0.0612 | 0.5637 ± 0.1068 | 0.7344 ± 0.1048 |
   26 | 0.6432 ± 0.0574 | 0.6250 ± 0.0839 | 0.6008 ± 0.0624 | 0.7000 ± 0.0768 |
   27 | 0.5720 ± 0.0863 | 0.7250 ± 0.0538 | 0.6305 ± 0.0808 | 0.6687 ± 0.0628 |
   28 | 0.7140 ± 0.1153 | 0.6000 ± 0.0956 | 0.6198 ± 0.0321 | 0.5938 ± 0.0988 |
   29 | 1.0099 ± 0.1393 | 0.4437 ± 0.1209 | 0.7328 ± 0.0581 | 0.5219 ± 0.0873 |
   30 | 0.6595 ± 0.0655 | 0.6500 ± 0.1072 | 0.7180 ± 0.0728 | 0.5844 ± 0.0766 |
   31 | 0.7693 ± 0.0880 | 0.5312 ± 0.0442 | 0.7056 ± 0.0170 | 0.4437 ± 0.1035 |
   32 | 0.7596 ± 0.0551 | 0.4437 ± 0.0667 | 0.6645 ± 0.0146 | 0.6562 ± 0.1411 |
   33 | 0.6973 ± 0.0556 | 0.5938 ± 0.0656 | 0.7046 ± 0.0245 | 0.4656 ± 0.1254 |
   34 | 0.6950 ± 0.0107 | 0.5250 ± 0.0667 | 0.7216 ± 0.0410 | 0.5031 ± 0.0832 |
   35 | 0.7652 ± 0.1282 | 0.5125 ± 0.1163 | 0.9240 ± 0.0989 | 0.4656 ± 0.0784 |
   36 | 0.6628 ± 0.0472 | 0.6125 ± 0.1551 | 0.6547 ± 0.0444 | 0.6656 ± 0.0626 |
   37 | 0.6616 ± 0.0665 | 0.6312 ± 0.0667 | 0.6170 ± 0.0656 | 0.6625 ± 0.0776 |
   38 | 0.6589 ± 0.0213 | 0.5563 ± 0.0848 | 0.6398 ± 0.0209 | 0.5625 ± 0.0559 |
   39 | 0.5916 ± 0.0256 | 0.7063 ± 0.0424 | 0.6147 ± 0.0564 | 0.6844 ± 0.0889 |
   40 | 0.6025 ± 0.0411 | 0.6750 ± 0.0673 | 0.5999 ± 0.0658 | 0.6875 ± 0.0827 |
   41 | 0.5709 ± 0.0551 | 0.7312 ± 0.0508 | 0.5997 ± 0.0976 | 0.6781 ± 0.0851 |
   42 | 0.5551 ± 0.0570 | 0.7063 ± 0.0729 | 0.6007 ± 0.0606 | 0.6375 ± 0.0897 |
   43 | 0.5804 ± 0.0276 | 0.7250 ± 0.0415 | 0.6206 ± 0.0732 | 0.6500 ± 0.0966 |
   44 | 0.6167 ± 0.0415 | 0.6500 ± 0.0800 | 0.5697 ± 0.0554 | 0.7281 ± 0.0766 |
   45 | 0.5953 ± 0.0618 | 0.6750 ± 0.0781 | 0.6133 ± 0.0643 | 0.6594 ± 0.0942 |
   46 | 0.6215 ± 0.0466 | 0.6500 ± 0.0538 | 0.5944 ± 0.0550 | 0.6875 ± 0.0884 |
   47 | 0.6459 ± 0.0808 | 0.6375 ± 0.1000 | 0.6010 ± 0.0722 | 0.6844 ± 0.0867 |
   48 | 0.5905 ± 0.0742 | 0.7063 ± 0.0643 | 0.5889 ± 0.0677 | 0.7063 ± 0.0729 |
   49 | 0.5657 ± 0.0926 | 0.7312 ± 0.0875 | 0.6383 ± 0.1158 | 0.6250 ± 0.1161 |
   50 | 0.6939 ± 0.0642 | 0.5875 ± 0.0637 | 0.6373 ± 0.0393 | 0.6250 ± 0.0753 |
   51 | 0.6088 ± 0.0703 | 0.6937 ± 0.0956 | 0.6228 ± 0.0853 | 0.6531 ± 0.1122 |
   52 | 0.6337 ± 0.0860 | 0.6687 ± 0.0829 | 0.5950 ± 0.0790 | 0.6937 ± 0.0800 |
   53 | 0.6149 ± 0.0508 | 0.6687 ± 0.0829 | 0.6334 ± 0.0708 | 0.6344 ± 0.0948 |
   54 | 0.6083 ± 0.0574 | 0.6562 ± 0.0593 | 0.5897 ± 0.0422 | 0.7031 ± 0.0509 |
   55 | 0.6096 ± 0.0631 | 0.7000 ± 0.0545 | 0.6070 ± 0.0528 | 0.6750 ± 0.0545 |
   56 | 0.5565 ± 0.0760 | 0.7438 ± 0.1016 | 0.5935 ± 0.0729 | 0.6750 ± 0.0908 |
   57 | 0.6061 ± 0.0271 | 0.6750 ± 0.0508 | 0.5954 ± 0.0662 | 0.7156 ± 0.0844 |
   58 | 0.5986 ± 0.0146 | 0.6875 ± 0.0342 | 0.6056 ± 0.0804 | 0.6781 ± 0.0740 |
   59 | 0.6137 ± 0.0774 | 0.6687 ± 0.0755 | 0.6368 ± 0.0858 | 0.6469 ± 0.0713 |
   60 | 0.6524 ± 0.0729 | 0.6000 ± 0.0723 | 0.5558 ± 0.0652 | 0.7406 ± 0.0671 |
   61 | 0.5763 ± 0.0641 | 0.7000 ± 0.0980 | 0.5714 ± 0.0632 | 0.6969 ± 0.0884 |
   62 | 0.6155 ± 0.0769 | 0.6625 ± 0.0606 | 0.5861 ± 0.0275 | 0.7125 ± 0.0682 |
   63 | 0.6291 ± 0.0512 | 0.6687 ± 0.0755 | 0.6216 ± 0.0644 | 0.6562 ± 0.0873 |
   64 | 0.6711 ± 0.0187 | 0.6125 ± 0.0424 | 0.6195 ± 0.0774 | 0.6562 ± 0.0938 |
   65 | 0.6057 ± 0.0515 | 0.7063 ± 0.0805 | 0.6339 ± 0.0784 | 0.6625 ± 0.0763 |
   66 | 0.6235 ± 0.0789 | 0.6937 ± 0.1108 | 0.6101 ± 0.0789 | 0.6656 ± 0.0873 |
   67 | 0.5980 ± 0.0670 | 0.6562 ± 0.0815 | 0.6069 ± 0.0513 | 0.6781 ± 0.0779 |
   68 | 0.6032 ± 0.0212 | 0.6625 ± 0.0538 | 0.5762 ± 0.0551 | 0.6937 ± 0.0737 |
   69 | 0.5714 ± 0.0230 | 0.6937 ± 0.0637 | 0.6287 ± 0.0525 | 0.6687 ± 0.0580 |
   70 | 0.5697 ± 0.0913 | 0.7125 ± 0.0824 | 0.5828 ± 0.0800 | 0.7188 ± 0.0699 |
   71 | 0.6288 ± 0.0712 | 0.6625 ± 0.0871 | 0.6301 ± 0.0570 | 0.6562 ± 0.0765 |
   72 | 0.5838 ± 0.0540 | 0.7312 ± 0.0424 | 0.6017 ± 0.0511 | 0.6813 ± 0.0415 |
   73 | 0.5409 ± 0.0519 | 0.7438 ± 0.0500 | 0.6763 ± 0.0937 | 0.6094 ± 0.0743 |
   74 | 0.5934 ± 0.0506 | 0.6750 ± 0.0508 | 0.5994 ± 0.0578 | 0.6625 ± 0.0946 |
   75 | 0.5972 ± 0.0325 | 0.6937 ± 0.0234 | 0.6360 ± 0.1110 | 0.6531 ± 0.1078 |
   76 | 0.5548 ± 0.0787 | 0.7188 ± 0.0791 | 0.5634 ± 0.0644 | 0.7406 ± 0.0815 |
   77 | 0.6290 ± 0.0559 | 0.6125 ± 0.0319 | 0.6140 ± 0.0581 | 0.6687 ± 0.0781 |
   78 | 0.6003 ± 0.0871 | 0.7125 ± 0.0848 | 0.6192 ± 0.0601 | 0.6406 ± 0.0425 |
   79 | 0.6145 ± 0.0878 | 0.6813 ± 0.1016 | 0.6251 ± 0.0426 | 0.6438 ± 0.0781 |
   80 | 0.5903 ± 0.0525 | 0.6937 ± 0.0848 | 0.5643 ± 0.0655 | 0.6875 ± 0.0726 |
   81 | 0.5802 ± 0.0233 | 0.7063 ± 0.0424 | 0.5823 ± 0.0788 | 0.7000 ± 0.0742 |
   82 | 0.6833 ± 0.0083 | 0.5750 ± 0.0319 | 0.7314 ± 0.0413 | 0.4719 ± 0.0911 |
   83 | 0.6497 ± 0.0963 | 0.6125 ± 0.1179 | 0.6285 ± 0.0349 | 0.6750 ± 0.0375 |
   84 | 0.6327 ± 0.0534 | 0.6813 ± 0.0459 | 0.6795 ± 0.0744 | 0.5750 ± 0.0424 |
   85 | 0.6162 ± 0.0662 | 0.6687 ± 0.0940 | 0.6102 ± 0.0419 | 0.6632 ± 0.0533 |
   86 | 0.6215 ± 0.0699 | 0.6312 ± 0.0824 | 0.6259 ± 0.0835 | 0.6875 ± 0.0699 |
   87 | 0.6176 ± 0.0515 | 0.6625 ± 0.0606 | 0.5754 ± 0.0303 | 0.7031 ± 0.0469 |
   88 | 0.5784 ± 0.0630 | 0.7125 ± 0.0606 | 0.6115 ± 0.0429 | 0.6375 ± 0.0612 |
   89 | 0.5696 ± 0.0558 | 0.7188 ± 0.0523 | 0.5965 ± 0.0518 | 0.6906 ± 0.0705 |
   90 | 0.5470 ± 0.0151 | 0.7312 ± 0.0250 | 0.5972 ± 0.0430 | 0.6844 ± 0.0567 |
   91 | 0.6212 ± 0.0802 | 0.7063 ± 0.1057 | 0.5806 ± 0.0607 | 0.6875 ± 0.0670 |
   92 | 0.6090 ± 0.0802 | 0.6500 ± 0.1209 | 0.5964 ± 0.0431 | 0.6875 ± 0.0523 |
   93 | 0.5860 ± 0.0413 | 0.7063 ± 0.0702 | 0.5798 ± 0.0809 | 0.7250 ± 0.0935 |
   94 | 0.5689 ± 0.0266 | 0.7063 ± 0.0153 | 0.6147 ± 0.0893 | 0.6594 ± 0.0832 |
   95 | 0.5675 ± 0.0218 | 0.7125 ± 0.0606 | 0.6177 ± 0.0480 | 0.6281 ± 0.0758 |
   96 | 0.5983 ± 0.0235 | 0.6937 ± 0.0667 | 0.6114 ± 0.0352 | 0.6813 ± 0.0500 |
   97 | 0.5525 ± 0.0859 | 0.7312 ± 0.1000 | 0.5992 ± 0.0740 | 0.6937 ± 0.0776 |
   98 | 0.6275 ± 0.0443 | 0.6562 ± 0.0342 | 0.5864 ± 0.0552 | 0.6906 ± 0.0691 |
   99 | 0.5673 ± 0.0703 | 0.6875 ± 0.0685 | 0.5881 ± 0.0869 | 0.6906 ± 0.1041 |
  100 | 0.5173 ± 0.0466 | 0.7250 ± 0.1108 | 0.6240 ± 0.0579 | 0.6875 ± 0.0640 |
  101 | 0.5955 ± 0.0585 | 0.6687 ± 0.0805 | 0.6035 ± 0.0764 | 0.6719 ± 0.0794 |
  102 | 0.6576 ± 0.1224 | 0.6188 ± 0.1272 | 0.5765 ± 0.0638 | 0.7250 ± 0.0848 |
  103 | 0.5895 ± 0.0622 | 0.6500 ± 0.0538 | 0.6164 ± 0.1016 | 0.6687 ± 0.0596 |
  104 | 0.6418 ± 0.0417 | 0.6375 ± 0.0612 | 0.6567 ± 0.0810 | 0.6125 ± 0.1102 |
  105 | 0.6018 ± 0.0511 | 0.6813 ± 0.0824 | 0.5873 ± 0.0623 | 0.6875 ± 0.0685 |
  106 | 0.6200 ± 0.0781 | 0.6125 ± 0.0919 | 0.5888 ± 0.0457 | 0.6937 ± 0.0573 |
  107 | 0.6259 ± 0.0576 | 0.6750 ± 0.0319 | 0.6225 ± 0.0592 | 0.6500 ± 0.0925 |
  108 | 0.5930 ± 0.0437 | 0.7063 ± 0.0729 | 0.6450 ± 0.0411 | 0.6469 ± 0.0542 |
  109 | 0.6414 ± 0.0467 | 0.6562 ± 0.0559 | 0.5889 ± 0.0577 | 0.6969 ± 0.0948 |
  110 | 0.6532 ± 0.0355 | 0.5938 ± 0.0442 | 0.6358 ± 0.0411 | 0.6344 ± 0.0839 |
  111 | 0.5796 ± 0.0378 | 0.6875 ± 0.0395 | 0.6057 ± 0.1010 | 0.6844 ± 0.1022 |
  112 | 0.5953 ± 0.0281 | 0.6750 ± 0.0250 | 0.5645 ± 0.0587 | 0.7188 ± 0.0884 |
  113 | 0.5975 ± 0.0373 | 0.6875 ± 0.0625 | 0.6231 ± 0.0616 | 0.6375 ± 0.0742 |
  114 | 0.6193 ± 0.0679 | 0.6750 ± 0.0643 | 0.5616 ± 0.1071 | 0.7250 ± 0.0904 |
  115 | 0.5788 ± 0.0586 | 0.6937 ± 0.0871 | 0.5780 ± 0.0615 | 0.7125 ± 0.0622 |
  116 | 0.6098 ± 0.0832 | 0.6937 ± 0.0459 | 0.6038 ± 0.0802 | 0.6844 ± 0.1012 |
  117 | 0.5856 ± 0.0610 | 0.6687 ± 0.0755 | 0.6144 ± 0.1052 | 0.7219 ± 0.1068 |
  118 | 0.5648 ± 0.0334 | 0.7375 ± 0.0508 | 0.6063 ± 0.0739 | 0.6562 ± 0.1008 |
  119 | 0.6237 ± 0.0851 | 0.6500 ± 0.0956 | 0.5950 ± 0.0539 | 0.7031 ± 0.0853 |
  120 | 0.5609 ± 0.0543 | 0.7000 ± 0.0468 | 0.6142 ± 0.1027 | 0.6781 ± 0.0928 |
  121 | 0.6464 ± 0.0303 | 0.6250 ± 0.0484 | 0.5880 ± 0.0691 | 0.6875 ± 0.0765 |
  122 | 0.6546 ± 0.1074 | 0.5875 ± 0.1176 | 0.6307 ± 0.0567 | 0.6469 ± 0.0740 |
  123 | 0.9168 ± 0.1816 | 0.4938 ± 0.0723 | 0.9048 ± 0.0765 | 0.5062 ± 0.0556 |
  124 | 0.7495 ± 0.0428 | 0.4500 ± 0.0673 | 0.6991 ± 0.0075 | 0.4625 ± 0.0653 |
  125 | 0.7648 ± 0.0770 | 0.5250 ± 0.1035 | 1.0928 ± 0.2124 | 0.4969 ± 0.1122 |
  126 | 0.7693 ± 0.0896 | 0.5188 ± 0.1019 | 0.7984 ± 0.0693 | 0.4719 ± 0.0844 |
  127 | 0.7219 ± 0.0484 | 0.5437 ± 0.0545 | 0.7061 ± 0.0309 | 0.4969 ± 0.0993 |
  128 | 0.7111 ± 0.0186 | 0.4688 ± 0.0280 | 0.6949 ± 0.0071 | 0.4688 ± 0.0778 |
  129 | 0.6905 ± 0.0331 | 0.5687 ± 0.0606 | 0.6992 ± 0.0124 | 0.4875 ± 0.0596 |
  130 | 0.6891 ± 0.0080 | 0.4938 ± 0.0976 | 0.7734 ± 0.0574 | 0.4469 ± 0.0917 |
  131 | 0.6922 ± 0.0210 | 0.5125 ± 0.0545 | 0.6581 ± 0.0234 | 0.6781 ± 0.0895 |
  132 | 0.6814 ± 0.0284 | 0.5750 ± 0.0829 | 0.6896 ± 0.0318 | 0.5125 ± 0.0687 |
  133 | 0.6908 ± 0.0647 | 0.5375 ± 0.1256 | 0.6900 ± 0.0205 | 0.4813 ± 0.0755 |
  134 | 0.6620 ± 0.0993 | 0.6312 ± 0.0696 | 0.6230 ± 0.1111 | 0.6438 ± 0.1137 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6072 ± 0.0873 | 0.6813 ± 0.1225 | 0.6730 ± 0.0723 | 0.6000 ± 0.0737 |
    2 | 0.6407 ± 0.0795 | 0.6500 ± 0.0459 | 0.6319 ± 0.0438 | 0.6312 ± 0.0637 |
    3 | 0.6208 ± 0.0705 | 0.7000 ± 0.0897 | 0.6366 ± 0.0564 | 0.6438 ± 0.0841 |
    4 | 0.6254 ± 0.0143 | 0.6750 ± 0.0250 | 0.6175 ± 0.0645 | 0.6781 ± 0.0577 |
    5 | 0.6904 ± 0.0435 | 0.5813 ± 0.0643 | 0.6916 ± 0.0156 | 0.5125 ± 0.1038 |
    6 | 0.6443 ± 0.0568 | 0.6438 ± 0.0643 | 0.6474 ± 0.0234 | 0.6188 ± 0.0637 |
    7 | 0.6826 ± 0.0426 | 0.6125 ± 0.0375 | 0.6535 ± 0.0269 | 0.5875 ± 0.0682 |
    8 | 0.5978 ± 0.0290 | 0.6875 ± 0.0395 | 0.6091 ± 0.0760 | 0.6687 ± 0.0897 |
    9 | 0.6610 ± 0.1594 | 0.6625 ± 0.1225 | 0.5767 ± 0.0512 | 0.7094 ± 0.0815 |
   10 | 0.6213 ± 0.0408 | 0.6875 ± 0.0559 | 0.6100 ± 0.0880 | 0.6813 ± 0.0750 |
   11 | 0.6228 ± 0.0645 | 0.6813 ± 0.0364 | 0.6115 ± 0.0435 | 0.6687 ± 0.0702 |
   12 | 0.6440 ± 0.0534 | 0.6687 ± 0.0319 | 0.6742 ± 0.0457 | 0.6156 ± 0.0610 |
   13 | 0.6319 ± 0.0313 | 0.6687 ± 0.0508 | 0.5835 ± 0.0951 | 0.7000 ± 0.1102 |
   14 | 0.6138 ± 0.0467 | 0.6750 ± 0.0805 | 0.6357 ± 0.0447 | 0.6625 ± 0.0500 |
   15 | 0.5727 ± 0.0718 | 0.7000 ± 0.0919 | 0.6344 ± 0.0740 | 0.6687 ± 0.0612 |
   16 | 0.5253 ± 0.0733 | 0.7562 ± 0.0848 | 0.6092 ± 0.0405 | 0.6719 ± 0.0644 |
   17 | 0.6256 ± 0.0604 | 0.6375 ± 0.0612 | 0.5859 ± 0.0530 | 0.6875 ± 0.0958 |
   18 | 0.5878 ± 0.0755 | 0.7000 ± 0.0897 | 0.6388 ± 0.0757 | 0.6531 ± 0.0549 |
   19 | 0.5470 ± 0.0546 | 0.7688 ± 0.0643 | 0.6371 ± 0.0963 | 0.6562 ± 0.0927 |
   20 | 0.6186 ± 0.0350 | 0.6375 ± 0.0897 | 0.6043 ± 0.0578 | 0.6813 ± 0.0637 |
   21 | 0.6303 ± 0.0567 | 0.6375 ± 0.0319 | 0.7699 ± 0.0615 | 0.4688 ± 0.0541 |
   22 | 0.7621 ± 0.1081 | 0.4688 ± 0.1266 | 0.7260 ± 0.0482 | 0.5531 ± 0.0505 |
   23 | 0.7211 ± 0.1530 | 0.5938 ± 0.1027 | 0.9573 ± 0.1319 | 0.5219 ± 0.0803 |
   24 | 0.7371 ± 0.0291 | 0.4500 ± 0.0508 | 0.6959 ± 0.0050 | 0.4813 ± 0.0545 |
   25 | 0.7022 ± 0.0087 | 0.4875 ± 0.0508 | 0.6905 ± 0.0214 | 0.5406 ± 0.0851 |
   26 | 0.7061 ± 0.0430 | 0.5500 ± 0.0580 | 0.7539 ± 0.0533 | 0.5250 ± 0.0653 |
   27 | 0.7291 ± 0.0420 | 0.4500 ± 0.0781 | 0.6976 ± 0.0103 | 0.4906 ± 0.0656 |
   28 | 0.7204 ± 0.0547 | 0.5062 ± 0.1016 | 0.7694 ± 0.0617 | 0.4906 ± 0.0815 |
   29 | 0.7169 ± 0.0294 | 0.4875 ± 0.1111 | 0.6808 ± 0.0044 | 0.5781 ± 0.0876 |
   30 | 0.7240 ± 0.0208 | 0.4562 ± 0.0643 | 0.6928 ± 0.0057 | 0.5094 ± 0.0839 |
   31 | 0.6586 ± 0.0148 | 0.6062 ± 0.0424 | 0.6480 ± 0.0550 | 0.6219 ± 0.0796 |
   32 | 0.6533 ± 0.0246 | 0.6312 ± 0.0538 | 0.6450 ± 0.0752 | 0.6375 ± 0.0805 |
   33 | 0.7053 ± 0.0783 | 0.5938 ± 0.0927 | 0.7000 ± 0.0305 | 0.4938 ± 0.0893 |
   34 | 0.6955 ± 0.0236 | 0.5625 ± 0.0280 | 0.7426 ± 0.0210 | 0.4469 ± 0.0862 |
   35 | 0.8343 ± 0.1064 | 0.4188 ± 0.0375 | 0.7538 ± 0.0445 | 0.4188 ± 0.0817 |
   36 | 0.6390 ± 0.0262 | 0.6750 ± 0.0319 | 0.6730 ± 0.0634 | 0.6188 ± 0.1035 |
   37 | 0.7493 ± 0.0768 | 0.5437 ± 0.0673 | 0.7115 ± 0.0084 | 0.4281 ± 0.0699 |
   38 | 0.6973 ± 0.0138 | 0.5625 ± 0.0523 | 0.6325 ± 0.0611 | 0.6813 ± 0.0946 |
   39 | 0.7479 ± 0.1282 | 0.5500 ± 0.1259 | 0.6811 ± 0.0110 | 0.5094 ± 0.0465 |
   40 | 0.7005 ± 0.0860 | 0.6125 ± 0.1228 | 0.6600 ± 0.0365 | 0.6469 ± 0.0577 |
   41 | 0.7084 ± 0.0900 | 0.6375 ± 0.0612 | 0.6947 ± 0.0857 | 0.5875 ± 0.1134 |
   42 | 0.6691 ± 0.0912 | 0.6500 ± 0.0776 | 0.6602 ± 0.0370 | 0.6344 ± 0.0851 |
   43 | 0.7139 ± 0.0177 | 0.5125 ± 0.0643 | 0.7485 ± 0.0352 | 0.3875 ± 0.0702 |
   44 | 0.7763 ± 0.0956 | 0.5188 ± 0.1320 | 0.7622 ± 0.0258 | 0.4125 ± 0.0519 |
   45 | 0.7033 ± 0.0063 | 0.4313 ± 0.0459 | 0.6782 ± 0.0098 | 0.6188 ± 0.0653 |
   46 | 0.6881 ± 0.0222 | 0.5813 ± 0.0468 | 0.6844 ± 0.0216 | 0.5687 ± 0.0976 |
   47 | 0.6321 ± 0.0402 | 0.6875 ± 0.0656 | 0.6748 ± 0.0738 | 0.6219 ± 0.0889 |
   48 | 0.6869 ± 0.0186 | 0.5062 ± 0.0871 | 0.7135 ± 0.0831 | 0.5687 ± 0.1006 |
   49 | 0.6887 ± 0.0455 | 0.6125 ± 0.1128 | 0.7159 ± 0.0565 | 0.5406 ± 0.0948 |
   50 | 0.7284 ± 0.0258 | 0.4813 ± 0.0580 | 0.7490 ± 0.0424 | 0.4719 ± 0.0745 |
   51 | 0.7142 ± 0.0399 | 0.5125 ± 0.0919 | 0.7212 ± 0.0822 | 0.5469 ± 0.1163 |
   52 | 0.6899 ± 0.0099 | 0.5938 ± 0.0442 | 0.7304 ± 0.0788 | 0.5219 ± 0.1219 |
   53 | 0.7178 ± 0.0102 | 0.5125 ± 0.0424 | 0.6966 ± 0.0265 | 0.5188 ± 0.1029 |
   54 | 0.7347 ± 0.0453 | 0.5250 ± 0.0459 | 0.8023 ± 0.0332 | 0.4656 ± 0.0406 |
   55 | 0.6969 ± 0.0089 | 0.5062 ± 0.0500 | 0.6968 ± 0.0095 | 0.5094 ± 0.0443 |
   56 | 0.7267 ± 0.0422 | 0.4500 ± 0.0755 | 0.7267 ± 0.0371 | 0.5125 ± 0.0643 |
   57 | 0.7220 ± 0.0266 | 0.5188 ± 0.0580 | 0.6932 ± 0.0078 | 0.5094 ± 0.0969 |
   58 | 0.7280 ± 0.0525 | 0.4938 ± 0.0914 | 0.7284 ± 0.1056 | 0.5375 ± 0.1484 |
   59 | 0.6999 ± 0.0169 | 0.4813 ± 0.0781 | 0.7292 ± 0.0324 | 0.4844 ± 0.0674 |
   60 | 0.7020 ± 0.0156 | 0.5000 ± 0.0625 | 0.6946 ± 0.0312 | 0.5281 ± 0.1131 |
   61 | 0.7205 ± 0.0256 | 0.5000 ± 0.0625 | 0.6970 ± 0.0135 | 0.5125 ± 0.0580 |
   62 | 0.7439 ± 0.1331 | 0.5312 ± 0.1296 | 0.6923 ± 0.0017 | 0.5281 ± 0.0493 |
   63 | 0.7693 ± 0.0488 | 0.4750 ± 0.0573 | 0.7061 ± 0.0276 | 0.4781 ± 0.1118 |
   64 | 0.6999 ± 0.0359 | 0.4437 ± 0.1497 | 0.7818 ± 0.0726 | 0.5250 ± 0.0750 |
   65 | 0.8099 ± 0.1631 | 0.5188 ± 0.1290 | 0.7972 ± 0.0737 | 0.4844 ± 0.0853 |
   66 | 0.7733 ± 0.0590 | 0.5188 ± 0.0580 | 0.6976 ± 0.0046 | 0.4719 ± 0.0430 |
   67 | 0.6919 ± 0.0125 | 0.5437 ± 0.1075 | 0.7605 ± 0.0556 | 0.4938 ± 0.0776 |
   68 | 0.7040 ± 0.0157 | 0.5062 ± 0.0364 | 0.7043 ± 0.0171 | 0.5031 ± 0.0549 |
   69 | 0.7659 ± 0.0828 | 0.4125 ± 0.0871 | 0.6908 ± 0.0098 | 0.5375 ± 0.1108 |
   70 | 0.7018 ± 0.0146 | 0.4813 ± 0.0580 | 0.7002 ± 0.0175 | 0.4719 ± 0.1165 |
   71 | 0.7249 ± 0.0622 | 0.5062 ± 0.0914 | 0.6989 ± 0.0313 | 0.5344 ± 0.0796 |
   72 | 0.7463 ± 0.0369 | 0.5125 ± 0.0250 | 0.6931 ± 0.0072 | 0.5156 ± 0.0580 |
   73 | 0.7294 ± 0.0318 | 0.4750 ± 0.0606 | 0.6922 ± 0.0197 | 0.5312 ± 0.0916 |
   74 | 0.6980 ± 0.0167 | 0.5437 ± 0.0424 | 0.7786 ± 0.0545 | 0.4844 ± 0.0702 |
   75 | 0.7072 ± 0.0430 | 0.5062 ± 0.1090 | 0.7062 ± 0.0237 | 0.4781 ± 0.0959 |
   76 | 0.8325 ± 0.0914 | 0.5000 ± 0.0656 | 0.9564 ± 0.1138 | 0.4969 ± 0.0758 |
   77 | 0.7053 ± 0.0321 | 0.5437 ± 0.0702 | 0.6918 ± 0.0051 | 0.5281 ± 0.0758 |
   78 | 0.6985 ± 0.0255 | 0.5312 ± 0.0625 | 0.7184 ± 0.0302 | 0.4750 ± 0.0836 |
   79 | 0.7253 ± 0.0329 | 0.4625 ± 0.0500 | 0.7014 ± 0.0167 | 0.5031 ± 0.0616 |
   80 | 0.7075 ± 0.0282 | 0.4813 ± 0.0805 | 0.6932 ± 0.0004 | 0.4969 ± 0.0844 |
   81 | 0.6926 ± 0.0049 | 0.5125 ± 0.0545 | 0.7018 ± 0.0082 | 0.4875 ± 0.0375 |
   82 | 0.6946 ± 0.0062 | 0.4938 ± 0.0723 | 0.6898 ± 0.0107 | 0.5406 ± 0.0641 |
   83 | 0.7924 ± 0.0821 | 0.4688 ± 0.1202 | 0.7239 ± 0.0507 | 0.5219 ± 0.0851 |
   84 | 0.6938 ± 0.0054 | 0.5062 ± 0.0723 | 0.7054 ± 0.0159 | 0.4906 ± 0.0560 |
   85 | 0.7070 ± 0.0264 | 0.4938 ± 0.0776 | 0.7236 ± 0.0188 | 0.4599 ± 0.0525 |
   86 | 0.7732 ± 0.0861 | 0.4500 ± 0.0960 | 0.7670 ± 0.0604 | 0.5312 ± 0.0656 |
   87 | 0.7689 ± 0.0677 | 0.4938 ± 0.0848 | 0.8342 ± 0.0501 | 0.5094 ± 0.0443 |
   88 | 0.7692 ± 0.0554 | 0.4875 ± 0.0643 | 0.7362 ± 0.0572 | 0.5062 ± 0.0925 |
   89 | 0.6921 ± 0.0313 | 0.5625 ± 0.0685 | 0.6950 ± 0.0030 | 0.4656 ± 0.0584 |
   90 | 0.6906 ± 0.0251 | 0.5437 ± 0.0729 | 0.7195 ± 0.0409 | 0.5094 ± 0.0815 |
   91 | 0.6918 ± 0.0018 | 0.5563 ± 0.0538 | 0.6923 ± 0.0144 | 0.5250 ± 0.0800 |
   92 | 0.6929 ± 0.0063 | 0.5375 ± 0.0364 | 0.6967 ± 0.0222 | 0.5250 ± 0.0750 |
   93 | 0.7173 ± 0.0154 | 0.4500 ± 0.0612 | 0.6934 ± 0.0006 | 0.4906 ± 0.0485 |
   94 | 0.7442 ± 0.0387 | 0.4938 ± 0.0500 | 0.7060 ± 0.0461 | 0.5281 ± 0.1012 |
   95 | 0.7157 ± 0.0505 | 0.5437 ± 0.0643 | 0.7285 ± 0.0423 | 0.5031 ± 0.0771 |
   96 | 0.7858 ± 0.1114 | 0.5000 ± 0.0948 | 0.9488 ± 0.1445 | 0.4719 ± 0.1059 |
   97 | 0.7590 ± 0.0601 | 0.4750 ± 0.0637 | 0.7150 ± 0.0233 | 0.4844 ± 0.0644 |
   98 | 0.7338 ± 0.0292 | 0.4938 ± 0.0667 | 0.7034 ± 0.0189 | 0.4969 ± 0.0691 |
   99 | 0.7352 ± 0.0564 | 0.4500 ± 0.0829 | 0.6949 ± 0.0116 | 0.5375 ± 0.0337 |
  100 | 0.7249 ± 0.0389 | 0.4437 ± 0.1225 | 0.7502 ± 0.0670 | 0.5031 ± 0.0963 |
  101 | 0.7057 ± 0.0228 | 0.5188 ± 0.0508 | 0.6969 ± 0.0236 | 0.5188 ± 0.0897 |
  102 | 0.7146 ± 0.0550 | 0.5250 ± 0.0935 | 0.8085 ± 0.0732 | 0.4906 ± 0.0779 |
  103 | 0.7289 ± 0.0625 | 0.4938 ± 0.0848 | 0.6952 ± 0.0082 | 0.4813 ± 0.1128 |
  104 | 0.7487 ± 0.0810 | 0.5250 ± 0.0848 | 0.7207 ± 0.0540 | 0.4938 ± 0.1209 |
  105 | 0.7312 ± 0.0435 | 0.5250 ± 0.0538 | 0.8082 ± 0.0380 | 0.4656 ± 0.0452 |
  106 | 0.7355 ± 0.1009 | 0.5125 ± 0.1019 | 0.7517 ± 0.0414 | 0.4938 ± 0.0622 |
  107 | 0.7159 ± 0.0145 | 0.4938 ± 0.0637 | 0.7018 ± 0.0110 | 0.4938 ± 0.0459 |
  108 | 0.7445 ± 0.0518 | 0.5312 ± 0.0442 | 0.6938 ± 0.0052 | 0.4969 ± 0.0855 |
  109 | 0.7705 ± 0.0650 | 0.4562 ± 0.0755 | 0.8359 ± 0.0571 | 0.4906 ± 0.0542 |
  110 | 0.7646 ± 0.0573 | 0.5563 ± 0.0500 | 0.6955 ± 0.0027 | 0.4437 ± 0.0723 |
  111 | 0.7176 ± 0.0427 | 0.5125 ± 0.0755 | 0.7124 ± 0.0314 | 0.5156 ± 0.0674 |
  112 | 0.7042 ± 0.0127 | 0.5000 ± 0.0884 | 0.6626 ± 0.0289 | 0.6250 ± 0.0815 |
  113 | 0.6822 ± 0.0237 | 0.5938 ± 0.0884 | 0.7483 ± 0.0616 | 0.5188 ± 0.0817 |
  114 | 0.6898 ± 0.0060 | 0.5500 ± 0.0375 | 0.7044 ± 0.0164 | 0.4406 ± 0.1131 |
  115 | 0.6963 ± 0.0094 | 0.5062 ± 0.0606 | 0.6901 ± 0.0056 | 0.5406 ± 0.0485 |
  116 | 0.7111 ± 0.0213 | 0.4625 ± 0.0538 | 0.6965 ± 0.0075 | 0.4906 ± 0.0577 |
  117 | 0.7032 ± 0.0116 | 0.4375 ± 0.0740 | 0.6934 ± 0.0039 | 0.5000 ± 0.0916 |
  118 | 0.7068 ± 0.0376 | 0.5188 ± 0.0852 | 0.7182 ± 0.0471 | 0.5188 ± 0.0886 |
  119 | 0.7121 ± 0.0489 | 0.5437 ± 0.0755 | 0.7372 ± 0.0320 | 0.4250 ± 0.0875 |
  120 | 0.7152 ± 0.0442 | 0.5062 ± 0.1053 | 0.7068 ± 0.0306 | 0.4969 ± 0.0963 |
  121 | 0.7553 ± 0.1598 | 0.5437 ± 0.1406 | 1.0396 ± 0.1191 | 0.4594 ± 0.0753 |
  122 | 0.7627 ± 0.0746 | 0.4938 ± 0.0871 | 0.6948 ± 0.0248 | 0.5281 ± 0.0911 |
  123 | 0.7188 ± 0.0337 | 0.4688 ± 0.0685 | 0.6971 ± 0.0189 | 0.5000 ± 0.1092 |
  124 | 0.7422 ± 0.0701 | 0.4688 ± 0.0988 | 0.7182 ± 0.0283 | 0.4969 ± 0.0647 |
  125 | 0.7020 ± 0.0063 | 0.4750 ± 0.0306 | 0.6925 ± 0.0006 | 0.5469 ± 0.0806 |
  126 | 0.7445 ± 0.0529 | 0.4625 ± 0.1159 | 0.7493 ± 0.0423 | 0.4813 ± 0.0702 |
  127 | 0.7094 ± 0.0291 | 0.5062 ± 0.0914 | 0.6881 ± 0.0049 | 0.5625 ± 0.0464 |
  128 | 0.7885 ± 0.0676 | 0.4313 ± 0.0500 | 0.6952 ± 0.0109 | 0.5000 ± 0.0753 |
  129 | 0.7100 ± 0.0286 | 0.4125 ± 0.1287 | 0.7993 ± 0.0920 | 0.4719 ± 0.1113 |
  130 | 0.7004 ± 0.0213 | 0.5000 ± 0.1186 | 0.7129 ± 0.0236 | 0.4906 ± 0.0641 |
  131 | 0.6978 ± 0.0091 | 0.5000 ± 0.0593 | 0.7042 ± 0.0178 | 0.4813 ± 0.0768 |
  132 | 0.6978 ± 0.0376 | 0.5437 ± 0.0755 | 0.7209 ± 0.0280 | 0.4531 ± 0.0876 |
  133 | 0.7469 ± 0.0369 | 0.4938 ± 0.0306 | 0.7297 ± 0.0263 | 0.5000 ± 0.0484 |
  134 | 0.6955 ± 0.0176 | 0.5125 ± 0.0940 | 0.7381 ± 0.0522 | 0.4844 ± 0.0961 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda/disk1/e.darco/venvs/pytorch-nightly/bin/python: Error while finding module specification for 'grid-search.py' (ModuleNotFoundError: __path__ attribute not found on 'grid-search' while trying to find 'grid-search.py'). Try using 'grid-search' instead of 'grid-search.py' as the module name.

 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6852 ± 0.0077 | 0.5875 ± 0.0935 | 0.6794 ± 0.0117 | 0.5906 ± 0.1022 |
    2 | 0.5895 ± 0.0469 | 0.7125 ± 0.0459 | 0.6001 ± 0.0561 | 0.7094 ± 0.0713 |
    3 | 0.6616 ± 0.0305 | 0.6375 ± 0.0319 | 0.5985 ± 0.0420 | 0.7375 ± 0.0643 |
    4 | 0.5719 ± 0.0714 | 0.7125 ± 0.0667 | 0.5929 ± 0.0636 | 0.6469 ± 0.0884 |
    5 | 0.5350 ± 0.0619 | 0.7812 ± 0.0442 | 0.6191 ± 0.1422 | 0.6906 ± 0.0808 |
    6 | 0.6534 ± 0.0508 | 0.6438 ± 0.0980 | 0.6626 ± 0.0545 | 0.6375 ± 0.0715 |
    7 | 0.6706 ± 0.0936 | 0.6125 ± 0.1128 | 0.6068 ± 0.0488 | 0.6875 ± 0.0609 |
    8 | 0.6313 ± 0.0795 | 0.6188 ± 0.1272 | 0.6714 ± 0.0276 | 0.5375 ± 0.0622 |
    9 | 0.6275 ± 0.0716 | 0.6562 ± 0.0765 | 0.8591 ± 0.1102 | 0.5437 ± 0.0864 |
   10 | 0.6191 ± 0.0648 | 0.6937 ± 0.0667 | 0.6209 ± 0.0370 | 0.7281 ± 0.0610 |
   11 | 0.7621 ± 0.1540 | 0.6188 ± 0.1035 | 0.6815 ± 0.0203 | 0.5875 ± 0.0914 |
   12 | 0.6838 ± 0.0367 | 0.5437 ± 0.0960 | 0.5951 ± 0.0496 | 0.7312 ± 0.0852 |
   13 | 0.6470 ± 0.0265 | 0.6062 ± 0.0755 | 0.6750 ± 0.0317 | 0.4688 ± 0.1083 |
   14 | 0.6996 ± 0.1356 | 0.6250 ± 0.0815 | 0.8293 ± 0.1201 | 0.5687 ± 0.0710 |
   15 | 0.6964 ± 0.0532 | 0.5250 ± 0.0776 | 0.7321 ± 0.0793 | 0.5406 ± 0.0713 |
   16 | 0.7604 ± 0.0972 | 0.6188 ± 0.0935 | 0.6513 ± 0.0594 | 0.6281 ± 0.0647 |
   17 | 0.6902 ± 0.0896 | 0.6750 ± 0.1128 | 0.6457 ± 0.0421 | 0.6469 ± 0.0959 |
   18 | 0.6220 ± 0.0222 | 0.7000 ± 0.0319 | 0.6316 ± 0.0564 | 0.6687 ± 0.0508 |
   19 | 0.6811 ± 0.0713 | 0.5687 ± 0.1108 | 0.5944 ± 0.0419 | 0.7125 ± 0.0788 |
   20 | 0.6968 ± 0.1238 | 0.6000 ± 0.1302 | 0.7670 ± 0.0835 | 0.5094 ± 0.0791 |
   21 | 0.6279 ± 0.0748 | 0.6750 ± 0.0643 | 0.6819 ± 0.0825 | 0.6531 ± 0.0493 |
   22 | 0.7413 ± 0.1178 | 0.5750 ± 0.1259 | 0.7008 ± 0.0825 | 0.5938 ± 0.0850 |
   23 | 0.7188 ± 0.1651 | 0.6062 ± 0.1349 | 0.5845 ± 0.0839 | 0.7250 ± 0.0813 |
   24 | 0.6212 ± 0.0861 | 0.6125 ± 0.0755 | 0.5847 ± 0.0956 | 0.7031 ± 0.0919 |
   25 | 0.5819 ± 0.0587 | 0.7500 ± 0.0442 | 0.5922 ± 0.0582 | 0.7031 ± 0.0613 |
   26 | 0.6655 ± 0.1086 | 0.5000 ± 0.1169 | 0.6496 ± 0.0545 | 0.5687 ± 0.0480 |
   27 | 0.5741 ± 0.0530 | 0.7063 ± 0.0755 | 0.7851 ± 0.1294 | 0.6531 ± 0.0705 |
   28 | 0.5886 ± 0.0679 | 0.6500 ± 0.1346 | 0.6335 ± 0.1283 | 0.6656 ± 0.1391 |
   29 | 0.6914 ± 0.0451 | 0.5750 ± 0.0897 | 0.6486 ± 0.0571 | 0.6656 ± 0.0959 |
   30 | 0.6212 ± 0.0320 | 0.6813 ± 0.0637 | 0.6267 ± 0.0663 | 0.6375 ± 0.0742 |
   31 | 0.6710 ± 0.0603 | 0.5750 ± 0.0755 | 0.6496 ± 0.0841 | 0.5312 ± 0.1036 |
   32 | 0.5363 ± 0.0643 | 0.7500 ± 0.0685 | 0.6826 ± 0.0930 | 0.6531 ± 0.0705 |
   33 | 0.5754 ± 0.1172 | 0.7063 ± 0.0940 | 0.5507 ± 0.0374 | 0.7219 ± 0.0326 |
   34 | 0.5793 ± 0.0664 | 0.6312 ± 0.0459 | 0.6967 ± 0.1094 | 0.5500 ± 0.0970 |
   35 | 0.5555 ± 0.1558 | 0.7562 ± 0.1142 | 0.5942 ± 0.0549 | 0.6937 ± 0.0622 |
   36 | 0.6327 ± 0.0440 | 0.6813 ± 0.0606 | 0.5879 ± 0.0514 | 0.7000 ± 0.0688 |
   37 | 0.6535 ± 0.0585 | 0.5625 ± 0.0815 | 0.6205 ± 0.0519 | 0.7125 ± 0.0606 |
   38 | 0.6652 ± 0.0427 | 0.6062 ± 0.0643 | 0.6499 ± 0.0563 | 0.6344 ± 0.0884 |
   39 | 0.6154 ± 0.0790 | 0.6813 ± 0.1016 | 0.6492 ± 0.0380 | 0.6469 ± 0.0753 |
   40 | 0.6977 ± 0.0982 | 0.6750 ± 0.0940 | 0.6261 ± 0.0841 | 0.6656 ± 0.0740 |
   41 | 0.7720 ± 0.1268 | 0.5687 ± 0.1090 | 0.6265 ± 0.0619 | 0.6719 ± 0.0841 |
   42 | 0.6697 ± 0.0570 | 0.5062 ± 0.0871 | 0.5968 ± 0.0905 | 0.6937 ± 0.0824 |
   43 | 0.6036 ± 0.0449 | 0.6937 ± 0.0573 | 0.6350 ± 0.0809 | 0.6500 ± 0.1025 |
   44 | 0.6398 ± 0.0289 | 0.5813 ± 0.0729 | 0.6752 ± 0.0597 | 0.5594 ± 0.0758 |
   45 | 0.6444 ± 0.0940 | 0.6562 ± 0.0862 | 0.5785 ± 0.0642 | 0.7031 ± 0.0730 |
   46 | 0.6416 ± 0.0948 | 0.6813 ± 0.0723 | 0.6670 ± 0.0792 | 0.5437 ± 0.0643 |
   47 | 0.6679 ± 0.0821 | 0.5938 ± 0.0656 | 0.6121 ± 0.1435 | 0.6875 ± 0.0938 |
   48 | 0.6306 ± 0.0434 | 0.5875 ± 0.0500 | 0.7178 ± 0.0703 | 0.4875 ± 0.0875 |
   49 | 0.6361 ± 0.0767 | 0.6937 ± 0.0500 | 0.6470 ± 0.0643 | 0.6687 ± 0.0527 |
   50 | 0.6909 ± 0.0535 | 0.5813 ± 0.1057 | 0.6874 ± 0.0789 | 0.6438 ± 0.0829 |
   51 | 0.6108 ± 0.1038 | 0.7063 ± 0.0897 | 0.6266 ± 0.0772 | 0.6625 ± 0.0637 |
   52 | 0.6491 ± 0.0496 | 0.6500 ± 0.0415 | 0.6589 ± 0.0883 | 0.6281 ± 0.0549 |
   53 | 0.6360 ± 0.0448 | 0.6625 ± 0.0723 | 0.5826 ± 0.0421 | 0.7219 ± 0.0632 |
   54 | 0.6170 ± 0.1084 | 0.6875 ± 0.0906 | 0.6265 ± 0.0624 | 0.6750 ± 0.0545 |
   55 | 0.6233 ± 0.0531 | 0.6062 ± 0.1057 | 0.6107 ± 0.0523 | 0.6531 ± 0.0878 |
   56 | 0.6300 ± 0.0611 | 0.6562 ± 0.0395 | 0.6709 ± 0.0947 | 0.4875 ± 0.1282 |
   57 | 0.6741 ± 0.0566 | 0.5750 ± 0.0612 | 0.7917 ± 0.1069 | 0.4781 ± 0.0948 |
   58 | 0.6265 ± 0.0643 | 0.6687 ± 0.0508 | 0.6564 ± 0.0729 | 0.5188 ± 0.0829 |
   59 | 0.6411 ± 0.0582 | 0.6125 ± 0.1589 | 0.6201 ± 0.0511 | 0.6750 ± 0.0715 |
   60 | 0.6407 ± 0.0305 | 0.6438 ± 0.0424 | 0.6189 ± 0.0413 | 0.6594 ± 0.0942 |
   61 | 0.5957 ± 0.0561 | 0.6813 ± 0.0723 | 0.5931 ± 0.0274 | 0.6500 ± 0.0737 |
   62 | 0.5956 ± 0.0343 | 0.6750 ± 0.0643 | 0.6021 ± 0.0712 | 0.6750 ± 0.0793 |
   63 | 0.6667 ± 0.1246 | 0.6813 ± 0.0914 | 0.5894 ± 0.0558 | 0.6906 ± 0.0584 |
   64 | 0.6799 ± 0.0484 | 0.5125 ± 0.0468 | 0.6739 ± 0.0680 | 0.5062 ± 0.0935 |
   65 | 0.6819 ± 0.0162 | 0.5375 ± 0.0824 | 0.6932 ± 0.0174 | 0.5188 ± 0.1212 |
   66 | 0.7348 ± 0.0704 | 0.5375 ± 0.0637 | 0.7479 ± 0.0575 | 0.5000 ± 0.0862 |
   67 | 0.7497 ± 0.0578 | 0.5188 ± 0.0545 | 0.7865 ± 0.0923 | 0.5125 ± 0.0990 |
   68 | 0.6973 ± 0.0103 | 0.5188 ± 0.0755 | 0.6950 ± 0.0223 | 0.5312 ± 0.0740 |
   69 | 0.6665 ± 0.0391 | 0.5625 ± 0.1046 | 0.6699 ± 0.0549 | 0.5719 ± 0.0979 |
   70 | 0.6060 ± 0.0562 | 0.7188 ± 0.0815 | 0.6037 ± 0.0701 | 0.7000 ± 0.0715 |
   71 | 0.6983 ± 0.0667 | 0.5188 ± 0.1487 | 0.7499 ± 0.0920 | 0.4938 ± 0.1035 |
   72 | 0.6007 ± 0.0722 | 0.7000 ± 0.0643 | 0.5864 ± 0.0809 | 0.6937 ± 0.1025 |
   73 | 0.6145 ± 0.0439 | 0.5813 ± 0.0729 | 0.6704 ± 0.0877 | 0.6562 ± 0.0753 |
   74 | 0.7108 ± 0.0862 | 0.5125 ± 0.1075 | 0.6936 ± 0.0750 | 0.5437 ± 0.1093 |
   75 | 0.5514 ± 0.0951 | 0.7125 ± 0.0893 | 0.6595 ± 0.0853 | 0.5000 ± 0.0958 |
   76 | 0.6206 ± 0.1018 | 0.6875 ± 0.0765 | 0.6574 ± 0.0649 | 0.6375 ± 0.0702 |
   77 | 0.6208 ± 0.0836 | 0.7063 ± 0.0729 | 0.6336 ± 0.0836 | 0.6531 ± 0.0942 |
   78 | 0.6337 ± 0.0958 | 0.6875 ± 0.1326 | 0.6339 ± 0.0288 | 0.6469 ± 0.0542 |
   79 | 0.6117 ± 0.0794 | 0.6937 ± 0.0750 | 0.5726 ± 0.0389 | 0.7344 ± 0.0730 |
   80 | 0.5996 ± 0.0252 | 0.6562 ± 0.0713 | 0.6701 ± 0.0335 | 0.4813 ± 0.0508 |
   81 | 0.6641 ± 0.0482 | 0.6562 ± 0.0342 | 0.6062 ± 0.0655 | 0.6969 ± 0.0740 |
   82 | 0.6886 ± 0.0756 | 0.5437 ± 0.1378 | 0.6515 ± 0.0830 | 0.6750 ± 0.0545 |
   83 | 0.6125 ± 0.0366 | 0.6375 ± 0.0755 | 0.6238 ± 0.0495 | 0.6094 ± 0.1048 |
   84 | 0.6081 ± 0.0513 | 0.6375 ± 0.0702 | 0.6525 ± 0.0573 | 0.4875 ± 0.0980 |
   85 | 0.7158 ± 0.1040 | 0.5375 ± 0.0996 | 0.5953 ± 0.0732 | 0.7447 ± 0.1159 |
   86 | 0.6185 ± 0.0829 | 0.6750 ± 0.0897 | 0.6299 ± 0.0861 | 0.6531 ± 0.0952 |
   87 | 0.7487 ± 0.1780 | 0.5813 ± 0.1500 | 0.6402 ± 0.0781 | 0.7125 ± 0.0637 |
   88 | 0.5969 ± 0.0359 | 0.5938 ± 0.1544 | 0.6343 ± 0.0572 | 0.4844 ± 0.0830 |
   89 | 0.6044 ± 0.0309 | 0.6875 ± 0.1046 | 0.5867 ± 0.0442 | 0.7312 ± 0.0658 |
   90 | 0.6023 ± 0.0736 | 0.7000 ± 0.0897 | 0.6018 ± 0.0754 | 0.6969 ± 0.1074 |
   91 | 0.6625 ± 0.0426 | 0.6062 ± 0.0852 | 0.6349 ± 0.0720 | 0.5750 ± 0.0852 |
   92 | 0.6747 ± 0.0417 | 0.6125 ± 0.1433 | 0.8039 ± 0.1592 | 0.6687 ± 0.0950 |
   93 | 0.6257 ± 0.0278 | 0.5563 ± 0.0848 | 0.7047 ± 0.0586 | 0.5563 ± 0.0737 |
   94 | 0.6551 ± 0.0545 | 0.6562 ± 0.0625 | 0.6122 ± 0.0310 | 0.6969 ± 0.0281 |
   95 | 0.6415 ± 0.0519 | 0.6250 ± 0.0656 | 0.6287 ± 0.0472 | 0.7219 ± 0.0632 |
   96 | 0.6352 ± 0.0286 | 0.5437 ± 0.0702 | 0.6277 ± 0.0473 | 0.6469 ± 0.0656 |
   97 | 0.6812 ± 0.0604 | 0.5813 ± 0.0875 | 0.5948 ± 0.0455 | 0.7156 ± 0.0719 |
   98 | 0.5985 ± 0.0645 | 0.5875 ± 0.0914 | 0.6831 ± 0.0485 | 0.4594 ± 0.1008 |
   99 | 0.6233 ± 0.0966 | 0.6937 ± 0.0848 | 0.6316 ± 0.0877 | 0.6531 ± 0.0796 |
  100 | 0.6397 ± 0.1136 | 0.6687 ± 0.0875 | 0.6243 ± 0.0393 | 0.4625 ± 0.0459 |
  101 | 0.6189 ± 0.0808 | 0.6687 ± 0.0897 | 0.6761 ± 0.0861 | 0.6344 ± 0.0766 |
  102 | 0.6894 ± 0.0538 | 0.5625 ± 0.0342 | 0.6714 ± 0.0735 | 0.5156 ± 0.0841 |
  103 | 0.6314 ± 0.0830 | 0.5875 ± 0.1431 | 0.6312 ± 0.0632 | 0.6844 ± 0.0647 |
  104 | 0.5666 ± 0.0315 | 0.7438 ± 0.0364 | 0.6091 ± 0.1186 | 0.6875 ± 0.0978 |
  105 | 0.6621 ± 0.0277 | 0.5563 ± 0.1332 | 0.6474 ± 0.0742 | 0.6312 ± 0.0682 |
  106 | 0.6411 ± 0.1115 | 0.6250 ± 0.1008 | 0.5845 ± 0.0838 | 0.7156 ± 0.0867 |
  107 | 0.6243 ± 0.0993 | 0.6875 ± 0.1202 | 0.6592 ± 0.1095 | 0.6438 ± 0.0940 |
  108 | 0.6911 ± 0.0791 | 0.5938 ± 0.1027 | 0.6551 ± 0.1182 | 0.7063 ± 0.0841 |
  109 | 0.6377 ± 0.1510 | 0.6625 ± 0.1072 | 0.6088 ± 0.0941 | 0.7125 ± 0.1053 |
  110 | 0.6228 ± 0.0627 | 0.6937 ± 0.0538 | 0.5895 ± 0.0653 | 0.6844 ± 0.0632 |
  111 | 0.6671 ± 0.0142 | 0.5687 ± 0.1225 | 0.6747 ± 0.0354 | 0.5375 ± 0.0667 |
  112 | 0.6650 ± 0.0577 | 0.5750 ± 0.1057 | 0.6276 ± 0.0565 | 0.6406 ± 0.1111 |
  113 | 0.6660 ± 0.0749 | 0.6375 ± 0.0580 | 0.6462 ± 0.0712 | 0.5156 ± 0.0991 |
  114 | 0.6611 ± 0.0568 | 0.6125 ± 0.1228 | 0.6452 ± 0.0963 | 0.6781 ± 0.0766 |
  115 | 0.5962 ± 0.0703 | 0.7125 ± 0.0696 | 0.7144 ± 0.0655 | 0.6219 ± 0.0632 |
  116 | 0.6437 ± 0.0943 | 0.6250 ± 0.1218 | 0.6042 ± 0.0595 | 0.5656 ± 0.0911 |
  117 | 0.6731 ± 0.0553 | 0.6125 ± 0.0375 | 0.6171 ± 0.0629 | 0.6625 ± 0.0696 |
  118 | 0.6771 ± 0.0297 | 0.5875 ± 0.0800 | 0.6903 ± 0.0660 | 0.5344 ± 0.0771 |
  119 | 0.6391 ± 0.0219 | 0.6750 ± 0.0545 | 0.6565 ± 0.0264 | 0.6094 ± 0.0730 |
  120 | 0.6537 ± 0.0498 | 0.6625 ± 0.0606 | 0.5891 ± 0.0286 | 0.6969 ± 0.0443 |
  121 | 0.6369 ± 0.0142 | 0.6687 ± 0.0319 | 0.5930 ± 0.0228 | 0.7063 ± 0.0468 |
  122 | 0.6656 ± 0.0588 | 0.6375 ± 0.1000 | 0.6075 ± 0.0573 | 0.7031 ± 0.0908 |
  123 | 0.6283 ± 0.0323 | 0.6312 ± 0.0606 | 0.6697 ± 0.0529 | 0.6250 ± 0.0640 |
  124 | 0.6272 ± 0.1302 | 0.5687 ± 0.0776 | 0.6281 ± 0.0355 | 0.6781 ± 0.0420 |
  125 | 0.6640 ± 0.0414 | 0.5687 ± 0.0667 | 0.6238 ± 0.0947 | 0.6656 ± 0.0577 |
  126 | 0.6618 ± 0.0371 | 0.6438 ± 0.0545 | 0.6495 ± 0.0569 | 0.5687 ± 0.1225 |
  127 | 0.6341 ± 0.0658 | 0.6062 ± 0.1128 | 0.6392 ± 0.0736 | 0.6875 ± 0.0713 |
  128 | 0.6005 ± 0.0301 | 0.7312 ± 0.0424 | 0.6653 ± 0.0733 | 0.6406 ± 0.0981 |
  129 | 0.6329 ± 0.0478 | 0.6750 ± 0.0375 | 0.6245 ± 0.0697 | 0.6562 ± 0.1064 |
  130 | 0.6802 ± 0.0677 | 0.5500 ± 0.0375 | 0.6512 ± 0.0711 | 0.6281 ± 0.0952 |
  131 | 0.5718 ± 0.0623 | 0.7500 ± 0.0948 | 0.6370 ± 0.0823 | 0.6594 ± 0.0745 |
  132 | 0.5488 ± 0.1118 | 0.6687 ± 0.0852 | 0.5695 ± 0.0900 | 0.7063 ± 0.0628 |
  133 | 0.5539 ± 0.0201 | 0.6875 ± 0.0523 | 0.6208 ± 0.0638 | 0.6594 ± 0.0662 |
  134 | 0.6110 ± 0.0360 | 0.6625 ± 0.0459 | 0.6641 ± 0.0885 | 0.6406 ± 0.0563 |
Error in cpuinfo: prctl(PR_SVE_GET_VL) failed
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6834 ± 0.0190 | 0.5437 ± 0.0319 | 0.6802 ± 0.0088 | 0.5781 ± 0.0674 |
    2 | 0.6455 ± 0.0725 | 0.6312 ± 0.1317 | 0.6445 ± 0.0515 | 0.6094 ± 0.0743 |
    3 | 0.5858 ± 0.0772 | 0.7125 ± 0.0824 | 0.5952 ± 0.0325 | 0.7063 ± 0.0286 |
    4 | 0.6258 ± 0.0457 | 0.6500 ± 0.0500 | 0.6014 ± 0.0529 | 0.6750 ± 0.1154 |
    5 | 0.6858 ± 0.0875 | 0.6312 ± 0.1108 | 0.6138 ± 0.0460 | 0.6406 ± 0.0644 |
    6 | 0.6148 ± 0.0589 | 0.5875 ± 0.0606 | 0.6204 ± 0.0580 | 0.6656 ± 0.0465 |
    7 | 0.6476 ± 0.0785 | 0.6188 ± 0.0776 | 0.5861 ± 0.0289 | 0.6969 ± 0.0766 |
    8 | 0.6697 ± 0.0738 | 0.6562 ± 0.0791 | 0.6382 ± 0.0447 | 0.6406 ± 0.0613 |
    9 | 0.5783 ± 0.0596 | 0.6937 ± 0.1209 | 0.6334 ± 0.0580 | 0.6062 ± 0.0729 |
   10 | 0.6141 ± 0.0671 | 0.6813 ± 0.0776 | 0.5936 ± 0.0777 | 0.7031 ± 0.0716 |
   11 | 0.6047 ± 0.1029 | 0.7000 ± 0.1128 | 0.5984 ± 0.0641 | 0.6813 ± 0.0871 |
   12 | 0.6308 ± 0.0232 | 0.6125 ± 0.0424 | 0.6109 ± 0.0637 | 0.6781 ± 0.0839 |
   13 | 0.5894 ± 0.0812 | 0.6937 ± 0.0606 | 0.6096 ± 0.0479 | 0.6625 ± 0.0776 |
   14 | 0.5611 ± 0.0723 | 0.7250 ± 0.0637 | 0.6226 ± 0.0669 | 0.6531 ± 0.0691 |
   15 | 0.6360 ± 0.0583 | 0.6687 ± 0.0468 | 0.5965 ± 0.0611 | 0.6844 ± 0.0808 |
   16 | 0.5615 ± 0.0378 | 0.7188 ± 0.0342 | 0.5878 ± 0.0756 | 0.6969 ± 0.0884 |
   17 | 0.6245 ± 0.0534 | 0.6500 ± 0.0914 | 0.6004 ± 0.0315 | 0.6656 ± 0.0713 |
   18 | 0.5592 ± 0.0508 | 0.7063 ± 0.0643 | 0.5938 ± 0.0574 | 0.6875 ± 0.0978 |
   19 | 0.6139 ± 0.0256 | 0.7375 ± 0.0729 | 0.6047 ± 0.0431 | 0.6906 ± 0.0921 |
   20 | 0.6272 ± 0.0737 | 0.6500 ± 0.0956 | 0.6147 ± 0.0651 | 0.6844 ± 0.0719 |
   21 | 0.5901 ± 0.0664 | 0.6562 ± 0.0685 | 0.6034 ± 0.0632 | 0.7063 ± 0.0596 |
   22 | 0.5939 ± 0.0616 | 0.6813 ± 0.0824 | 0.6381 ± 0.0299 | 0.6375 ± 0.0250 |
   23 | 0.6231 ± 0.0678 | 0.6562 ± 0.0862 | 0.6024 ± 0.0444 | 0.6469 ± 0.0656 |
   24 | 0.6227 ± 0.0537 | 0.6875 ± 0.0815 | 0.5860 ± 0.0580 | 0.6937 ± 0.0914 |
   25 | 0.6292 ± 0.0214 | 0.6625 ± 0.0234 | 0.5956 ± 0.0269 | 0.6937 ± 0.0480 |
   26 | 0.5942 ± 0.0523 | 0.6875 ± 0.0523 | 0.6637 ± 0.0907 | 0.6469 ± 0.0779 |
   27 | 0.6199 ± 0.1205 | 0.7188 ± 0.0862 | 0.6458 ± 0.0702 | 0.6375 ± 0.0960 |
   28 | 0.5937 ± 0.0824 | 0.6875 ± 0.0927 | 0.6080 ± 0.0681 | 0.6969 ± 0.0727 |
   29 | 0.5581 ± 0.0303 | 0.7500 ± 0.0395 | 0.6480 ± 0.1047 | 0.6687 ± 0.1137 |
   30 | 0.5891 ± 0.0525 | 0.7000 ± 0.0424 | 0.5948 ± 0.0428 | 0.6875 ± 0.0827 |
   31 | 0.6454 ± 0.0831 | 0.6312 ± 0.1090 | 0.6083 ± 0.0557 | 0.6844 ± 0.0942 |
   32 | 0.6325 ± 0.0756 | 0.6750 ± 0.0940 | 0.6077 ± 0.0609 | 0.6906 ± 0.0531 |
   33 | 0.6706 ± 0.0576 | 0.6188 ± 0.0234 | 0.6223 ± 0.0344 | 0.6687 ± 0.0658 |
   34 | 0.5797 ± 0.0671 | 0.6625 ± 0.0606 | 0.5852 ± 0.0526 | 0.6906 ± 0.0732 |
   35 | 0.6288 ± 0.0284 | 0.6500 ± 0.0306 | 0.6311 ± 0.0370 | 0.6031 ± 0.0753 |
   36 | 0.6077 ± 0.1076 | 0.6062 ± 0.1433 | 0.6193 ± 0.0296 | 0.6344 ± 0.0626 |
   37 | 0.5858 ± 0.0633 | 0.7000 ± 0.1163 | 0.5906 ± 0.0583 | 0.6594 ± 0.0691 |
   38 | 0.6108 ± 0.0741 | 0.6687 ± 0.0612 | 0.6159 ± 0.0495 | 0.6562 ± 0.0815 |
   39 | 0.6150 ± 0.1353 | 0.6813 ± 0.1484 | 0.5984 ± 0.0460 | 0.6656 ± 0.0465 |
   40 | 0.6171 ± 0.0463 | 0.6625 ± 0.0538 | 0.6020 ± 0.0385 | 0.7063 ± 0.0596 |
   41 | 0.5828 ± 0.0409 | 0.7125 ± 0.0459 | 0.5855 ± 0.0396 | 0.7312 ± 0.0563 |
   42 | 0.6193 ± 0.0758 | 0.6875 ± 0.1064 | 0.6128 ± 0.0453 | 0.6500 ± 0.0776 |
   43 | 0.5304 ± 0.0277 | 0.7250 ± 0.0848 | 0.5619 ± 0.0759 | 0.6469 ± 0.0594 |
   44 | 0.6387 ± 0.0881 | 0.6375 ± 0.0612 | 0.6014 ± 0.0599 | 0.6750 ± 0.0781 |
   45 | 0.6166 ± 0.0742 | 0.6625 ± 0.1090 | 0.6089 ± 0.0456 | 0.6969 ± 0.0485 |
   46 | 0.5613 ± 0.0636 | 0.7375 ± 0.0875 | 0.6308 ± 0.0563 | 0.6719 ± 0.0563 |
   47 | 0.5830 ± 0.0348 | 0.7125 ± 0.0606 | 0.6070 ± 0.0709 | 0.6844 ± 0.0632 |
   48 | 0.5564 ± 0.0449 | 0.7500 ± 0.0342 | 0.5846 ± 0.0593 | 0.6969 ± 0.0827 |
   49 | 0.6713 ± 0.0822 | 0.6188 ± 0.0800 | 0.5971 ± 0.0739 | 0.6875 ± 0.0713 |
   50 | 0.6196 ± 0.0324 | 0.6438 ± 0.0508 | 0.6194 ± 0.0720 | 0.6750 ± 0.0897 |
   51 | 0.6548 ± 0.0392 | 0.6750 ± 0.0468 | 0.6130 ± 0.0717 | 0.6719 ± 0.0991 |
   52 | 0.6632 ± 0.0388 | 0.5687 ± 0.0696 | 0.6369 ± 0.0372 | 0.5813 ± 0.0508 |
   53 | 0.6367 ± 0.0569 | 0.5813 ± 0.1163 | 0.6176 ± 0.0319 | 0.6781 ± 0.0862 |
   54 | 0.5949 ± 0.0541 | 0.6813 ± 0.0750 | 0.6105 ± 0.0471 | 0.6719 ± 0.0743 |
   55 | 0.6415 ± 0.0442 | 0.5875 ± 0.0538 | 0.6000 ± 0.0569 | 0.6687 ± 0.1000 |
   56 | 0.5858 ± 0.0686 | 0.7000 ± 0.0673 | 0.5805 ± 0.0494 | 0.7063 ± 0.0628 |
   57 | 0.6199 ± 0.0263 | 0.6687 ± 0.0643 | 0.6027 ± 0.0395 | 0.7063 ± 0.0829 |
   58 | 0.5310 ± 0.0370 | 0.7063 ± 0.0643 | 0.6733 ± 0.0750 | 0.5656 ± 0.0889 |
   59 | 0.6272 ± 0.0666 | 0.6250 ± 0.1218 | 0.5810 ± 0.0575 | 0.6687 ± 0.0628 |
   60 | 0.6134 ± 0.0591 | 0.6312 ± 0.0538 | 0.6004 ± 0.0728 | 0.6656 ± 0.0979 |
   61 | 0.6118 ± 0.0587 | 0.6438 ± 0.0755 | 0.5683 ± 0.0638 | 0.7219 ± 0.1002 |
   62 | 0.5448 ± 0.0562 | 0.7750 ± 0.0776 | 0.6126 ± 0.0606 | 0.6906 ± 0.0832 |
   63 | 0.5326 ± 0.0558 | 0.7438 ± 0.0459 | 0.5883 ± 0.0942 | 0.6906 ± 0.0867 |
   64 | 0.5859 ± 0.0394 | 0.7063 ± 0.0468 | 0.5973 ± 0.0581 | 0.6969 ± 0.0873 |
   65 | 0.5830 ± 0.0747 | 0.6375 ± 0.0643 | 0.6131 ± 0.0451 | 0.5906 ± 0.0911 |
   66 | 0.6089 ± 0.0564 | 0.6813 ± 0.0500 | 0.6259 ± 0.0835 | 0.6625 ± 0.0836 |
   67 | 0.6506 ± 0.0433 | 0.6062 ± 0.0508 | 0.5908 ± 0.0790 | 0.6594 ± 0.0983 |
   68 | 0.5634 ± 0.0543 | 0.7188 ± 0.0740 | 0.5774 ± 0.0538 | 0.7094 ± 0.0827 |
   69 | 0.6365 ± 0.0599 | 0.6438 ± 0.0424 | 0.5843 ± 0.0449 | 0.6813 ± 0.0710 |
   70 | 0.5997 ± 0.0538 | 0.6562 ± 0.0523 | 0.6184 ± 0.0514 | 0.6375 ± 0.0805 |
   71 | 0.5889 ± 0.0850 | 0.7188 ± 0.1100 | 0.6356 ± 0.0881 | 0.6781 ± 0.0671 |
   72 | 0.6003 ± 0.0168 | 0.5500 ± 0.0375 | 0.6480 ± 0.0635 | 0.5437 ± 0.0929 |
   73 | 0.6045 ± 0.0464 | 0.6375 ± 0.0468 | 0.6274 ± 0.0744 | 0.6500 ± 0.0653 |
   74 | 0.5619 ± 0.0548 | 0.7000 ± 0.0897 | 0.6057 ± 0.0597 | 0.6562 ± 0.0803 |
   75 | 0.5743 ± 0.0552 | 0.6937 ± 0.0606 | 0.6150 ± 0.0553 | 0.6906 ± 0.0493 |
   76 | 0.5922 ± 0.0570 | 0.7063 ± 0.0980 | 0.5906 ± 0.0664 | 0.6906 ± 0.0867 |
   77 | 0.6155 ± 0.0497 | 0.6625 ± 0.1142 | 0.5918 ± 0.0585 | 0.6656 ± 0.0803 |
   78 | 0.5880 ± 0.0548 | 0.6937 ± 0.1375 | 0.5649 ± 0.0546 | 0.7156 ± 0.0531 |
   79 | 0.5968 ± 0.0682 | 0.6750 ± 0.0940 | 0.6251 ± 0.0521 | 0.6750 ± 0.0545 |
   80 | 0.6159 ± 0.0513 | 0.6750 ± 0.0643 | 0.5858 ± 0.0562 | 0.7031 ± 0.0806 |
   81 | 0.6079 ± 0.0478 | 0.7188 ± 0.0523 | 0.6075 ± 0.0867 | 0.6750 ± 0.0886 |
   82 | 0.5725 ± 0.0805 | 0.7063 ± 0.0897 | 0.6132 ± 0.0826 | 0.6875 ± 0.0778 |
   83 | 0.5464 ± 0.0429 | 0.7625 ± 0.0468 | 0.5909 ± 0.0676 | 0.6937 ± 0.0637 |
   84 | 0.6036 ± 0.0618 | 0.6687 ± 0.1057 | 0.6308 ± 0.0601 | 0.6594 ± 0.0796 |
   85 | 0.6331 ± 0.0470 | 0.6375 ± 0.0673 | 0.6281 ± 0.0433 | 0.6288 ± 0.0867 |
   86 | 0.5794 ± 0.0553 | 0.7562 ± 0.0459 | 0.5642 ± 0.0501 | 0.7219 ± 0.0705 |
   87 | 0.5496 ± 0.0167 | 0.7312 ± 0.0424 | 0.5787 ± 0.0613 | 0.7063 ± 0.0488 |
   88 | 0.5571 ± 0.0436 | 0.7125 ± 0.0500 | 0.5794 ± 0.0400 | 0.7188 ± 0.0442 |
   89 | 0.5608 ± 0.0856 | 0.7063 ± 0.0980 | 0.5885 ± 0.0796 | 0.7125 ± 0.0824 |
   90 | 0.5961 ± 0.0461 | 0.6687 ± 0.0612 | 0.5966 ± 0.0432 | 0.6469 ± 0.0542 |
   91 | 0.5754 ± 0.1133 | 0.6813 ± 0.1693 | 0.6058 ± 0.0772 | 0.6781 ± 0.0671 |
   92 | 0.6314 ± 0.0728 | 0.6875 ± 0.0395 | 0.5751 ± 0.0453 | 0.7000 ± 0.0545 |
   93 | 0.6054 ± 0.0333 | 0.5875 ± 0.0723 | 0.6498 ± 0.0840 | 0.6000 ± 0.1053 |
   94 | 0.5608 ± 0.0861 | 0.7312 ± 0.1163 | 0.6181 ± 0.0549 | 0.6406 ± 0.0644 |
   95 | 0.6064 ± 0.0370 | 0.6687 ± 0.0580 | 0.6138 ± 0.0797 | 0.7000 ± 0.1019 |
   96 | 0.6231 ± 0.0238 | 0.5687 ± 0.0667 | 0.5994 ± 0.0674 | 0.6312 ± 0.0682 |
   97 | 0.5984 ± 0.0219 | 0.6312 ± 0.0914 | 0.6106 ± 0.0546 | 0.6781 ± 0.0524 |
   98 | 0.6079 ± 0.0343 | 0.6937 ± 0.0538 | 0.6472 ± 0.0655 | 0.6406 ± 0.0743 |
   99 | 0.6169 ± 0.0962 | 0.6687 ± 0.1179 | 0.6089 ± 0.0657 | 0.6687 ± 0.0715 |
  100 | 0.5857 ± 0.0505 | 0.6500 ± 0.0538 | 0.6024 ± 0.0521 | 0.6781 ± 0.0895 |
  101 | 0.5894 ± 0.0436 | 0.6687 ± 0.0580 | 0.5684 ± 0.0625 | 0.6719 ± 0.1000 |
  102 | 0.5801 ± 0.0700 | 0.6687 ± 0.1111 | 0.5703 ± 0.0367 | 0.7063 ± 0.0580 |
  103 | 0.5972 ± 0.0619 | 0.7063 ± 0.0545 | 0.5914 ± 0.0658 | 0.7000 ± 0.0729 |
  104 | 0.5857 ± 0.0208 | 0.7125 ± 0.0364 | 0.5806 ± 0.0998 | 0.7125 ± 0.0986 |
  105 | 0.6335 ± 0.0245 | 0.6000 ± 0.0538 | 0.5991 ± 0.0516 | 0.6781 ± 0.0560 |
  106 | 0.5790 ± 0.0667 | 0.7312 ± 0.0781 | 0.6135 ± 0.0573 | 0.6969 ± 0.0610 |
  107 | 0.6482 ± 0.0303 | 0.6500 ± 0.0538 | 0.6206 ± 0.0364 | 0.6375 ± 0.0658 |
  108 | 0.5226 ± 0.0448 | 0.7188 ± 0.0906 | 0.6011 ± 0.0641 | 0.6781 ± 0.0641 |
  109 | 0.6330 ± 0.0439 | 0.6062 ± 0.0319 | 0.5744 ± 0.0648 | 0.7250 ± 0.0763 |
  110 | 0.6272 ± 0.0553 | 0.6312 ± 0.0935 | 0.6281 ± 0.0571 | 0.6500 ± 0.0637 |
  111 | 0.5753 ± 0.0833 | 0.6875 ± 0.0884 | 0.6484 ± 0.0654 | 0.6438 ± 0.0545 |
  112 | 0.5720 ± 0.0708 | 0.7250 ± 0.0667 | 0.6240 ± 0.0662 | 0.6594 ± 0.1113 |
  113 | 0.5999 ± 0.0951 | 0.6750 ± 0.0755 | 0.5981 ± 0.0635 | 0.6781 ± 0.0610 |
  114 | 0.5832 ± 0.0352 | 0.7125 ± 0.0364 | 0.6015 ± 0.0982 | 0.6875 ± 0.0938 |
  115 | 0.6267 ± 0.0688 | 0.6188 ± 0.0667 | 0.5957 ± 0.0455 | 0.6750 ± 0.0864 |
  116 | 0.6156 ± 0.0206 | 0.6687 ± 0.0375 | 0.6052 ± 0.0452 | 0.7031 ± 0.0756 |
  117 | 0.6021 ± 0.0664 | 0.6937 ± 0.0667 | 0.5941 ± 0.0596 | 0.7031 ± 0.0580 |
  118 | 0.5638 ± 0.0438 | 0.7562 ± 0.0637 | 0.6012 ± 0.0594 | 0.6844 ± 0.0677 |
  119 | 0.5963 ± 0.0356 | 0.6625 ± 0.0996 | 0.5589 ± 0.0539 | 0.7375 ± 0.0688 |
  120 | 0.5367 ± 0.0458 | 0.7688 ± 0.0250 | 0.6262 ± 0.0644 | 0.6656 ± 0.0656 |
  121 | 0.6337 ± 0.0475 | 0.6625 ± 0.0538 | 0.5818 ± 0.0362 | 0.7125 ± 0.0538 |
  122 | 0.5428 ± 0.0545 | 0.7250 ± 0.0637 | 0.5971 ± 0.0782 | 0.6906 ± 0.0808 |
  123 | 0.6262 ± 0.1381 | 0.6625 ± 0.1417 | 0.6028 ± 0.0642 | 0.6875 ± 0.0753 |
  124 | 0.5539 ± 0.0626 | 0.7125 ± 0.0848 | 0.5965 ± 0.0405 | 0.6687 ± 0.0508 |
  125 | 0.5394 ± 0.0310 | 0.7312 ± 0.0702 | 0.6182 ± 0.0542 | 0.6719 ± 0.0597 |
  126 | 0.5775 ± 0.0723 | 0.6500 ± 0.0956 | 0.6126 ± 0.0512 | 0.5969 ± 0.0549 |
  127 | 0.5764 ± 0.0728 | 0.7125 ± 0.1125 | 0.6007 ± 0.0567 | 0.6719 ± 0.0756 |
  128 | 0.5766 ± 0.0696 | 0.6937 ± 0.0996 | 0.5361 ± 0.0488 | 0.7281 ± 0.0827 |
  129 | 0.5907 ± 0.0356 | 0.7000 ± 0.0319 | 0.6128 ± 0.0429 | 0.6594 ± 0.0531 |
  130 | 0.6824 ± 0.0651 | 0.6125 ± 0.0960 | 0.6063 ± 0.0553 | 0.7031 ± 0.0376 |
  131 | 0.5134 ± 0.0737 | 0.7562 ± 0.0776 | 0.6189 ± 0.0596 | 0.6719 ± 0.0580 |
  132 | 0.6219 ± 0.0408 | 0.6562 ± 0.0442 | 0.5801 ± 0.0705 | 0.6937 ± 0.0723 |
  133 | 0.5732 ± 0.0641 | 0.7000 ± 0.0852 | 0.5906 ± 0.0727 | 0.6844 ± 0.0820 |
  134 | 0.5797 ± 0.0509 | 0.7188 ± 0.0395 | 0.6016 ± 0.0681 | 0.6906 ± 0.0808 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6473 ± 0.0708 | 0.6000 ± 0.0776 | 0.6217 ± 0.0475 | 0.6406 ± 0.0659 |
    2 | 0.5632 ± 0.0509 | 0.7438 ± 0.0637 | 0.6389 ± 0.0506 | 0.6531 ± 0.0705 |
    3 | 0.5523 ± 0.0518 | 0.7125 ± 0.0723 | 0.6275 ± 0.0797 | 0.5969 ± 0.1041 |
    4 | 0.6416 ± 0.0673 | 0.6125 ± 0.0729 | 0.6488 ± 0.0761 | 0.6406 ± 0.1020 |
    5 | 0.6310 ± 0.0464 | 0.6750 ± 0.0580 | 0.5933 ± 0.0570 | 0.6906 ± 0.0473 |
    6 | 0.6253 ± 0.0371 | 0.6312 ± 0.0573 | 0.5883 ± 0.0341 | 0.6625 ± 0.0976 |
    7 | 0.5731 ± 0.0367 | 0.7250 ± 0.0459 | 0.6040 ± 0.0826 | 0.7000 ± 0.0793 |
    8 | 0.6101 ± 0.0416 | 0.6250 ± 0.0685 | 0.5934 ± 0.0399 | 0.6656 ± 0.0656 |
    9 | 0.5803 ± 0.0618 | 0.6687 ± 0.0612 | 0.6266 ± 0.0659 | 0.6813 ± 0.0848 |
   10 | 0.6324 ± 0.0489 | 0.6250 ± 0.0523 | 0.6152 ± 0.0368 | 0.6531 ± 0.0493 |
   11 | 0.6461 ± 0.1047 | 0.6188 ± 0.1332 | 0.6220 ± 0.0762 | 0.6813 ± 0.0800 |
   12 | 0.5591 ± 0.0301 | 0.7562 ± 0.0459 | 0.6232 ± 0.0654 | 0.6906 ± 0.0632 |
   13 | 0.6483 ± 0.0581 | 0.6687 ± 0.0805 | 0.5799 ± 0.0550 | 0.7063 ± 0.0829 |
   14 | 0.5424 ± 0.0336 | 0.7562 ± 0.0364 | 0.5998 ± 0.0901 | 0.7156 ± 0.0616 |
   15 | 0.6311 ± 0.0621 | 0.6438 ± 0.0643 | 0.6167 ± 0.0678 | 0.6594 ± 0.0820 |
   16 | 0.6291 ± 0.0666 | 0.6375 ± 0.0897 | 0.5916 ± 0.0316 | 0.7219 ± 0.0691 |
   17 | 0.6071 ± 0.0782 | 0.6813 ± 0.0776 | 0.5685 ± 0.0451 | 0.7188 ± 0.0625 |
   18 | 0.5799 ± 0.0414 | 0.6937 ± 0.0667 | 0.5939 ± 0.0665 | 0.6750 ± 0.0793 |
   19 | 0.5740 ± 0.0241 | 0.7063 ± 0.0545 | 0.5998 ± 0.0437 | 0.6844 ± 0.0493 |
   20 | 0.5994 ± 0.0375 | 0.6062 ± 0.0424 | 0.6705 ± 0.0797 | 0.5219 ± 0.0791 |
   21 | 0.6406 ± 0.0441 | 0.5625 ± 0.0342 | 0.6191 ± 0.0309 | 0.6344 ± 0.0839 |
   22 | 0.6187 ± 0.1200 | 0.6875 ± 0.1281 | 0.5619 ± 0.0587 | 0.7438 ± 0.0871 |
   23 | 0.6081 ± 0.0757 | 0.7000 ± 0.0940 | 0.6187 ± 0.0681 | 0.6875 ± 0.0699 |
   24 | 0.5858 ± 0.0521 | 0.7125 ± 0.0606 | 0.6006 ± 0.0516 | 0.6594 ± 0.0878 |
   25 | 0.6274 ± 0.0297 | 0.5875 ± 0.0364 | 0.6315 ± 0.0530 | 0.5969 ± 0.0844 |
   26 | 0.6427 ± 0.0281 | 0.6375 ± 0.0919 | 0.6150 ± 0.0592 | 0.6594 ± 0.0942 |
   27 | 0.6058 ± 0.0336 | 0.6750 ± 0.0375 | 0.6327 ± 0.0501 | 0.6281 ± 0.0632 |
   28 | 0.6263 ± 0.0887 | 0.6687 ± 0.0468 | 0.6477 ± 0.0647 | 0.6281 ± 0.0616 |
   29 | 0.5713 ± 0.0961 | 0.7125 ± 0.0824 | 0.6149 ± 0.0546 | 0.6781 ± 0.0610 |
   30 | 0.6102 ± 0.0472 | 0.6000 ± 0.0696 | 0.6177 ± 0.0896 | 0.6250 ± 0.0640 |
   31 | 0.6250 ± 0.0426 | 0.6875 ± 0.0395 | 0.5773 ± 0.0981 | 0.7031 ± 0.0950 |
   32 | 0.6396 ± 0.0665 | 0.6438 ± 0.0580 | 0.6122 ± 0.0474 | 0.6781 ± 0.0610 |
   33 | 0.5539 ± 0.0608 | 0.6937 ± 0.0935 | 0.6265 ± 0.0870 | 0.6375 ± 0.0980 |
   34 | 0.5814 ± 0.0477 | 0.7250 ± 0.0306 | 0.6015 ± 0.0932 | 0.7000 ± 0.0841 |
   35 | 0.6592 ± 0.0245 | 0.6438 ± 0.0424 | 0.5711 ± 0.0387 | 0.6969 ± 0.0560 |
   36 | 0.6084 ± 0.1175 | 0.6438 ± 0.0897 | 0.6384 ± 0.0594 | 0.6406 ± 0.1120 |
   37 | 0.6292 ± 0.0561 | 0.6625 ± 0.0606 | 0.6036 ± 0.0651 | 0.6937 ± 0.0925 |
   38 | 0.5785 ± 0.0794 | 0.6875 ± 0.0656 | 0.6330 ± 0.0552 | 0.6625 ± 0.0538 |
   39 | 0.6214 ± 0.0636 | 0.6625 ± 0.0848 | 0.6114 ± 0.0599 | 0.6656 ± 0.0815 |
   40 | 0.5510 ± 0.0326 | 0.7250 ± 0.0459 | 0.6376 ± 0.0573 | 0.6188 ± 0.0606 |
   41 | 0.6788 ± 0.0602 | 0.6312 ± 0.0500 | 0.6021 ± 0.0492 | 0.6219 ± 0.0820 |
   42 | 0.5890 ± 0.0549 | 0.6438 ± 0.0612 | 0.6256 ± 0.0698 | 0.6562 ± 0.0895 |
   43 | 0.6349 ± 0.0341 | 0.6438 ± 0.0424 | 0.5848 ± 0.0467 | 0.6813 ± 0.0750 |
   44 | 0.5518 ± 0.0462 | 0.7562 ± 0.0606 | 0.6122 ± 0.0691 | 0.6656 ± 0.0851 |
   45 | 0.6069 ± 0.0443 | 0.6813 ± 0.0776 | 0.5998 ± 0.0567 | 0.6719 ± 0.0756 |
   46 | 0.6338 ± 0.0230 | 0.6188 ± 0.0459 | 0.5988 ± 0.0524 | 0.6813 ± 0.0710 |
   47 | 0.5677 ± 0.0517 | 0.7375 ± 0.0673 | 0.5749 ± 0.0833 | 0.7219 ± 0.0844 |
   48 | 0.5973 ± 0.0614 | 0.7125 ± 0.0667 | 0.5984 ± 0.0692 | 0.6906 ± 0.0889 |
   49 | 0.6053 ± 0.0560 | 0.6875 ± 0.0342 | 0.6132 ± 0.0416 | 0.6469 ± 0.0862 |
   50 | 0.5464 ± 0.0437 | 0.7312 ± 0.0545 | 0.6321 ± 0.0742 | 0.6469 ± 0.0791 |
   51 | 0.5542 ± 0.0474 | 0.7312 ± 0.0424 | 0.6377 ± 0.0552 | 0.6344 ± 0.0610 |
   52 | 0.6482 ± 0.0918 | 0.6438 ± 0.0897 | 0.6008 ± 0.0450 | 0.6875 ± 0.0839 |
   53 | 0.7060 ± 0.1084 | 0.6375 ± 0.0940 | 0.6250 ± 0.1065 | 0.6781 ± 0.0928 |
   54 | 0.5728 ± 0.0416 | 0.7063 ± 0.0468 | 0.6301 ± 0.0741 | 0.6531 ± 0.0867 |
   55 | 0.5946 ± 0.0085 | 0.6813 ± 0.0306 | 0.5782 ± 0.0455 | 0.7562 ± 0.0667 |
   56 | 0.6220 ± 0.0254 | 0.6500 ± 0.0538 | 0.5825 ± 0.0438 | 0.6813 ± 0.0904 |
   57 | 0.5875 ± 0.0739 | 0.7250 ± 0.0723 | 0.5688 ± 0.0525 | 0.7031 ± 0.0730 |
   58 | 0.5451 ± 0.0732 | 0.7688 ± 0.0673 | 0.6301 ± 0.0931 | 0.6719 ± 0.0887 |
   59 | 0.6036 ± 0.0978 | 0.6875 ± 0.0927 | 0.6136 ± 0.0563 | 0.6469 ± 0.0766 |
   60 | 0.6316 ± 0.0649 | 0.6000 ± 0.0871 | 0.5951 ± 0.0737 | 0.6594 ± 0.1181 |
   61 | 0.5936 ± 0.0467 | 0.6937 ± 0.0415 | 0.5847 ± 0.0239 | 0.6906 ± 0.0295 |
   62 | 0.6286 ± 0.0604 | 0.6562 ± 0.0740 | 0.6320 ± 0.0502 | 0.6719 ± 0.0841 |
   63 | 0.5913 ± 0.0233 | 0.6625 ± 0.0637 | 0.5877 ± 0.0403 | 0.6875 ± 0.0640 |
   64 | 0.5905 ± 0.0839 | 0.6813 ± 0.1090 | 0.5828 ± 0.0900 | 0.7031 ± 0.0794 |
   65 | 0.6175 ± 0.0347 | 0.6500 ± 0.0914 | 0.6084 ± 0.0443 | 0.6750 ± 0.0643 |
   66 | 0.5980 ± 0.0320 | 0.7000 ± 0.0375 | 0.6263 ± 0.0722 | 0.6813 ± 0.0848 |
   67 | 0.5749 ± 0.0642 | 0.6687 ± 0.0702 | 0.5555 ± 0.0497 | 0.7344 ± 0.0613 |
   68 | 0.6008 ± 0.0382 | 0.6750 ± 0.0375 | 0.5978 ± 0.0820 | 0.6625 ± 0.1053 |
   69 | 0.6318 ± 0.0396 | 0.6500 ± 0.0415 | 0.5975 ± 0.0817 | 0.7125 ± 0.0914 |
   70 | 0.5872 ± 0.0505 | 0.6750 ± 0.0805 | 0.5899 ± 0.0544 | 0.7156 ± 0.0745 |
   71 | 0.5572 ± 0.0410 | 0.7063 ± 0.0508 | 0.6207 ± 0.0755 | 0.6687 ± 0.0805 |
   72 | 0.6882 ± 0.0714 | 0.6000 ± 0.0637 | 0.5897 ± 0.0634 | 0.7063 ± 0.0468 |
   73 | 0.5881 ± 0.0506 | 0.6937 ± 0.0637 | 0.5684 ± 0.0584 | 0.7063 ± 0.0596 |
   74 | 0.5853 ± 0.0625 | 0.7250 ± 0.0637 | 0.6184 ± 0.0735 | 0.6750 ± 0.0673 |
   75 | 0.6212 ± 0.0980 | 0.6625 ± 0.0776 | 0.6360 ± 0.0740 | 0.6469 ± 0.0803 |
   76 | 0.5819 ± 0.0981 | 0.6750 ± 0.0980 | 0.6303 ± 0.0763 | 0.6125 ± 0.0755 |
   77 | 0.6108 ± 0.0677 | 0.6312 ± 0.0935 | 0.6091 ± 0.0606 | 0.6562 ± 0.0726 |
   78 | 0.6459 ± 0.0256 | 0.5813 ± 0.0424 | 0.5751 ± 0.0630 | 0.6500 ± 0.0667 |
   79 | 0.6011 ± 0.0489 | 0.6687 ± 0.0673 | 0.6066 ± 0.0731 | 0.6781 ± 0.0803 |
   80 | 0.5623 ± 0.0781 | 0.7375 ± 0.1000 | 0.5688 ± 0.0453 | 0.7219 ± 0.0513 |
   81 | 0.6281 ± 0.0481 | 0.6500 ± 0.0364 | 0.5956 ± 0.0597 | 0.6906 ± 0.0844 |
   82 | 0.6007 ± 0.0451 | 0.7063 ± 0.0319 | 0.5813 ± 0.0707 | 0.7125 ± 0.0737 |
   83 | 0.5949 ± 0.0350 | 0.6875 ± 0.0442 | 0.5847 ± 0.0819 | 0.7125 ± 0.0480 |
   84 | 0.5867 ± 0.0435 | 0.7250 ± 0.0364 | 0.5857 ± 0.0644 | 0.6844 ± 0.0758 |
   85 | 0.6042 ± 0.0381 | 0.6562 ± 0.0395 | 0.5510 ± 0.0462 | 0.7362 ± 0.0686 |
   86 | 0.6106 ± 0.0227 | 0.6438 ± 0.0755 | 0.5955 ± 0.0506 | 0.6813 ± 0.0737 |
   87 | 0.6133 ± 0.1012 | 0.6875 ± 0.0656 | 0.6435 ± 0.0996 | 0.6625 ± 0.0813 |
   88 | 0.5682 ± 0.0468 | 0.6937 ± 0.0824 | 0.6209 ± 0.0862 | 0.6562 ± 0.1304 |
   89 | 0.5960 ± 0.0816 | 0.6875 ± 0.0948 | 0.6008 ± 0.0750 | 0.6781 ± 0.0839 |
   90 | 0.6270 ± 0.0742 | 0.6562 ± 0.0948 | 0.5759 ± 0.0528 | 0.7219 ± 0.0647 |
   91 | 0.5364 ± 0.0358 | 0.7500 ± 0.0442 | 0.6248 ± 0.0689 | 0.6438 ± 0.0658 |
   92 | 0.5518 ± 0.0889 | 0.7000 ± 0.0940 | 0.5797 ± 0.0589 | 0.7219 ± 0.0732 |
   93 | 0.6119 ± 0.0583 | 0.6937 ± 0.0606 | 0.5845 ± 0.0408 | 0.6719 ± 0.0876 |
   94 | 0.6043 ± 0.0444 | 0.6500 ± 0.0606 | 0.6109 ± 0.0520 | 0.6781 ± 0.0815 |
   95 | 0.6024 ± 0.0411 | 0.7250 ± 0.0306 | 0.5967 ± 0.0814 | 0.6906 ± 0.0584 |
   96 | 0.5814 ± 0.0597 | 0.7063 ± 0.0508 | 0.5899 ± 0.0616 | 0.6750 ± 0.0755 |
   97 | 0.5389 ± 0.0535 | 0.7625 ± 0.0612 | 0.6034 ± 0.0543 | 0.6875 ± 0.0484 |
   98 | 0.6560 ± 0.0509 | 0.6500 ± 0.0637 | 0.5568 ± 0.0611 | 0.7250 ± 0.0996 |
   99 | 0.5991 ± 0.0378 | 0.6687 ± 0.0673 | 0.5905 ± 0.0743 | 0.6844 ± 0.0662 |
  100 | 0.6217 ± 0.0543 | 0.6813 ± 0.0723 | 0.6018 ± 0.0414 | 0.6500 ± 0.0776 |
  101 | 0.6020 ± 0.0501 | 0.6875 ± 0.0523 | 0.5753 ± 0.0378 | 0.7219 ± 0.0662 |
  102 | 0.6000 ± 0.0828 | 0.6937 ± 0.0606 | 0.5468 ± 0.0792 | 0.7406 ± 0.0884 |
  103 | 0.6090 ± 0.0910 | 0.6937 ± 0.0824 | 0.5904 ± 0.0682 | 0.6875 ± 0.0699 |
  104 | 0.5875 ± 0.0348 | 0.6813 ± 0.0234 | 0.5737 ± 0.0446 | 0.7031 ± 0.0688 |
  105 | 0.5388 ± 0.0482 | 0.7063 ± 0.0673 | 0.6052 ± 0.0654 | 0.6656 ± 0.0839 |
  106 | 0.6172 ± 0.0592 | 0.6813 ± 0.0976 | 0.5970 ± 0.0483 | 0.6781 ± 0.0594 |
  107 | 0.6117 ± 0.0851 | 0.6937 ± 0.0723 | 0.5686 ± 0.0575 | 0.7188 ± 0.0685 |
  108 | 0.6081 ± 0.0423 | 0.6750 ± 0.0250 | 0.6080 ± 0.0592 | 0.7031 ± 0.0674 |
  109 | 0.6751 ± 0.0695 | 0.5938 ± 0.0815 | 0.5786 ± 0.0585 | 0.6594 ± 0.0973 |
  110 | 0.5402 ± 0.0115 | 0.7562 ± 0.0415 | 0.6221 ± 0.0659 | 0.6937 ± 0.0848 |
  111 | 0.5495 ± 0.0550 | 0.7937 ± 0.0643 | 0.6218 ± 0.1186 | 0.6687 ± 0.1171 |
  112 | 0.5577 ± 0.0544 | 0.7188 ± 0.0713 | 0.6467 ± 0.0374 | 0.6562 ± 0.0395 |
  113 | 0.6074 ± 0.0627 | 0.5938 ± 0.0656 | 0.6416 ± 0.0512 | 0.5969 ± 0.0677 |
  114 | 0.6025 ± 0.0826 | 0.6500 ± 0.0935 | 0.6004 ± 0.0728 | 0.6250 ± 0.1055 |
  115 | 0.5585 ± 0.0479 | 0.7063 ± 0.0468 | 0.5817 ± 0.1181 | 0.6813 ± 0.1248 |
  116 | 0.6122 ± 0.0877 | 0.6750 ± 0.1163 | 0.5742 ± 0.0788 | 0.6906 ± 0.0705 |
  117 | 0.6480 ± 0.0484 | 0.6250 ± 0.0395 | 0.6086 ± 0.0381 | 0.6813 ± 0.0788 |
  118 | 0.6308 ± 0.0252 | 0.6000 ± 0.0637 | 0.6233 ± 0.0705 | 0.6594 ± 0.0889 |
  119 | 0.5744 ± 0.0219 | 0.7000 ± 0.0319 | 0.5937 ± 0.0705 | 0.7031 ± 0.0769 |
  120 | 0.5641 ± 0.0565 | 0.7375 ± 0.0829 | 0.5732 ± 0.0260 | 0.7125 ± 0.0390 |
  121 | 0.5083 ± 0.0610 | 0.7875 ± 0.0459 | 0.6449 ± 0.1185 | 0.6500 ± 0.0914 |
  122 | 0.5612 ± 0.0876 | 0.7250 ± 0.0573 | 0.6058 ± 0.0636 | 0.7063 ± 0.0742 |
  123 | 0.5893 ± 0.0677 | 0.6438 ± 0.0580 | 0.6074 ± 0.0614 | 0.6719 ± 0.0688 |
  124 | 0.5590 ± 0.0769 | 0.7188 ± 0.0593 | 0.6108 ± 0.0827 | 0.6562 ± 0.0839 |
  125 | 0.6505 ± 0.0668 | 0.6438 ± 0.0643 | 0.6383 ± 0.0605 | 0.6219 ± 0.0784 |
  126 | 0.6076 ± 0.0454 | 0.6750 ± 0.0643 | 0.6134 ± 0.0398 | 0.6375 ± 0.0628 |
  127 | 0.6392 ± 0.0578 | 0.6125 ± 0.0729 | 0.6129 ± 0.0432 | 0.6625 ± 0.0606 |
  128 | 0.5791 ± 0.0582 | 0.6750 ± 0.0424 | 0.5622 ± 0.0554 | 0.6656 ± 0.0862 |
  129 | 0.5675 ± 0.0796 | 0.7312 ± 0.1057 | 0.6085 ± 0.0946 | 0.7031 ± 0.0887 |
  130 | 0.5874 ± 0.0477 | 0.6562 ± 0.0791 | 0.6027 ± 0.0719 | 0.6594 ± 0.0973 |
  131 | 0.5866 ± 0.0469 | 0.6875 ± 0.0839 | 0.5722 ± 0.0768 | 0.7156 ± 0.0900 |
  132 | 0.5571 ± 0.0291 | 0.7125 ± 0.0667 | 0.5701 ± 0.0485 | 0.7000 ± 0.0658 |
  133 | 0.5708 ± 0.0333 | 0.6813 ± 0.0573 | 0.6040 ± 0.0599 | 0.6844 ± 0.0808 |
  134 | 0.6087 ± 0.0815 | 0.5875 ± 0.0776 | 0.6124 ± 0.0714 | 0.6312 ± 0.0800 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6628 ± 0.0258 | 0.6438 ± 0.0702 | 0.6529 ± 0.0361 | 0.6188 ± 0.1256 |
    2 | 0.6029 ± 0.0832 | 0.6875 ± 0.1100 | 0.6244 ± 0.0782 | 0.6625 ± 0.0836 |
    3 | 0.5948 ± 0.0233 | 0.6312 ± 0.0364 | 0.5784 ± 0.0509 | 0.6656 ± 0.0505 |
    4 | 0.6128 ± 0.0610 | 0.7250 ± 0.0667 | 0.5910 ± 0.0563 | 0.7125 ± 0.0737 |
    5 | 0.6099 ± 0.0620 | 0.6813 ± 0.1159 | 0.5894 ± 0.0406 | 0.7125 ± 0.0925 |
    6 | 0.6061 ± 0.0416 | 0.6937 ± 0.0667 | 0.6306 ± 0.0567 | 0.6312 ± 0.0737 |
    7 | 0.6309 ± 0.0650 | 0.6625 ± 0.1176 | 0.6201 ± 0.0890 | 0.6406 ± 0.1048 |
    8 | 0.6138 ± 0.0313 | 0.7000 ± 0.0673 | 0.6001 ± 0.0444 | 0.6469 ± 0.0815 |
    9 | 0.5854 ± 0.0389 | 0.6875 ± 0.0713 | 0.5973 ± 0.0915 | 0.7063 ± 0.0950 |
   10 | 0.6170 ± 0.0388 | 0.6375 ± 0.0580 | 0.6025 ± 0.0475 | 0.7063 ± 0.0580 |
   11 | 0.5921 ± 0.0337 | 0.7250 ± 0.0606 | 0.5722 ± 0.0585 | 0.7438 ± 0.0750 |
   12 | 0.6719 ± 0.0718 | 0.6438 ± 0.1111 | 0.6341 ± 0.0568 | 0.6375 ± 0.0658 |
   13 | 0.5655 ± 0.0794 | 0.7063 ± 0.0805 | 0.6377 ± 0.0857 | 0.6813 ± 0.0637 |
   14 | 0.5966 ± 0.0608 | 0.7125 ± 0.0723 | 0.5799 ± 0.0664 | 0.7188 ± 0.0862 |
   15 | 0.5824 ± 0.0202 | 0.7125 ± 0.0306 | 0.6115 ± 0.0426 | 0.6937 ± 0.0538 |
   16 | 0.6021 ± 0.0475 | 0.6875 ± 0.0884 | 0.6114 ± 0.0641 | 0.6375 ± 0.0919 |
   17 | 0.5278 ± 0.0520 | 0.7063 ± 0.0673 | 0.5903 ± 0.0619 | 0.6906 ± 0.0921 |
   18 | 0.6165 ± 0.0694 | 0.6687 ± 0.1228 | 0.6072 ± 0.0694 | 0.6656 ± 0.0938 |
   19 | 0.6012 ± 0.0307 | 0.6750 ± 0.0729 | 0.6551 ± 0.0639 | 0.5656 ± 0.0796 |
   20 | 0.6532 ± 0.0608 | 0.6375 ± 0.0545 | 0.6325 ± 0.0409 | 0.6469 ± 0.0420 |
   21 | 0.6001 ± 0.0509 | 0.7000 ± 0.0673 | 0.6297 ± 0.0500 | 0.6469 ± 0.0791 |
   22 | 0.5993 ± 0.0540 | 0.6813 ± 0.0364 | 0.6227 ± 0.0855 | 0.6500 ± 0.0956 |
   23 | 0.6507 ± 0.0701 | 0.6375 ± 0.0805 | 0.5610 ± 0.0340 | 0.7469 ± 0.0616 |
   24 | 0.5893 ± 0.0425 | 0.7125 ± 0.0606 | 0.5726 ± 0.0669 | 0.7063 ± 0.0960 |
   25 | 0.6164 ± 0.0378 | 0.6750 ± 0.0702 | 0.6278 ± 0.0510 | 0.6281 ± 0.0932 |
   26 | 0.6000 ± 0.0316 | 0.7000 ± 0.0424 | 0.6056 ± 0.0636 | 0.6438 ± 0.0715 |
   27 | 0.6214 ± 0.0660 | 0.6500 ± 0.0606 | 0.5940 ± 0.0565 | 0.6937 ± 0.0776 |
   28 | 0.5874 ± 0.0778 | 0.7250 ± 0.0914 | 0.6099 ± 0.1083 | 0.6969 ± 0.0740 |
   29 | 0.6002 ± 0.0627 | 0.6875 ± 0.0484 | 0.5771 ± 0.0305 | 0.7094 ± 0.0397 |
   30 | 0.5721 ± 0.0508 | 0.6750 ± 0.1093 | 0.6024 ± 0.0662 | 0.6719 ± 0.0818 |
   31 | 0.6226 ± 0.0446 | 0.6438 ± 0.0781 | 0.6080 ± 0.0402 | 0.6594 ± 0.0493 |
   32 | 0.6143 ± 0.0431 | 0.6813 ± 0.0573 | 0.5853 ± 0.0742 | 0.6906 ± 0.0900 |
   33 | 0.6391 ± 0.0585 | 0.6937 ± 0.0606 | 0.6169 ± 0.0465 | 0.6844 ± 0.0549 |
   34 | 0.6466 ± 0.0760 | 0.6687 ± 0.0729 | 0.6048 ± 0.0486 | 0.7031 ± 0.0756 |
   35 | 0.5938 ± 0.0702 | 0.6438 ± 0.0580 | 0.6299 ± 0.0698 | 0.6469 ± 0.0713 |
   36 | 0.6130 ± 0.0227 | 0.6562 ± 0.0442 | 0.5951 ± 0.0675 | 0.6813 ± 0.0914 |
   37 | 0.6135 ± 0.0633 | 0.6813 ± 0.0606 | 0.5771 ± 0.0952 | 0.6687 ± 0.1320 |
   38 | 0.6640 ± 0.0812 | 0.6250 ± 0.0862 | 0.5738 ± 0.0639 | 0.7063 ± 0.0702 |
   39 | 0.5926 ± 0.0467 | 0.7000 ± 0.0755 | 0.5938 ± 0.0797 | 0.6750 ± 0.0768 |
   40 | 0.5575 ± 0.0370 | 0.7438 ± 0.0306 | 0.6326 ± 0.0659 | 0.6406 ± 0.0644 |
   41 | 0.5509 ± 0.1177 | 0.7250 ± 0.1108 | 0.6952 ± 0.0782 | 0.6094 ± 0.1039 |
   42 | 0.6092 ± 0.0227 | 0.6875 ± 0.0280 | 0.6061 ± 0.0684 | 0.6687 ± 0.0919 |
   43 | 0.6023 ± 0.0498 | 0.6375 ± 0.1075 | 0.5862 ± 0.0542 | 0.6656 ± 0.0713 |
   44 | 0.6043 ± 0.0658 | 0.6625 ± 0.0500 | 0.5956 ± 0.0610 | 0.6875 ± 0.0740 |
   45 | 0.5579 ± 0.0734 | 0.6875 ± 0.0815 | 0.5596 ± 0.0724 | 0.6438 ± 0.0612 |
   46 | 0.6428 ± 0.0619 | 0.6562 ± 0.0765 | 0.5994 ± 0.0621 | 0.6531 ± 0.1022 |
   47 | 0.5699 ± 0.0835 | 0.7312 ± 0.1163 | 0.5711 ± 0.0580 | 0.7094 ± 0.0626 |
   48 | 0.5847 ± 0.0469 | 0.6562 ± 0.0815 | 0.5861 ± 0.0604 | 0.6656 ± 0.0740 |
   49 | 0.5753 ± 0.0554 | 0.7188 ± 0.0862 | 0.5902 ± 0.0446 | 0.6781 ± 0.0465 |
   50 | 0.5982 ± 0.0688 | 0.6375 ± 0.1038 | 0.5941 ± 0.0502 | 0.6906 ± 0.0567 |
   51 | 0.5600 ± 0.0733 | 0.7312 ± 0.0960 | 0.5890 ± 0.0550 | 0.7156 ± 0.0473 |
   52 | 0.5588 ± 0.0388 | 0.7000 ± 0.0643 | 0.6093 ± 0.0306 | 0.6813 ± 0.0459 |
   53 | 0.6066 ± 0.0644 | 0.6438 ± 0.0508 | 0.5896 ± 0.0543 | 0.6813 ± 0.0573 |
   54 | 0.5370 ± 0.0538 | 0.7375 ± 0.0612 | 0.5933 ± 0.1153 | 0.6937 ± 0.0723 |
   55 | 0.6089 ± 0.0468 | 0.6500 ± 0.0800 | 0.5856 ± 0.0827 | 0.7125 ± 0.0904 |
   56 | 0.5887 ± 0.0585 | 0.7063 ± 0.0508 | 0.5980 ± 0.0706 | 0.6719 ± 0.0688 |
   57 | 0.6422 ± 0.0867 | 0.6250 ± 0.1383 | 0.5861 ± 0.0502 | 0.7219 ± 0.0677 |
   58 | 0.5963 ± 0.0495 | 0.7000 ± 0.0508 | 0.6142 ± 0.0702 | 0.6656 ± 0.0928 |
   59 | 0.5972 ± 0.0674 | 0.6750 ± 0.0852 | 0.6089 ± 0.0461 | 0.6844 ± 0.0691 |
   60 | 0.5602 ± 0.0575 | 0.7188 ± 0.0791 | 0.6169 ± 0.0512 | 0.6719 ± 0.0469 |
   61 | 0.5823 ± 0.0785 | 0.6937 ± 0.0824 | 0.5894 ± 0.0443 | 0.7063 ± 0.0508 |
   62 | 0.6209 ± 0.0633 | 0.6438 ± 0.0424 | 0.6082 ± 0.0784 | 0.6813 ± 0.1072 |
   63 | 0.6243 ± 0.0866 | 0.6562 ± 0.0988 | 0.5988 ± 0.0528 | 0.6844 ± 0.0567 |
   64 | 0.6190 ± 0.0810 | 0.6562 ± 0.0685 | 0.6157 ± 0.0857 | 0.6719 ± 0.0853 |
   65 | 0.6058 ± 0.0245 | 0.6625 ± 0.0696 | 0.6263 ± 0.0559 | 0.6562 ± 0.0670 |
   66 | 0.6361 ± 0.0328 | 0.6750 ± 0.0673 | 0.6222 ± 0.0470 | 0.6281 ± 0.0771 |
   67 | 0.5965 ± 0.0372 | 0.6375 ± 0.0424 | 0.6033 ± 0.0526 | 0.6500 ± 0.0848 |
   68 | 0.5925 ± 0.0591 | 0.7000 ± 0.0612 | 0.6300 ± 0.0559 | 0.6719 ± 0.0908 |
   69 | 0.6053 ± 0.0910 | 0.7125 ± 0.0956 | 0.6127 ± 0.0588 | 0.6219 ± 0.0632 |
   70 | 0.6360 ± 0.0566 | 0.6375 ± 0.0643 | 0.6160 ± 0.0590 | 0.6562 ± 0.0685 |
   71 | 0.5697 ± 0.0895 | 0.6875 ± 0.0815 | 0.6312 ± 0.0799 | 0.6469 ± 0.1027 |
   72 | 0.5663 ± 0.0598 | 0.7188 ± 0.0523 | 0.6092 ± 0.0483 | 0.6656 ± 0.0577 |
   73 | 0.5865 ± 0.0543 | 0.7188 ± 0.0839 | 0.5828 ± 0.0434 | 0.6844 ± 0.0844 |
   74 | 0.5898 ± 0.0717 | 0.6687 ± 0.1057 | 0.6338 ± 0.0706 | 0.6438 ± 0.0488 |
   75 | 0.6020 ± 0.0643 | 0.6687 ± 0.0319 | 0.5668 ± 0.0597 | 0.7312 ± 0.0508 |
   76 | 0.6419 ± 0.1019 | 0.6750 ± 0.0729 | 0.5901 ± 0.0621 | 0.7000 ± 0.0702 |
   77 | 0.6480 ± 0.0333 | 0.6562 ± 0.0559 | 0.5558 ± 0.0338 | 0.7188 ± 0.0609 |
   78 | 0.5884 ± 0.0246 | 0.7000 ± 0.0729 | 0.5885 ± 0.0324 | 0.6625 ± 0.0637 |
   79 | 0.6242 ± 0.0728 | 0.6250 ± 0.1100 | 0.5908 ± 0.0414 | 0.7188 ± 0.0523 |
   80 | 0.5705 ± 0.0573 | 0.7188 ± 0.0523 | 0.6225 ± 0.0809 | 0.6813 ± 0.0966 |
   81 | 0.6191 ± 0.0867 | 0.6687 ± 0.0852 | 0.5873 ± 0.0430 | 0.7188 ± 0.0699 |
   82 | 0.6790 ± 0.0547 | 0.6562 ± 0.0593 | 0.6130 ± 0.0460 | 0.6844 ± 0.0691 |
   83 | 0.5598 ± 0.0942 | 0.7188 ± 0.0927 | 0.6225 ± 0.0876 | 0.6625 ± 0.1053 |
   84 | 0.6098 ± 0.0497 | 0.6750 ± 0.0875 | 0.5855 ± 0.0389 | 0.6813 ± 0.0480 |
   85 | 0.5624 ± 0.0451 | 0.6750 ± 0.0580 | 0.6105 ± 0.0485 | 0.6643 ± 0.0803 |
   86 | 0.6329 ± 0.0981 | 0.6312 ± 0.0848 | 0.5603 ± 0.0699 | 0.6906 ± 0.0452 |
   87 | 0.5421 ± 0.0877 | 0.7188 ± 0.0765 | 0.5717 ± 0.0850 | 0.7125 ± 0.1006 |
   88 | 0.5676 ± 0.0507 | 0.7125 ± 0.0415 | 0.5927 ± 0.0526 | 0.6906 ± 0.0719 |
   89 | 0.6416 ± 0.0988 | 0.5062 ± 0.1272 | 0.5589 ± 0.0416 | 0.7063 ± 0.0950 |
   90 | 0.6560 ± 0.0859 | 0.6000 ± 0.1125 | 0.5800 ± 0.0536 | 0.6906 ± 0.1087 |
   91 | 0.5561 ± 0.0603 | 0.7250 ± 0.0776 | 0.5837 ± 0.0598 | 0.7031 ± 0.0509 |
   92 | 0.5894 ± 0.0732 | 0.7375 ± 0.0545 | 0.6433 ± 0.0798 | 0.6219 ± 0.0963 |
   93 | 0.5661 ± 0.0832 | 0.7250 ± 0.0914 | 0.5505 ± 0.0420 | 0.7344 ± 0.0546 |
   94 | 0.5688 ± 0.0434 | 0.6625 ± 0.0606 | 0.6127 ± 0.0706 | 0.6750 ± 0.0919 |
   95 | 0.6310 ± 0.0546 | 0.6687 ± 0.0729 | 0.5916 ± 0.0444 | 0.6969 ± 0.0560 |
   96 | 0.5514 ± 0.0376 | 0.7250 ± 0.0606 | 0.5751 ± 0.0898 | 0.7312 ± 0.0829 |
   97 | 0.5736 ± 0.0603 | 0.6687 ± 0.0612 | 0.6056 ± 0.0936 | 0.6531 ± 0.1113 |
   98 | 0.5831 ± 0.0447 | 0.7250 ± 0.0573 | 0.6072 ± 0.0615 | 0.6781 ± 0.0699 |
   99 | 0.6284 ± 0.0884 | 0.6375 ± 0.1244 | 0.5488 ± 0.0632 | 0.7500 ± 0.0827 |
  100 | 0.6162 ± 0.0703 | 0.6562 ± 0.0656 | 0.5835 ± 0.0524 | 0.6844 ± 0.0662 |
  101 | 0.6443 ± 0.0586 | 0.6438 ± 0.0468 | 0.5939 ± 0.0593 | 0.6750 ± 0.0829 |
  102 | 0.6011 ± 0.0479 | 0.6687 ± 0.0424 | 0.5757 ± 0.0522 | 0.7344 ± 0.0940 |
  103 | 0.5766 ± 0.0544 | 0.6937 ± 0.0667 | 0.5959 ± 0.0787 | 0.6656 ± 0.1178 |
  104 | 0.5761 ± 0.1151 | 0.6875 ± 0.0765 | 0.6714 ± 0.0712 | 0.6562 ± 0.0625 |
  105 | 0.6027 ± 0.0583 | 0.6562 ± 0.0656 | 0.6247 ± 0.0423 | 0.6562 ± 0.0541 |
  106 | 0.6381 ± 0.0894 | 0.6500 ± 0.0573 | 0.6013 ± 0.0782 | 0.6625 ± 0.1006 |
  107 | 0.5716 ± 0.0565 | 0.7188 ± 0.0484 | 0.6034 ± 0.0921 | 0.6781 ± 0.0895 |
  108 | 0.6378 ± 0.0211 | 0.6125 ± 0.0153 | 0.6052 ± 0.0630 | 0.6500 ± 0.0519 |
  109 | 0.5486 ± 0.0332 | 0.7625 ± 0.0468 | 0.5402 ± 0.0865 | 0.7312 ± 0.0643 |
  110 | 0.5378 ± 0.0787 | 0.7625 ± 0.0643 | 0.6753 ± 0.0901 | 0.6375 ± 0.0980 |
  111 | 0.6026 ± 0.0550 | 0.6438 ± 0.1111 | 0.6043 ± 0.0497 | 0.6562 ± 0.0640 |
  112 | 0.5869 ± 0.0479 | 0.6750 ± 0.0729 | 0.5793 ± 0.0601 | 0.7094 ± 0.0524 |
  113 | 0.5977 ± 0.0229 | 0.7063 ± 0.0468 | 0.6034 ± 0.0842 | 0.7031 ± 0.0743 |
  114 | 0.5949 ± 0.0643 | 0.6875 ± 0.0559 | 0.5714 ± 0.0474 | 0.6969 ± 0.0656 |
  115 | 0.6015 ± 0.0691 | 0.6875 ± 0.0395 | 0.6140 ± 0.0549 | 0.6937 ± 0.0882 |
  116 | 0.6105 ± 0.0736 | 0.7063 ± 0.0729 | 0.6027 ± 0.0516 | 0.6906 ± 0.0513 |
  117 | 0.5960 ± 0.0510 | 0.6500 ± 0.0824 | 0.5915 ± 0.0788 | 0.6906 ± 0.1096 |
  118 | 0.6218 ± 0.0545 | 0.6500 ± 0.0776 | 0.6103 ± 0.0412 | 0.6594 ± 0.0705 |
  119 | 0.5724 ± 0.0582 | 0.6937 ± 0.0976 | 0.5969 ± 0.0533 | 0.7156 ± 0.0584 |
  120 | 0.5951 ± 0.0433 | 0.6813 ± 0.0364 | 0.6192 ± 0.0614 | 0.6719 ± 0.0702 |
  121 | 0.5758 ± 0.0669 | 0.6937 ± 0.0306 | 0.5785 ± 0.0591 | 0.6906 ± 0.0771 |
  122 | 0.6056 ± 0.0543 | 0.6875 ± 0.0740 | 0.5486 ± 0.0730 | 0.7344 ± 0.0961 |
  123 | 0.5703 ± 0.0345 | 0.6813 ± 0.1108 | 0.6004 ± 0.0597 | 0.6750 ± 0.0829 |
  124 | 0.6032 ± 0.0942 | 0.6687 ± 0.0897 | 0.5819 ± 0.0915 | 0.6875 ± 0.0791 |
  125 | 0.5834 ± 0.0712 | 0.6687 ± 0.0852 | 0.6236 ± 0.0648 | 0.6438 ± 0.0527 |
  126 | 0.5885 ± 0.0612 | 0.7000 ± 0.0612 | 0.6041 ± 0.0518 | 0.6531 ± 0.1131 |
  127 | 0.5720 ± 0.0529 | 0.7000 ± 0.0805 | 0.6248 ± 0.0564 | 0.6562 ± 0.0850 |
  128 | 0.5909 ± 0.0828 | 0.6750 ± 0.1038 | 0.5676 ± 0.0615 | 0.7281 ± 0.0485 |
  129 | 0.5427 ± 0.0638 | 0.7562 ± 0.0606 | 0.6009 ± 0.0879 | 0.6906 ± 0.0844 |
  130 | 0.6617 ± 0.0753 | 0.6312 ± 0.0667 | 0.5683 ± 0.0681 | 0.6969 ± 0.0766 |
  131 | 0.5989 ± 0.0993 | 0.7000 ± 0.1179 | 0.6115 ± 0.0950 | 0.6625 ± 0.1256 |
  132 | 0.6005 ± 0.0794 | 0.6625 ± 0.1209 | 0.5648 ± 0.0401 | 0.7000 ± 0.0580 |
  133 | 0.5586 ± 0.0804 | 0.7250 ± 0.0956 | 0.5788 ± 0.0621 | 0.6781 ± 0.0671 |
  134 | 0.6083 ± 0.0283 | 0.6937 ± 0.0306 | 0.5905 ± 0.0489 | 0.7312 ± 0.0562 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6534 ± 0.0844 | 0.6125 ± 0.1259 | 0.8023 ± 0.1037 | 0.5406 ± 0.0753 |
    2 | 0.7128 ± 0.0602 | 0.5750 ± 0.1075 | 0.6668 ± 0.0302 | 0.5844 ± 0.0928 |
    3 | 0.7143 ± 0.0599 | 0.5500 ± 0.0940 | 0.6909 ± 0.0598 | 0.4656 ± 0.1165 |
    4 | 0.6714 ± 0.0420 | 0.6000 ± 0.0500 | 0.6430 ± 0.0290 | 0.6094 ± 0.0730 |
    5 | 0.6270 ± 0.0700 | 0.6500 ± 0.0914 | 0.6260 ± 0.0652 | 0.6625 ± 0.0737 |
    6 | 0.6054 ± 0.0650 | 0.6813 ± 0.1159 | 0.6315 ± 0.0984 | 0.6000 ± 0.1081 |
    7 | 0.6415 ± 0.0948 | 0.6312 ± 0.1389 | 0.6140 ± 0.0592 | 0.6719 ± 0.0876 |
    8 | 0.6032 ± 0.0145 | 0.6438 ± 0.0468 | 0.6081 ± 0.0701 | 0.6031 ± 0.0685 |
    9 | 0.6364 ± 0.0313 | 0.6500 ± 0.0364 | 0.6169 ± 0.0188 | 0.7000 ± 0.0319 |
   10 | 0.6069 ± 0.0442 | 0.6438 ± 0.1196 | 0.6259 ± 0.0240 | 0.6656 ± 0.0505 |
   11 | 0.6497 ± 0.0629 | 0.6062 ± 0.1000 | 0.6382 ± 0.0425 | 0.6406 ± 0.0898 |
   12 | 0.5862 ± 0.0843 | 0.7063 ± 0.0852 | 0.6006 ± 0.0556 | 0.7000 ± 0.0715 |
   13 | 0.6138 ± 0.0445 | 0.6687 ± 0.0702 | 0.6223 ± 0.0594 | 0.6750 ± 0.0563 |
   14 | 0.5811 ± 0.0458 | 0.7063 ± 0.0643 | 0.6200 ± 0.0787 | 0.6813 ± 0.0800 |
   15 | 0.6232 ± 0.0459 | 0.6500 ± 0.0500 | 0.5917 ± 0.0732 | 0.7094 ± 0.0884 |
   16 | 0.6451 ± 0.0872 | 0.6625 ± 0.0800 | 0.6029 ± 0.0705 | 0.6844 ± 0.0719 |
   17 | 0.6691 ± 0.0358 | 0.5813 ± 0.0755 | 0.6308 ± 0.0351 | 0.6312 ± 0.0710 |
   18 | 0.5966 ± 0.0800 | 0.6375 ± 0.1075 | 0.5862 ± 0.0281 | 0.6687 ± 0.0673 |
   19 | 0.6349 ± 0.0321 | 0.6062 ± 0.0805 | 0.6336 ± 0.0609 | 0.6562 ± 0.1008 |
   20 | 0.6029 ± 0.0394 | 0.6562 ± 0.0523 | 0.6263 ± 0.0504 | 0.6562 ± 0.0656 |
   21 | 0.6038 ± 0.0318 | 0.6625 ± 0.0415 | 0.6321 ± 0.0521 | 0.6531 ± 0.0531 |
   22 | 0.6058 ± 0.0860 | 0.6875 ± 0.0713 | 0.5954 ± 0.0477 | 0.7000 ± 0.0400 |
   23 | 0.5653 ± 0.0495 | 0.7375 ± 0.0612 | 0.5795 ± 0.0538 | 0.7000 ± 0.0628 |
   24 | 0.6252 ± 0.0405 | 0.6438 ± 0.0702 | 0.6461 ± 0.0456 | 0.6188 ± 0.0848 |
   25 | 0.6236 ± 0.0426 | 0.6625 ± 0.0415 | 0.6462 ± 0.0630 | 0.6562 ± 0.1100 |
   26 | 0.6093 ± 0.0325 | 0.6500 ± 0.0415 | 0.6302 ± 0.0704 | 0.6406 ± 0.0930 |
   27 | 0.6142 ± 0.0309 | 0.6687 ± 0.0468 | 0.6445 ± 0.0887 | 0.6312 ± 0.0925 |
   28 | 0.6265 ± 0.0169 | 0.6375 ± 0.0250 | 0.5936 ± 0.0662 | 0.6906 ± 0.0911 |
   29 | 0.5882 ± 0.0548 | 0.7312 ± 0.0781 | 0.5925 ± 0.0437 | 0.6906 ± 0.0900 |
   30 | 0.5922 ± 0.0586 | 0.7250 ± 0.1035 | 0.6117 ± 0.0533 | 0.6500 ± 0.0606 |
   31 | 0.5448 ± 0.0638 | 0.7125 ± 0.0996 | 0.6127 ± 0.0769 | 0.6750 ± 0.0886 |
   32 | 0.6901 ± 0.1366 | 0.5687 ± 0.1225 | 0.6116 ± 0.0431 | 0.6687 ± 0.0628 |
   33 | 0.6402 ± 0.0476 | 0.6625 ± 0.0459 | 0.6008 ± 0.0640 | 0.6844 ± 0.0796 |
   34 | 0.6345 ± 0.0697 | 0.6125 ± 0.0580 | 0.6108 ± 0.0730 | 0.6531 ± 0.1096 |
   35 | 0.6243 ± 0.0579 | 0.6937 ± 0.0538 | 0.6119 ± 0.0621 | 0.6719 ± 0.1076 |
   36 | 0.6381 ± 0.0560 | 0.6250 ± 0.0862 | 0.6235 ± 0.0464 | 0.6844 ± 0.0732 |
   37 | 0.9411 ± 0.1781 | 0.4562 ± 0.0702 | 0.7046 ± 0.0233 | 0.5000 ± 0.0765 |
   38 | 0.6988 ± 0.0162 | 0.5750 ± 0.1057 | 0.6822 ± 0.0124 | 0.5875 ± 0.0590 |
   39 | 0.8871 ± 0.0476 | 0.4750 ± 0.0776 | 1.2097 ± 0.1916 | 0.4531 ± 0.0971 |
   40 | 0.9072 ± 0.1355 | 0.4688 ± 0.0656 | 0.7310 ± 0.0492 | 0.4500 ± 0.1179 |
   41 | 0.7075 ± 0.1162 | 0.5813 ± 0.0852 | 0.6660 ± 0.0312 | 0.6344 ± 0.0505 |
   42 | 0.7080 ± 0.0212 | 0.4813 ± 0.0508 | 0.7383 ± 0.0248 | 0.4469 ± 0.0577 |
   43 | 0.8373 ± 0.0934 | 0.5062 ± 0.0893 | 0.9374 ± 0.0685 | 0.4938 ± 0.0480 |
   44 | 0.8073 ± 0.1143 | 0.5125 ± 0.1019 | 0.7761 ± 0.0693 | 0.4875 ± 0.0919 |
   45 | 0.7167 ± 0.0372 | 0.5000 ± 0.0713 | 0.7890 ± 0.0774 | 0.4906 ± 0.0895 |
   46 | 0.7710 ± 0.1492 | 0.5875 ± 0.0996 | 0.7800 ± 0.0620 | 0.5312 ± 0.0625 |
   47 | 0.8693 ± 0.1597 | 0.4688 ± 0.0815 | 0.8199 ± 0.0809 | 0.4781 ± 0.0862 |
   48 | 0.7157 ± 0.0395 | 0.4813 ± 0.1075 | 0.7144 ± 0.0272 | 0.4688 ± 0.0884 |
   49 | 0.7836 ± 0.1011 | 0.4688 ± 0.0815 | 0.7608 ± 0.0575 | 0.4625 ± 0.0925 |
   50 | 0.7194 ± 0.0498 | 0.5062 ± 0.0996 | 0.7319 ± 0.0281 | 0.3969 ± 0.0753 |
   51 | 0.8054 ± 0.0836 | 0.4375 ± 0.0625 | 0.6947 ± 0.0056 | 0.4781 ± 0.1046 |
   52 | 0.7501 ± 0.0679 | 0.5563 ± 0.0573 | 0.8189 ± 0.1319 | 0.5156 ± 0.1204 |
   53 | 0.8532 ± 0.1917 | 0.4250 ± 0.1259 | 0.7277 ± 0.0461 | 0.4969 ± 0.0878 |
   54 | 0.7588 ± 0.0601 | 0.4938 ± 0.0956 | 0.7143 ± 0.0249 | 0.4844 ± 0.0702 |
   55 | 0.7310 ± 0.0571 | 0.4938 ± 0.0848 | 0.6971 ± 0.0247 | 0.5344 ± 0.0677 |
   56 | 0.6925 ± 0.0083 | 0.5375 ± 0.0306 | 0.7108 ± 0.0265 | 0.4813 ± 0.0841 |
   57 | 0.6849 ± 0.0132 | 0.5500 ± 0.0508 | 0.6822 ± 0.0256 | 0.5938 ± 0.1092 |
   58 | 0.7021 ± 0.0639 | 0.5750 ± 0.1275 | 0.7025 ± 0.0560 | 0.5719 ± 0.0827 |
   59 | 0.6862 ± 0.0270 | 0.5188 ± 0.0980 | 0.6930 ± 0.0029 | 0.5062 ± 0.0914 |
   60 | 0.7442 ± 0.0403 | 0.4688 ± 0.0442 | 0.7300 ± 0.0335 | 0.4875 ± 0.0673 |
   61 | 0.7218 ± 0.0265 | 0.4938 ± 0.0459 | 0.6964 ± 0.0199 | 0.5188 ± 0.0875 |
   62 | 0.6737 ± 0.0200 | 0.5687 ± 0.0848 | 0.6906 ± 0.0375 | 0.4969 ± 0.1002 |
   63 | 0.6564 ± 0.0237 | 0.6375 ± 0.0424 | 0.6585 ± 0.0214 | 0.6500 ± 0.0836 |
   64 | 0.6971 ± 0.0301 | 0.5813 ± 0.0940 | 0.7221 ± 0.0117 | 0.4281 ± 0.0610 |
   65 | 0.6642 ± 0.0354 | 0.6125 ± 0.0643 | 0.6532 ± 0.0313 | 0.6531 ± 0.0921 |
   66 | 0.7009 ± 0.0208 | 0.5062 ± 0.1035 | 0.7139 ± 0.0370 | 0.4844 ± 0.1057 |
   67 | 0.6931 ± 0.0937 | 0.6062 ± 0.1146 | 0.6950 ± 0.0043 | 0.4562 ± 0.0980 |
   68 | 0.6023 ± 0.0761 | 0.7000 ± 0.0643 | 0.6424 ± 0.0574 | 0.6375 ± 0.0805 |
   69 | 0.6658 ± 0.0417 | 0.5813 ± 0.0580 | 0.6163 ± 0.0311 | 0.7063 ± 0.0563 |
   70 | 0.5823 ± 0.0312 | 0.7063 ± 0.0319 | 0.6040 ± 0.0499 | 0.6937 ± 0.0776 |
   71 | 0.6177 ± 0.0270 | 0.6875 ± 0.0395 | 0.6510 ± 0.0666 | 0.6094 ± 0.1138 |
   72 | 0.6309 ± 0.0702 | 0.6562 ± 0.0927 | 0.6385 ± 0.0349 | 0.6469 ± 0.0524 |
   73 | 0.6210 ± 0.0562 | 0.6500 ± 0.0538 | 0.6130 ± 0.0387 | 0.6781 ± 0.0873 |
   74 | 0.6352 ± 0.0371 | 0.6312 ± 0.0637 | 0.6219 ± 0.0615 | 0.6594 ± 0.0705 |
   75 | 0.6246 ± 0.0278 | 0.6625 ± 0.0848 | 0.5894 ± 0.0568 | 0.6813 ± 0.0813 |
   76 | 0.6688 ± 0.0509 | 0.6250 ± 0.0442 | 0.6045 ± 0.0557 | 0.6875 ± 0.0670 |
   77 | 0.5943 ± 0.0348 | 0.6750 ± 0.0508 | 0.6494 ± 0.0474 | 0.6062 ± 0.0612 |
   78 | 0.6521 ± 0.0687 | 0.6438 ± 0.0755 | 0.6276 ± 0.0604 | 0.6344 ± 0.0969 |
   79 | 0.5734 ± 0.0344 | 0.7250 ± 0.0459 | 0.5889 ± 0.0569 | 0.7125 ± 0.0538 |
   80 | 0.6639 ± 0.0683 | 0.6312 ± 0.1431 | 0.6498 ± 0.0614 | 0.6188 ± 0.0710 |
   81 | 0.7844 ± 0.1215 | 0.5062 ± 0.1142 | 0.7476 ± 0.0389 | 0.4781 ± 0.0779 |
   82 | 0.8149 ± 0.2199 | 0.5188 ± 0.1320 | 0.7336 ± 0.0434 | 0.5125 ± 0.0673 |
   83 | 0.6885 ± 0.0256 | 0.5500 ± 0.0424 | 0.6472 ± 0.0186 | 0.6094 ± 0.0688 |
   84 | 0.6619 ± 0.0164 | 0.5938 ± 0.0442 | 0.6445 ± 0.0725 | 0.6656 ± 0.1008 |
   85 | 0.7046 ± 0.0655 | 0.5875 ± 0.1142 | 0.8770 ± 0.0875 | 0.4817 ± 0.0672 |
   86 | 0.8037 ± 0.0596 | 0.5375 ± 0.0606 | 0.6266 ± 0.0551 | 0.6375 ± 0.0950 |
   87 | 0.7255 ± 0.0355 | 0.4688 ± 0.0395 | 0.7446 ± 0.0471 | 0.4594 ± 0.0938 |
   88 | 0.8408 ± 0.1063 | 0.4625 ± 0.0637 | 0.7114 ± 0.0730 | 0.5406 ± 0.1250 |
   89 | 0.8711 ± 0.2973 | 0.5625 ± 0.1202 | 0.7405 ± 0.0445 | 0.4906 ± 0.0839 |
   90 | 0.7818 ± 0.0619 | 0.4750 ± 0.0637 | 0.6915 ± 0.0119 | 0.5156 ± 0.0898 |
   91 | 0.7573 ± 0.0823 | 0.4813 ± 0.0897 | 0.6864 ± 0.0584 | 0.5625 ± 0.0873 |
   92 | 0.6717 ± 0.0309 | 0.5813 ± 0.1111 | 0.6440 ± 0.0292 | 0.6344 ± 0.0839 |
   93 | 0.6571 ± 0.0592 | 0.6000 ± 0.0848 | 0.5757 ± 0.0629 | 0.7375 ± 0.0805 |
   94 | 0.5973 ± 0.0339 | 0.7125 ± 0.0234 | 0.6037 ± 0.0403 | 0.6687 ± 0.0488 |
   95 | 0.6060 ± 0.0389 | 0.6937 ± 0.0606 | 0.6139 ± 0.0409 | 0.6813 ± 0.0682 |
   96 | 0.6416 ± 0.0525 | 0.6438 ± 0.0729 | 0.5903 ± 0.0622 | 0.6906 ± 0.0758 |
   97 | 0.5405 ± 0.0464 | 0.7438 ± 0.0573 | 0.6493 ± 0.0629 | 0.6500 ± 0.0696 |
   98 | 0.6269 ± 0.0381 | 0.6750 ± 0.0673 | 0.6168 ± 0.0470 | 0.6750 ± 0.0527 |
   99 | 0.6663 ± 0.0301 | 0.5938 ± 0.0559 | 0.6294 ± 0.0448 | 0.6562 ± 0.0559 |
  100 | 0.6396 ± 0.0544 | 0.6438 ± 0.0729 | 0.5914 ± 0.0546 | 0.7125 ± 0.0788 |
  101 | 0.6264 ± 0.1083 | 0.6625 ± 0.1241 | 0.5764 ± 0.0427 | 0.7125 ± 0.0573 |
  102 | 0.5716 ± 0.0322 | 0.7312 ± 0.0580 | 0.6265 ± 0.0601 | 0.6438 ± 0.0545 |
  103 | 0.5527 ± 0.0612 | 0.7250 ± 0.0606 | 0.6341 ± 0.0766 | 0.6062 ± 0.0980 |
  104 | 0.6080 ± 0.0442 | 0.6625 ± 0.0667 | 0.6217 ± 0.0819 | 0.6625 ± 0.0976 |
  105 | 0.5507 ± 0.0512 | 0.7500 ± 0.0625 | 0.5817 ± 0.0673 | 0.7063 ± 0.0643 |
  106 | 0.6357 ± 0.0584 | 0.6375 ± 0.0612 | 0.6304 ± 0.0444 | 0.6312 ± 0.0763 |
  107 | 0.6226 ± 0.0738 | 0.6625 ± 0.0935 | 0.5873 ± 0.0405 | 0.7094 ± 0.0443 |
  108 | 0.5863 ± 0.0745 | 0.6937 ± 0.0637 | 0.5691 ± 0.0676 | 0.7188 ± 0.0699 |
  109 | 0.6054 ± 0.0489 | 0.6937 ± 0.0538 | 0.6159 ± 0.0524 | 0.6781 ± 0.0641 |
  110 | 0.6572 ± 0.0831 | 0.6125 ± 0.1111 | 0.6375 ± 0.0734 | 0.6406 ± 0.0864 |
  111 | 0.5971 ± 0.0407 | 0.6937 ± 0.0776 | 0.6119 ± 0.0612 | 0.6781 ± 0.1186 |
  112 | 0.5940 ± 0.0285 | 0.7063 ± 0.0508 | 0.6104 ± 0.0549 | 0.6594 ± 0.0796 |
  113 | 0.6306 ± 0.0303 | 0.6562 ± 0.0442 | 0.6012 ± 0.0599 | 0.6875 ± 0.0656 |
  114 | 0.5750 ± 0.0563 | 0.7063 ± 0.0508 | 0.5831 ± 0.0458 | 0.6937 ± 0.0653 |
  115 | 0.5848 ± 0.0429 | 0.6937 ± 0.0637 | 0.6561 ± 0.0671 | 0.6219 ± 0.0796 |
  116 | 0.5942 ± 0.0061 | 0.6937 ± 0.0459 | 0.6215 ± 0.0461 | 0.6750 ± 0.0897 |
  117 | 0.5778 ± 0.0652 | 0.7125 ± 0.0606 | 0.5972 ± 0.0442 | 0.6781 ± 0.0485 |
  118 | 0.5940 ± 0.0692 | 0.7063 ± 0.0940 | 0.6098 ± 0.0696 | 0.6719 ± 0.0864 |
  119 | 0.6543 ± 0.0407 | 0.5875 ± 0.0538 | 0.6559 ± 0.0488 | 0.5781 ± 0.0674 |
  120 | 0.6024 ± 0.0997 | 0.6875 ± 0.1027 | 0.6053 ± 0.0725 | 0.6875 ± 0.1055 |
  121 | 0.6237 ± 0.1076 | 0.6438 ± 0.1275 | 0.6493 ± 0.0669 | 0.6594 ± 0.0921 |
  122 | 0.6234 ± 0.0215 | 0.6000 ± 0.0364 | 0.8462 ± 0.1104 | 0.5250 ± 0.0696 |
  123 | 0.6414 ± 0.0383 | 0.6625 ± 0.0573 | 0.6750 ± 0.1008 | 0.5219 ± 0.0740 |
  124 | 0.7034 ± 0.1838 | 0.6125 ± 0.1335 | 0.6894 ± 0.0858 | 0.6312 ± 0.0637 |
  125 | 0.7277 ± 0.0587 | 0.4938 ± 0.0500 | 0.6939 ± 0.0310 | 0.5344 ± 0.1012 |
  126 | 0.7248 ± 0.0900 | 0.6125 ± 0.0545 | 0.7197 ± 0.0234 | 0.5156 ± 0.0376 |
  127 | 0.7722 ± 0.1416 | 0.4437 ± 0.1035 | 0.6862 ± 0.0149 | 0.5594 ± 0.0473 |
  128 | 0.7305 ± 0.0409 | 0.5000 ± 0.0442 | 0.7042 ± 0.0166 | 0.4938 ± 0.0556 |
  129 | 0.7725 ± 0.0804 | 0.5625 ± 0.0625 | 0.8066 ± 0.0877 | 0.5031 ± 0.0889 |
  130 | 0.8195 ± 0.1204 | 0.4688 ± 0.0559 | 0.8305 ± 0.0891 | 0.4813 ± 0.0897 |
  131 | 0.7565 ± 0.1138 | 0.5250 ± 0.0871 | 0.7237 ± 0.0547 | 0.5375 ± 0.0813 |
  132 | 0.7026 ± 0.0149 | 0.5062 ± 0.0573 | 0.6970 ± 0.0176 | 0.5156 ± 0.0702 |
  133 | 0.6979 ± 0.0102 | 0.4875 ± 0.0781 | 0.6946 ± 0.0025 | 0.4406 ± 0.1078 |
  134 | 0.7190 ± 0.0250 | 0.4813 ± 0.0545 | 0.7230 ± 0.0313 | 0.5062 ± 0.0606 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.7020 ± 0.0469 | 0.5437 ± 0.1000 | 0.6627 ± 0.0607 | 0.6156 ± 0.1136 |
    2 | 0.6551 ± 0.0476 | 0.6000 ± 0.1142 | 0.6327 ± 0.0299 | 0.6594 ± 0.0932 |
    3 | 0.5710 ± 0.0696 | 0.7188 ± 0.0342 | 0.6090 ± 0.1065 | 0.7000 ± 0.0580 |
    4 | 0.6162 ± 0.0109 | 0.6562 ± 0.0342 | 0.6445 ± 0.0390 | 0.6281 ± 0.1041 |
    5 | 0.6138 ± 0.0283 | 0.6687 ± 0.0508 | 0.6567 ± 0.0348 | 0.5500 ± 0.0424 |
    6 | 0.5916 ± 0.0522 | 0.7188 ± 0.0765 | 0.6529 ± 0.0713 | 0.6438 ± 0.0545 |
    7 | 0.6045 ± 0.0298 | 0.6937 ± 0.0637 | 0.6193 ± 0.0768 | 0.6687 ± 0.0829 |
    8 | 0.6039 ± 0.0307 | 0.6937 ± 0.0538 | 0.6094 ± 0.0366 | 0.6219 ± 0.0832 |
    9 | 0.5843 ± 0.0402 | 0.7063 ± 0.0673 | 0.6301 ± 0.0716 | 0.6219 ± 0.1050 |
   10 | 0.6122 ± 0.0233 | 0.6813 ± 0.0364 | 0.6344 ± 0.0980 | 0.6562 ± 0.0740 |
   11 | 0.6360 ± 0.0801 | 0.6813 ± 0.0573 | 0.6074 ± 0.0550 | 0.6781 ± 0.0959 |
   12 | 0.6625 ± 0.1013 | 0.6125 ± 0.0919 | 0.6524 ± 0.0636 | 0.5219 ± 0.0928 |
   13 | 0.6235 ± 0.0252 | 0.6813 ± 0.0459 | 0.6460 ± 0.0468 | 0.5875 ± 0.0737 |
   14 | 0.5977 ± 0.0477 | 0.7000 ± 0.0545 | 0.6123 ± 0.0701 | 0.6594 ± 0.0844 |
   15 | 0.5993 ± 0.0730 | 0.6813 ± 0.0637 | 0.7421 ± 0.1076 | 0.6219 ± 0.0758 |
   16 | 0.5999 ± 0.0557 | 0.7125 ± 0.0573 | 0.7768 ± 0.1266 | 0.6594 ± 0.0616 |
   17 | 0.6244 ± 0.0296 | 0.6687 ± 0.0612 | 0.6446 ± 0.0773 | 0.6562 ± 0.0593 |
   18 | 0.6422 ± 0.1093 | 0.6438 ± 0.1093 | 0.5941 ± 0.0801 | 0.7125 ± 0.0776 |
   19 | 0.7344 ± 0.1336 | 0.5312 ± 0.1665 | 0.7106 ± 0.0438 | 0.5687 ± 0.0622 |
   20 | 0.6769 ± 0.0380 | 0.5687 ± 0.1072 | 0.6697 ± 0.1576 | 0.6625 ± 0.1044 |
   21 | 0.5415 ± 0.0818 | 0.7375 ± 0.0960 | 0.7771 ± 0.1242 | 0.6562 ± 0.0699 |
   22 | 0.6557 ± 0.0756 | 0.5625 ± 0.1100 | 0.6613 ± 0.0837 | 0.6375 ± 0.0781 |
   23 | 0.6441 ± 0.0466 | 0.6750 ± 0.0319 | 0.6239 ± 0.0787 | 0.6594 ± 0.0844 |
   24 | 0.7163 ± 0.1105 | 0.6562 ± 0.0815 | 0.7728 ± 0.0368 | 0.3438 ± 0.0827 |
   25 | 0.5653 ± 0.0609 | 0.7562 ± 0.0500 | 0.6016 ± 0.0135 | 0.6781 ± 0.0244 |
   26 | 0.7267 ± 0.0750 | 0.5625 ± 0.0948 | 0.6174 ± 0.0537 | 0.6625 ± 0.0836 |
   27 | 0.6916 ± 0.0588 | 0.5750 ± 0.0781 | 0.6806 ± 0.0228 | 0.5625 ± 0.0978 |
   28 | 0.7647 ± 0.0826 | 0.5750 ± 0.0852 | 0.6351 ± 0.0592 | 0.6375 ± 0.0897 |
   29 | 0.7548 ± 0.1152 | 0.5188 ± 0.0940 | 0.7441 ± 0.0366 | 0.5250 ± 0.0459 |
   30 | 0.9059 ± 0.2143 | 0.4875 ± 0.0673 | 0.7553 ± 0.0715 | 0.4938 ± 0.0763 |
   31 | 0.7855 ± 0.1103 | 0.5188 ± 0.0781 | 0.8323 ± 0.1437 | 0.5437 ± 0.1111 |
   32 | 0.7581 ± 0.0708 | 0.5375 ± 0.0606 | 0.8068 ± 0.0658 | 0.4719 ± 0.0745 |
   33 | 0.7697 ± 0.0908 | 0.5062 ± 0.1272 | 0.8517 ± 0.1164 | 0.5000 ± 0.0873 |
   34 | 0.7773 ± 0.0656 | 0.5188 ± 0.0468 | 0.7120 ± 0.0752 | 0.4875 ± 0.1335 |
   35 | 0.7369 ± 0.0629 | 0.5312 ± 0.1008 | 0.7019 ± 0.0210 | 0.5031 ± 0.0745 |
   36 | 0.7043 ± 0.0289 | 0.5125 ± 0.0545 | 0.6964 ± 0.0133 | 0.5000 ± 0.0815 |
   37 | 0.9464 ± 0.2021 | 0.4313 ± 0.1035 | 1.4193 ± 0.2069 | 0.4500 ± 0.0852 |
   38 | 0.7032 ± 0.0146 | 0.4500 ± 0.0702 | 0.6905 ± 0.0259 | 0.5437 ± 0.0950 |
   39 | 0.7357 ± 0.0860 | 0.5312 ± 0.1169 | 0.7158 ± 0.0506 | 0.5062 ± 0.1116 |
   40 | 0.9352 ± 0.1500 | 0.4688 ± 0.0765 | 1.2185 ± 0.0983 | 0.4781 ± 0.0465 |
   41 | 0.8394 ± 0.1336 | 0.4875 ± 0.0755 | 0.6995 ± 0.0110 | 0.4813 ± 0.0673 |
   42 | 0.6869 ± 0.0145 | 0.5563 ± 0.0459 | 0.7421 ± 0.0310 | 0.4625 ± 0.0622 |
   43 | 0.7072 ± 0.0277 | 0.4813 ± 0.0580 | 0.6937 ± 0.0061 | 0.4969 ± 0.0745 |
   44 | 0.7683 ± 0.0791 | 0.4938 ± 0.1346 | 0.6954 ± 0.0134 | 0.5188 ± 0.0563 |
   45 | 0.6969 ± 0.0169 | 0.5125 ± 0.0805 | 0.7144 ± 0.0442 | 0.4969 ± 0.1059 |
   46 | 0.6774 ± 0.0353 | 0.6000 ± 0.0667 | 0.6853 ± 0.0300 | 0.4844 ± 0.0781 |
   47 | 0.7420 ± 0.1146 | 0.5000 ± 0.1152 | 0.6851 ± 0.0547 | 0.5469 ± 0.1163 |
   48 | 0.7252 ± 0.0265 | 0.5188 ± 0.0545 | 0.6918 ± 0.0566 | 0.6094 ± 0.0730 |
   49 | 0.8854 ± 0.3450 | 0.6250 ± 0.1281 | 0.7017 ± 0.0178 | 0.5188 ± 0.0702 |
   50 | 0.7211 ± 0.0390 | 0.5437 ± 0.0424 | 0.7166 ± 0.0524 | 0.5687 ± 0.0737 |
   51 | 0.7240 ± 0.0572 | 0.5750 ± 0.0580 | 0.6845 ± 0.0085 | 0.5219 ± 0.0699 |
   52 | 0.6710 ± 0.0421 | 0.6188 ± 0.0538 | 0.6763 ± 0.0293 | 0.6062 ± 0.0545 |
   53 | 0.7150 ± 0.0404 | 0.5312 ± 0.0968 | 0.6958 ± 0.0134 | 0.5219 ± 0.0803 |
   54 | 0.6821 ± 0.1031 | 0.6125 ± 0.1637 | 0.7308 ± 0.0192 | 0.3844 ± 0.0873 |
   55 | 0.6643 ± 0.0901 | 0.5875 ± 0.1241 | 0.6757 ± 0.0393 | 0.5813 ± 0.0829 |
   56 | 0.6389 ± 0.0407 | 0.6625 ± 0.0606 | 0.6752 ± 0.0185 | 0.6094 ± 0.0818 |
   57 | 1.0125 ± 0.1223 | 0.4437 ± 0.1256 | 0.7845 ± 0.0394 | 0.4469 ± 0.0542 |
   58 | 0.6951 ± 0.1352 | 0.6312 ± 0.1332 | 0.7619 ± 0.0866 | 0.6094 ± 0.0613 |
   59 | 0.7389 ± 0.0774 | 0.4562 ± 0.0940 | 0.6644 ± 0.0513 | 0.6344 ± 0.1398 |
   60 | 0.7819 ± 0.0663 | 0.4188 ± 0.0852 | 0.6885 ± 0.0532 | 0.5687 ± 0.1176 |
   61 | 0.7505 ± 0.0731 | 0.5437 ± 0.1392 | 0.7265 ± 0.0385 | 0.4531 ± 0.1000 |
   62 | 0.7011 ± 0.0231 | 0.5500 ± 0.0580 | 0.6828 ± 0.0223 | 0.5719 ± 0.0779 |
   63 | 0.6850 ± 0.0632 | 0.6188 ± 0.0637 | 0.6830 ± 0.0223 | 0.5781 ± 0.0613 |
   64 | 0.7869 ± 0.0848 | 0.4188 ± 0.0643 | 0.7123 ± 0.0246 | 0.4938 ± 0.0667 |
   65 | 0.6807 ± 0.0168 | 0.5687 ± 0.0500 | 0.6940 ± 0.0028 | 0.4875 ± 0.0596 |
   66 | 0.6907 ± 0.0090 | 0.5437 ± 0.0424 | 0.7310 ± 0.0260 | 0.4562 ± 0.0643 |
   67 | 0.7819 ± 0.0954 | 0.5125 ± 0.1019 | 0.6943 ± 0.0112 | 0.5031 ± 0.0973 |
   68 | 0.7052 ± 0.0135 | 0.4813 ± 0.0755 | 0.7313 ± 0.0437 | 0.4906 ± 0.0839 |
   69 | 0.7899 ± 0.0985 | 0.4437 ± 0.0996 | 0.7061 ± 0.0537 | 0.5469 ± 0.0950 |
   70 | 0.7404 ± 0.0505 | 0.4000 ± 0.0871 | 0.7100 ± 0.0291 | 0.4750 ± 0.1035 |
   71 | 0.7956 ± 0.1196 | 0.4375 ± 0.1355 | 0.6868 ± 0.0472 | 0.5719 ± 0.0999 |
   72 | 0.6953 ± 0.0058 | 0.4750 ± 0.0538 | 0.6922 ± 0.0010 | 0.5750 ± 0.0563 |
   73 | 0.7619 ± 0.0593 | 0.4500 ± 0.0508 | 0.7921 ± 0.0686 | 0.4625 ± 0.0904 |
   74 | 0.6967 ± 0.0077 | 0.5687 ± 0.0364 | 0.6885 ± 0.0381 | 0.5594 ± 0.1002 |
   75 | 0.7323 ± 0.0599 | 0.5500 ± 0.0468 | 0.7477 ± 0.0657 | 0.4844 ± 0.1085 |
   76 | 0.7450 ± 0.0690 | 0.5125 ± 0.0919 | 0.7125 ± 0.0134 | 0.5031 ± 0.0326 |
   77 | 0.7341 ± 0.0848 | 0.6188 ± 0.0500 | 0.7042 ± 0.0191 | 0.4906 ± 0.0727 |
   78 | 0.7679 ± 0.0716 | 0.4625 ± 0.0606 | 0.8467 ± 0.1000 | 0.5188 ± 0.0817 |
   79 | 0.8600 ± 0.1731 | 0.4750 ± 0.0871 | 0.9611 ± 0.0795 | 0.5094 ± 0.0505 |
   80 | 0.8446 ± 0.1013 | 0.5000 ± 0.0713 | 0.6935 ± 0.0037 | 0.5000 ± 0.0656 |
   81 | 0.7189 ± 0.0318 | 0.4875 ± 0.1057 | 0.7047 ± 0.0213 | 0.5000 ± 0.0699 |
   82 | 0.7030 ± 0.0275 | 0.5188 ± 0.0508 | 0.7102 ± 0.0220 | 0.4781 ± 0.0740 |
   83 | 0.7454 ± 0.0501 | 0.5000 ± 0.0685 | 0.7118 ± 0.0363 | 0.4906 ± 0.1018 |
   84 | 0.6911 ± 0.0056 | 0.5250 ± 0.0459 | 0.6964 ± 0.0186 | 0.5281 ± 0.0600 |
   85 | 0.7938 ± 0.0933 | 0.5563 ± 0.0306 | 0.7841 ± 0.0812 | 0.4890 ± 0.0989 |
   86 | 0.6924 ± 0.0375 | 0.5250 ± 0.1072 | 0.7670 ± 0.0713 | 0.5156 ± 0.0841 |
   87 | 0.6804 ± 0.0120 | 0.5875 ± 0.0459 | 0.7123 ± 0.0265 | 0.4719 ± 0.0889 |
   88 | 0.6918 ± 0.0571 | 0.5188 ± 0.1613 | 0.8412 ± 0.0518 | 0.4875 ± 0.0488 |
   89 | 0.7234 ± 0.0422 |/disk1/e.darco/venvs/pytorch-nightly/bin/python: Error while finding module specification for 'grid-search.py' (ModuleNotFoundError: __path__ attribute not found on 'grid-search' while trying to find 'grid-search.py'). Try using 'grid-search' instead of 'grid-search.py' as the module name.
 0.5250 ± 0.0606 | 0.8061 ± 0.0675 | 0.4906 ± 0.0727 |
   90 | 0.7293 ± 0.0617 | 0.4688 ± 0.0968 | 0.9351 ± 0.0795 | 0.4719 ± 0.0600 |
   91 | 0.7887 ± 0.0855 | 0.4688 ± 0.0523 | 0.7676 ± 0.0664 | 0.5188 ± 0.0768 |
   92 | 0.8078 ± 0.1076 | 0.5188 ± 0.0424 | 0.7138 ± 0.0364 | 0.4906 ± 0.0979 |
   93 | 0.7211 ± 0.0250 | 0.4938 ± 0.0234 | 0.7085 ± 0.0475 | 0.5219 ± 0.1056 |
   94 | 0.7354 ± 0.0368 | 0.4062 ± 0.0342 | 0.7330 ± 0.0344 | 0.4656 ± 0.0771 |
   95 | 0.7753 ± 0.1360 | 0.5188 ± 0.1433 | 0.7815 ± 0.0742 | 0.4656 ± 0.1031 |
   96 | 0.7394 ± 0.1110 | 0.5750 ± 0.0755 | 0.6937 ± 0.0009 | 0.4875 ± 0.0424 |
   97 | 0.7164 ± 0.0279 | 0.4313 ± 0.0723 | 0.6937 ± 0.0284 | 0.5344 ± 0.0973 |
   98 | 0.8530 ± 0.1257 | 0.4188 ± 0.0643 | 1.0090 ± 0.1318 | 0.4594 ± 0.0884 |
   99 | 0.7164 ± 0.0148 | 0.4500 ± 0.0755 | 0.6986 ± 0.0100 | 0.4844 ± 0.0644 |
  100 | 0.8049 ± 0.0653 | 0.5563 ± 0.0606 | 1.1009 ± 0.1624 | 0.4906 ± 0.0862 |
  101 | 0.7280 ± 0.0490 | 0.5125 ± 0.0805 | 0.7866 ± 0.0864 | 0.5250 ± 0.0871 |
  102 | 0.7620 ± 0.0660 | 0.5125 ± 0.0702 | 0.7768 ± 0.0724 | 0.5156 ± 0.0806 |
  103 | 0.6960 ± 0.0043 | 0.4938 ± 0.0364 | 0.6936 ± 0.0046 | 0.4969 ± 0.0719 |
  104 | 0.7377 ± 0.0417 | 0.4375 ± 0.0815 | 0.7098 ± 0.0245 | 0.4719 ± 0.0911 |
  105 | 0.7153 ± 0.0199 | 0.4625 ± 0.0500 | 0.7041 ± 0.0251 | 0.5250 ± 0.0606 |
  106 | 0.7602 ± 0.0852 | 0.5188 ± 0.0424 | 0.7544 ± 0.0452 | 0.4875 ± 0.0688 |
  107 | 0.7176 ± 0.0246 | 0.5250 ± 0.0696 | 0.6981 ± 0.0186 | 0.5219 ± 0.0610 |
  108 | 0.7686 ± 0.0705 | 0.5125 ± 0.0805 | 0.7639 ± 0.0560 | 0.5094 ± 0.0699 |
  109 | 0.6946 ± 0.0083 | 0.5188 ± 0.0612 | 0.6985 ± 0.0131 | 0.4906 ± 0.0753 |
  110 | 1.0046 ± 0.3671 | 0.4875 ± 0.0980 | 0.7310 ± 0.0540 | 0.4844 ± 0.1094 |
  111 | 0.7243 ± 0.0207 | 0.4750 ± 0.0364 | 0.6930 ± 0.0058 | 0.5094 ± 0.0938 |
  112 | 0.7171 ± 0.0188 | 0.4750 ± 0.0364 | 0.6931 ± 0.0027 | 0.5062 ± 0.0622 |
  113 | 0.7320 ± 0.0318 | 0.5563 ± 0.0364 | 0.7408 ± 0.0675 | 0.5281 ± 0.0900 |
  114 | 0.7997 ± 0.1104 | 0.4813 ± 0.0755 | 0.7955 ± 0.0669 | 0.5219 ± 0.0656 |
  115 | 0.7329 ± 0.0664 | 0.5250 ± 0.0935 | 0.9288 ± 0.0777 | 0.4969 ± 0.0549 |
  116 | 0.8227 ± 0.0892 | 0.5000 ± 0.0442 | 0.7297 ± 0.1012 | 0.5563 ± 0.1225 |
  117 | 0.7159 ± 0.0367 | 0.4813 ± 0.0919 | 0.7028 ± 0.0116 | 0.4750 ± 0.0590 |
  118 | 0.8098 ± 0.0689 | 0.4813 ± 0.0153 | 0.8449 ± 0.0871 | 0.4781 ± 0.0839 |
  119 | 0.7313 ± 0.0652 | 0.5125 ± 0.0829 | 0.7134 ± 0.0370 | 0.4688 ± 0.1250 |
  120 | 0.7119 ± 0.0529 | 0.5312 ± 0.0884 | 0.7633 ± 0.0575 | 0.5031 ± 0.0745 |
  121 | 0.7785 ± 0.1473 | 0.4938 ± 0.1444 | 0.7883 ± 0.0782 | 0.5156 ± 0.0818 |
  122 | 0.7193 ± 0.0491 | 0.4875 ± 0.0805 | 0.7213 ± 0.0262 | 0.4469 ± 0.0851 |
  123 | 0.7226 ± 0.0495 | 0.4688 ± 0.0713 | 0.7101 ± 0.0497 | 0.5219 ± 0.1027 |
  124 | 0.7311 ± 0.0471 | 0.4750 ± 0.0500 | 0.7455 ± 0.0606 | 0.4781 ± 0.1065 |
  125 | 0.7513 ± 0.1202 | 0.5000 ± 0.0927 | 0.7016 ± 0.0169 | 0.5219 ± 0.0465 |
  126 | 0.7377 ± 0.0559 | 0.4562 ± 0.0755 | 0.7893 ± 0.0587 | 0.4906 ± 0.0685 |
  127 | 0.6849 ± 0.0208 | 0.5500 ± 0.0643 | 0.7026 ± 0.0289 | 0.5406 ± 0.0594 |
  128 | 0.7424 ± 0.0611 | 0.4938 ± 0.0776 | 0.8291 ± 0.0795 | 0.4844 ± 0.0794 |
  129 | 0.7682 ± 0.1428 | 0.5625 ± 0.1518 | 0.7642 ± 0.0422 | 0.4938 ± 0.0573 |
  130 | 0.8065 ± 0.0595 | 0.4625 ± 0.0606 | 0.8362 ± 0.1175 | 0.5031 ± 0.1059 |
  131 | 0.7482 ± 0.0568 | 0.5125 ± 0.0468 | 0.6996 ± 0.0402 | 0.5344 ± 0.0993 |
  132 | 0.7280 ± 0.0188 | 0.4750 ± 0.0606 | 0.7298 ± 0.0514 | 0.4781 ± 0.1110 |
  133 | 0.7123 ± 0.0257 | 0.5000 ± 0.0559 | 0.7009 ± 0.0276 | 0.5062 ± 0.0996 |
  134 | 0.6997 ± 0.0120 | 0.5062 ± 0.0606 | 0.6968 ± 0.0133 | 0.5031 ± 0.0719 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.7041 ± 0.0312 | 0.5188 ± 0.1406 | 0.7275 ± 0.0218 | 0.3844 ± 0.0948 |
    2 | 0.7205 ± 0.0399 | 0.5437 ± 0.0319 | 0.6791 ± 0.0291 | 0.5250 ± 0.0824 |
    3 | 0.7132 ± 0.0279 | 0.5000 ± 0.0484 | 0.7282 ± 0.0337 | 0.5188 ± 0.0508 |
    4 | 0.6697 ± 0.0484 | 0.5750 ± 0.0875 | 0.6830 ± 0.0159 | 0.5875 ± 0.0710 |
    5 | 0.6162 ± 0.0575 | 0.6875 ± 0.0791 | 0.6497 ± 0.0387 | 0.6094 ± 0.0613 |
    6 | 0.6128 ± 0.0540 | 0.7125 ± 0.0871 | 0.6400 ± 0.0564 | 0.6406 ± 0.0898 |
    7 | 0.6302 ± 0.0749 | 0.6687 ± 0.0580 | 0.6016 ± 0.0362 | 0.6969 ± 0.0560 |
    8 | 0.6871 ± 0.0785 | 0.6250 ± 0.1027 | 0.5790 ± 0.0890 | 0.7000 ± 0.1075 |
    9 | 0.6315 ± 0.0694 | 0.6562 ± 0.0765 | 0.6392 ± 0.0854 | 0.6531 ± 0.0889 |
   10 | 0.5696 ± 0.0947 | 0.7000 ± 0.0508 | 0.5906 ± 0.0493 | 0.6844 ± 0.0513 |
   11 | 0.6613 ± 0.0586 | 0.5563 ± 0.1035 | 0.5861 ± 0.0638 | 0.7031 ± 0.0743 |
   12 | 0.6108 ± 0.1076 | 0.7125 ± 0.1035 | 0.6858 ± 0.0713 | 0.6344 ± 0.0685 |
   13 | 0.7437 ± 0.0813 | 0.4750 ± 0.1192 | 0.6862 ± 0.0100 | 0.5094 ± 0.0839 |
   14 | 0.6669 ± 0.0752 | 0.6188 ± 0.0750 | 0.5953 ± 0.0309 | 0.7125 ± 0.0500 |
   15 | 0.6629 ± 0.0346 | 0.5750 ± 0.0643 | 0.6188 ± 0.0405 | 0.6312 ± 0.0904 |
   16 | 0.6304 ± 0.0786 | 0.6937 ± 0.0723 | 0.5759 ± 0.0727 | 0.7125 ± 0.0813 |
   17 | 0.6416 ± 0.0635 | 0.6500 ± 0.0723 | 0.6403 ± 0.0912 | 0.6687 ± 0.0768 |
   18 | 0.5698 ± 0.0930 | 0.7188 ± 0.0625 | 0.6045 ± 0.0778 | 0.6875 ± 0.0523 |
   19 | 0.5917 ± 0.0620 | 0.7250 ± 0.0667 | 0.5744 ± 0.0999 | 0.7219 ± 0.0820 |
   20 | 0.6082 ± 0.0800 | 0.6750 ± 0.0897 | 0.6204 ± 0.0710 | 0.6656 ± 0.0969 |
   21 | 0.7077 ± 0.0617 | 0.5563 ± 0.1016 | 0.6495 ± 0.0691 | 0.6438 ± 0.0829 |
   22 | 0.5843 ± 0.0444 | 0.7375 ± 0.0673 | 0.6012 ± 0.0734 | 0.6937 ± 0.0710 |
   23 | 0.6373 ± 0.0707 | 0.6875 ± 0.0559 | 0.5768 ± 0.0523 | 0.7094 ± 0.0685 |
   24 | 0.5463 ± 0.0900 | 0.7438 ± 0.0723 | 0.6538 ± 0.0732 | 0.6531 ± 0.0600 |
   25 | 0.5974 ± 0.0509 | 0.7063 ± 0.0702 | 0.6731 ± 0.0662 | 0.6375 ± 0.0545 |
   26 | 0.6403 ± 0.0425 | 0.6375 ± 0.0375 | 0.5875 ± 0.0339 | 0.7125 ± 0.0606 |
   27 | 0.5909 ± 0.0419 | 0.6937 ± 0.0800 | 0.6045 ± 0.0473 | 0.6781 ± 0.0791 |
   28 | 0.5884 ± 0.0380 | 0.6813 ± 0.0871 | 0.5742 ± 0.0405 | 0.7063 ± 0.0527 |
   29 | 0.6193 ± 0.1354 | 0.6438 ± 0.1057 | 0.5662 ± 0.0729 | 0.7250 ± 0.0667 |
   30 | 0.7864 ± 0.0719 | 0.4875 ± 0.0919 | 0.9975 ± 0.1403 | 0.4500 ± 0.1000 |
   31 | 0.7475 ± 0.0435 | 0.5188 ± 0.0468 | 0.7799 ± 0.0390 | 0.4531 ± 0.0580 |
   32 | 0.8086 ± 0.1547 | 0.5250 ± 0.1142 | 1.1890 ± 0.1432 | 0.4469 ± 0.0753 |
   33 | 0.7268 ± 0.0676 | 0.5250 ± 0.0824 | 0.6873 ± 0.0269 | 0.5469 ± 0.0876 |
   34 | 0.7149 ± 0.0811 | 0.5625 ± 0.1383 | 0.8606 ± 0.0937 | 0.5250 ± 0.0800 |
   35 | 0.7815 ± 0.1481 | 0.4938 ± 0.1016 | 0.7570 ± 0.0846 | 0.5094 ± 0.0779 |
   36 | 0.7636 ± 0.0926 | 0.5125 ± 0.0940 | 0.8151 ± 0.1028 | 0.4875 ± 0.1038 |
   37 | 0.7897 ± 0.0761 | 0.5250 ± 0.0415 | 0.7174 ± 0.0469 | 0.5375 ± 0.0653 |
   38 | 0.7476 ± 0.0474 | 0.5125 ± 0.0468 | 0.7092 ± 0.0302 | 0.4906 ± 0.0485 |
   39 | 0.7676 ± 0.0934 | 0.5062 ± 0.0824 | 0.7124 ± 0.0533 | 0.4781 ± 0.0815 |
   40 | 0.6839 ± 0.0116 | 0.5375 ± 0.0606 | 0.7273 ± 0.0643 | 0.5156 ± 0.1057 |
   41 | 0.8712 ± 0.1927 | 0.5563 ± 0.0606 | 0.8089 ± 0.0998 | 0.5062 ± 0.0986 |
   42 | 0.7156 ± 0.0249 | 0.4813 ± 0.0702 | 0.7632 ± 0.0405 | 0.4625 ± 0.0653 |
   43 | 0.7561 ± 0.0780 | 0.5000 ± 0.1046 | 0.8000 ± 0.0678 | 0.5531 ± 0.0542 |
   44 | 0.7541 ± 0.0667 | 0.5000 ± 0.0685 | 0.7435 ± 0.0538 | 0.4844 ± 0.0887 |
   45 | 0.7532 ± 0.0421 | 0.5250 ± 0.0459 | 0.8186 ± 0.0629 | 0.4813 ± 0.0715 |
   46 | 0.8224 ± 0.0977 | 0.4875 ± 0.0508 | 0.7985 ± 0.0431 | 0.4062 ± 0.0726 |
   47 | 0.7352 ± 0.0428 | 0.4688 ± 0.0442 | 0.6828 ± 0.0269 | 0.5500 ± 0.0729 |
   48 | 0.8549 ± 0.1639 | 0.5188 ± 0.0468 | 0.8503 ± 0.0633 | 0.4437 ± 0.0653 |
   49 | 0.6937 ± 0.0306 | 0.5500 ± 0.0755 | 0.8119 ± 0.0829 | 0.4531 ± 0.0991 |
   50 | 0.6865 ± 0.0444 | 0.5625 ± 0.1986 | 0.6748 ± 0.0168 | 0.6000 ± 0.0710 |
   51 | 0.7022 ± 0.0302 | 0.5062 ± 0.0893 | 0.6786 ± 0.0075 | 0.6469 ± 0.0839 |
   52 | 0.7118 ± 0.0449 | 0.4750 ± 0.0824 | 0.6632 ± 0.0269 | 0.6531 ± 0.0855 |
   53 | 0.7588 ± 0.0730 | 0.5125 ± 0.0702 | 0.8579 ± 0.0968 | 0.4437 ± 0.1016 |
   54 | 0.7433 ± 0.0869 | 0.4813 ± 0.1228 | 0.6813 ± 0.0399 | 0.6062 ± 0.0990 |
   55 | 0.7163 ± 0.0220 | 0.5500 ± 0.0468 | 0.7122 ± 0.0512 | 0.4813 ± 0.0563 |
   56 | 0.7217 ± 0.0982 | 0.5437 ± 0.0852 | 0.7330 ± 0.0636 | 0.5188 ± 0.0508 |
   57 | 0.7043 ± 0.0263 | 0.5312 ± 0.0442 | 0.7343 ± 0.0347 | 0.4719 ± 0.0732 |
   58 | 0.7478 ± 0.1519 | 0.5938 ± 0.1100 | 0.6967 ± 0.0076 | 0.4719 ± 0.0844 |
   59 | 0.7197 ± 0.0530 | 0.5188 ± 0.0673 | 0.6931 ± 0.0038 | 0.5062 ± 0.0824 |
   60 | 0.9106 ± 0.2492 | 0.5375 ± 0.1510 | 0.7701 ± 0.0512 | 0.4875 ± 0.0688 |
   61 | 0.7653 ± 0.0584 | 0.4500 ± 0.0755 | 0.7814 ± 0.0715 | 0.4719 ± 0.0963 |
   62 | 0.7615 ± 0.0851 | 0.4938 ± 0.0956 | 0.7387 ± 0.0672 | 0.5406 ± 0.0839 |
   63 | 0.7153 ± 0.0256 | 0.5125 ± 0.0643 | 0.7079 ± 0.0337 | 0.5031 ± 0.0942 |
   64 | 0.7113 ± 0.0612 | 0.5813 ± 0.0580 | 0.7424 ± 0.0826 | 0.5188 ± 0.1154 |
   65 | 0.7118 ± 0.0247 | 0.4625 ± 0.0776 | 0.6975 ± 0.0140 | 0.5094 ± 0.0610 |
   66 | 0.7014 ± 0.0278 | 0.5125 ± 0.0980 | 0.7178 ± 0.0275 | 0.4969 ± 0.0600 |
   67 | 0.7790 ± 0.0308 | 0.4437 ± 0.0871 | 0.6990 ± 0.0209 | 0.4938 ± 0.1081 |
   68 | 0.7997 ± 0.0667 | 0.4250 ± 0.0580 | 0.7882 ± 0.0543 | 0.4906 ± 0.0641 |
   69 | 0.8142 ± 0.1246 | 0.5000 ± 0.0765 | 0.8328 ± 0.0875 | 0.5062 ± 0.0788 |
   70 | 0.7168 ± 0.0277 | 0.4625 ± 0.0956 | 0.7061 ± 0.0162 | 0.4906 ± 0.0560 |
   71 | 0.7547 ± 0.0807 | 0.4688 ± 0.0791 | 0.7950 ± 0.0989 | 0.5469 ± 0.0864 |
   72 | 0.7674 ± 0.0537 | 0.4625 ± 0.0606 | 0.6993 ± 0.0085 | 0.4781 ± 0.0560 |
   73 | 0.6991 ± 0.0410 | 0.5500 ± 0.0897 | 0.7193 ± 0.0295 | 0.4719 ± 0.0820 |
   74 | 0.7893 ± 0.1251 | 0.5062 ± 0.1176 | 0.9373 ± 0.1369 | 0.4375 ± 0.1144 |
   75 | 0.8199 ± 0.1034 | 0.4875 ± 0.0702 | 1.0826 ± 0.1409 | 0.4562 ± 0.0841 |
   76 | 0.7354 ± 0.0617 | 0.5125 ± 0.0980 | 0.6900 ± 0.0402 | 0.5656 ± 0.0844 |
   77 | 0.7104 ± 0.0356 | 0.4875 ± 0.1433 | 0.7880 ± 0.0968 | 0.5031 ± 0.1078 |
   78 | 0.7651 ± 0.0773 | 0.4813 ± 0.0781 | 0.7118 ± 0.0318 | 0.4719 ± 0.1096 |
   79 | 0.7211 ± 0.0294 | 0.4875 ± 0.0468 | 0.7205 ± 0.0703 | 0.5062 ± 0.1417 |
   80 | 0.8036 ± 0.0835 | 0.4938 ± 0.0415 | 0.6930 ± 0.0217 | 0.5312 ± 0.0884 |
   81 | 0.7036 ± 0.0142 | 0.5000 ± 0.0442 | 0.6942 ± 0.0125 | 0.5094 ± 0.0917 |
   82 | 0.7933 ± 0.0866 | 0.5000 ± 0.1169 | 0.8905 ± 0.1467 | 0.4719 ± 0.1246 |
   83 | 0.7112 ± 0.0179 | 0.4688 ± 0.0559 | 0.6955 ± 0.0125 | 0.4938 ± 0.1099 |
   84 | 0.8346 ± 0.1396 | 0.4813 ± 0.0755 | 0.6963 ± 0.0105 | 0.5125 ± 0.0488 |
   85 | 0.7046 ± 0.0165 | 0.4938 ± 0.0415 | 0.7269 ± 0.0331 | 0.4692 ± 0.0803 |
   86 | 0.7436 ± 0.0543 | 0.4500 ± 0.0781 | 0.7265 ± 0.0279 | 0.4750 ± 0.0653 |
   87 | 0.8579 ± 0.1141 | 0.4500 ± 0.0829 | 0.7389 ± 0.0511 | 0.5031 ± 0.0820 |
   88 | 0.8136 ± 0.1005 | 0.4562 ± 0.0919 | 0.7457 ± 0.0382 | 0.4844 ± 0.0644 |
   89 | 0.7094 ± 0.0405 | 0.5312 ± 0.1186 | 0.6925 ± 0.0095 | 0.5188 ± 0.0960 |
   90 | 0.8102 ± 0.1656 | 0.5500 ± 0.1057 | 0.8555 ± 0.1293 | 0.5938 ± 0.0765 |
   91 | 0.7743 ± 0.0821 | 0.4938 ± 0.0824 | 0.7512 ± 0.0563 | 0.5062 ± 0.0788 |
   92 | 0.7240 ± 0.0468 | 0.4437 ± 0.1142 | 0.7472 ± 0.0350 | 0.4344 ± 0.0784 |
   93 | 0.7131 ± 0.0187 | 0.5188 ± 0.0319 | 0.7077 ± 0.0280 | 0.4813 ± 0.1019 |
   94 | 0.6993 ± 0.0095 | 0.4625 ± 0.0976 | 0.7555 ± 0.0371 | 0.4375 ± 0.0740 |
   95 | 0.7982 ± 0.1222 | 0.4938 ± 0.0871 | 0.6932 ± 0.0018 | 0.5000 ± 0.0765 |
   96 | 0.7827 ± 0.1049 | 0.4813 ± 0.0805 | 0.9194 ± 0.0853 | 0.4781 ± 0.0656 |
   97 | 0.7304 ± 0.0538 | 0.5250 ± 0.0723 | 0.7068 ± 0.0331 | 0.5000 ± 0.0998 |
   98 | 0.7156 ± 0.0159 | 0.4813 ± 0.0424 | 0.7178 ± 0.0384 | 0.5125 ± 0.0768 |
   99 | 0.7143 ± 0.0353 | 0.5125 ± 0.0960 | 0.7440 ± 0.0628 | 0.4938 ± 0.1016 |
  100 | 0.9095 ± 0.1521 | 0.4562 ± 0.0852 | 0.9255 ± 0.1336 | 0.5312 ± 0.0850 |
  101 | 0.7991 ± 0.0815 | 0.4750 ± 0.0538 | 0.7162 ± 0.0485 | 0.5219 ± 0.0917 |
  102 | 0.7318 ± 0.0372 | 0.5375 ± 0.0234 | 0.7607 ± 0.0743 | 0.5031 ± 0.0983 |
  103 | 0.7770 ± 0.0540 | 0.4562 ± 0.0729 | 0.6945 ± 0.0225 | 0.5250 ± 0.0914 |
  104 | 0.7122 ± 0.0256 | 0.4750 ± 0.0871 | 0.7465 ± 0.0258 | 0.5188 ± 0.0348 |
  105 | 0.9605 ± 0.1660 | 0.4625 ± 0.0364 | 0.9314 ± 0.0782 | 0.4938 ± 0.0556 |
  106 | 0.8524 ± 0.0902 | 0.4938 ± 0.0667 | 0.8475 ± 0.0861 | 0.5188 ± 0.0702 |
  107 | 0.7000 ± 0.0103 | 0.5062 ± 0.0459 | 0.6926 ± 0.0030 | 0.5188 ± 0.0875 |
  108 | 0.8016 ± 0.0851 | 0.4688 ± 0.0442 | 0.6966 ± 0.0322 | 0.5375 ± 0.0859 |
  109 | 0.7460 ± 0.0412 | 0.4188 ± 0.0643 | 0.6936 ± 0.0167 | 0.5219 ± 0.0862 |
  110 | 0.6968 ± 0.0059 | 0.5000 ± 0.0395 | 0.7123 ± 0.0253 | 0.4844 ± 0.0756 |
  111 | 0.7002 ± 0.0057 | 0.4688 ± 0.0198 | 0.6921 ± 0.0042 | 0.5250 ± 0.0800 |
  112 | 0.7687 ± 0.0918 | 0.5250 ± 0.0776 | 0.9573 ± 0.2093 | 0.5031 ± 0.1365 |
  113 | 0.8068 ± 0.1407 | 0.5062 ± 0.0871 | 0.9141 ± 0.0726 | 0.4938 ± 0.0538 |
  114 | 0.6665 ± 0.0557 | 0.6375 ± 0.0940 | 0.8542 ± 0.0879 | 0.5188 ± 0.0702 |
  115 | 0.7035 ± 0.0490 | 0.5687 ± 0.1090 | 0.7143 ± 0.0248 | 0.4500 ± 0.0960 |
  116 | 0.7606 ± 0.0316 | 0.4375 ± 0.0765 | 0.8152 ± 0.0521 | 0.4625 ± 0.0606 |
  117 | 0.8793 ± 0.0975 | 0.4813 ± 0.0755 | 0.6924 ± 0.0198 | 0.5250 ± 0.1192 |
  118 | 0.7020 ± 0.0404 | 0.5375 ± 0.0637 | 0.7120 ± 0.0259 | 0.4938 ± 0.0710 |
  119 | 0.7251 ± 0.0576 | 0.5250 ± 0.0667 | 0.7050 ± 0.0112 | 0.4500 ± 0.0673 |
  120 | 0.8870 ± 0.1867 | 0.5250 ± 0.0914 | 0.7811 ± 0.0913 | 0.5281 ± 0.0932 |
  121 | 0.7402 ± 0.0635 | 0.4562 ± 0.0897 | 0.6979 ± 0.0265 | 0.5312 ± 0.0740 |
  122 | 0.7454 ± 0.0363 | 0.4688 ± 0.0559 | 0.7045 ± 0.0164 | 0.4781 ± 0.0727 |
  123 | 0.7395 ± 0.0583 | 0.4750 ± 0.0976 | 0.8240 ± 0.0633 | 0.4938 ± 0.0622 |
  124 | 0.7819 ± 0.0839 | 0.5625 ± 0.0685 | 0.9907 ± 0.1144 | 0.4781 ± 0.0753 |
  125 | 0.7196 ± 0.0746 | 0.5312 ± 0.1100 | 0.7379 ± 0.0902 | 0.5219 ± 0.1289 |
  126 | 0.7629 ± 0.0794 | 0.4750 ± 0.0723 | 0.7111 ± 0.0232 | 0.5469 ± 0.0376 |
  127 | 0.7444 ± 0.0758 | 0.4938 ± 0.0723 | 0.7099 ± 0.0338 | 0.5312 ± 0.0656 |
  128 | 0.7214 ± 0.0372 | 0.4562 ± 0.0852 | 0.6976 ± 0.0046 | 0.4437 ± 0.0667 |
  129 | 0.7199 ± 0.0307 | 0.5000 ± 0.1118 | 0.6960 ± 0.0074 | 0.4781 ± 0.0862 |
  130 | 0.7065 ± 0.0218 | 0.5000 ± 0.0968 | 0.7031 ± 0.0282 | 0.5094 ± 0.0873 |
  131 | 0.8028 ± 0.3107 | 0.5687 ± 0.1794 | 0.8865 ± 0.0969 | 0.4469 ± 0.0906 |
  132 | 0.7637 ± 0.0427 | 0.4375 ± 0.0395 | 0.6910 ± 0.0043 | 0.5437 ± 0.0729 |
  133 | 0.8205 ± 0.1308 | 0.4813 ± 0.1320 | 0.8934 ± 0.1189 | 0.5656 ± 0.0719 |
  134 | 0.8023 ± 0.1217 | 0.5375 ± 0.0848 | 1.1388 ± 0.1627 | 0.4375 ± 0.0938 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6865 ± 0.1035 | 0.6438 ± 0.0781 | 0.6507 ± 0.0677 | 0.6344 ± 0.0626 |
    2 | 0.5939 ± 0.0927 | 0.6625 ± 0.1090 | 0.7769 ± 0.1043 | 0.5469 ± 0.0930 |
    3 | 0.6150 ± 0.0195 | 0.7063 ± 0.0545 | 0.6038 ± 0.0524 | 0.6906 ± 0.0662 |
    4 | 0.5935 ± 0.0432 | 0.6875 ± 0.0815 | 0.5964 ± 0.0650 | 0.6750 ± 0.0897 |
    5 | 0.5926 ± 0.0494 | 0.6937 ± 0.0500 | 0.6377 ± 0.0675 | 0.6438 ± 0.0596 |
    6 | 0.5966 ± 0.0294 | 0.6687 ± 0.0852 | 0.6181 ± 0.1136 | 0.6687 ± 0.1038 |
    7 | 0.6391 ± 0.0710 | 0.6562 ± 0.0968 | 0.6062 ± 0.0746 | 0.6750 ± 0.0980 |
    8 | 0.5973 ± 0.0615 | 0.7125 ± 0.0996 | 0.6322 ± 0.0508 | 0.6375 ± 0.0702 |
    9 | 0.6449 ± 0.0989 | 0.6500 ± 0.1053 | 0.5602 ± 0.0544 | 0.6906 ± 0.0600 |
   10 | 0.6180 ± 0.0559 | 0.6813 ± 0.0750 | 0.5785 ± 0.0613 | 0.7031 ± 0.1067 |
   11 | 0.5871 ± 0.0855 | 0.7250 ± 0.0800 | 0.6257 ± 0.0785 | 0.6781 ± 0.0753 |
   12 | 0.6176 ± 0.0399 | 0.6562 ± 0.0625 | 0.6221 ± 0.0750 | 0.6625 ± 0.0986 |
   13 | 0.5942 ± 0.0345 | 0.6750 ± 0.0755 | 0.6155 ± 0.0289 | 0.6687 ± 0.0488 |
   14 | 0.6596 ± 0.0580 | 0.6438 ± 0.0508 | 0.6318 ± 0.0535 | 0.6625 ± 0.0848 |
   15 | 0.5837 ± 0.0386 | 0.7125 ± 0.0538 | 0.6081 ± 0.0445 | 0.6813 ± 0.0763 |
   16 | 0.6257 ± 0.0376 | 0.6875 ± 0.0815 | 0.5739 ± 0.0432 | 0.7156 ± 0.0600 |
   17 | 0.6085 ± 0.0266 | 0.6687 ± 0.0468 | 0.5998 ± 0.0542 | 0.6844 ± 0.0878 |
   18 | 0.6282 ± 0.0385 | 0.5938 ± 0.1169 | 0.6150 ± 0.0422 | 0.5969 ± 0.0832 |
   19 | 0.6201 ± 0.0591 | 0.7000 ± 0.0673 | 0.5852 ± 0.0383 | 0.6656 ± 0.0524 |
   20 | 0.5530 ± 0.0400 | 0.7562 ± 0.0696 | 0.5963 ± 0.0542 | 0.6813 ± 0.0696 |
   21 | 0.6417 ± 0.0833 | 0.6813 ± 0.0871 | 0.6079 ± 0.0682 | 0.6344 ± 0.0803 |
   22 | 0.5637 ± 0.0511 | 0.6687 ± 0.0545 | 0.5626 ± 0.0447 | 0.7344 ± 0.0674 |
   23 | 0.5877 ± 0.0567 | 0.7063 ± 0.0580 | 0.5726 ± 0.1006 | 0.7406 ± 0.1037 |
   24 | 0.6213 ± 0.0483 | 0.6188 ± 0.0415 | 0.6018 ± 0.0430 | 0.7031 ± 0.0528 |
   25 | 0.6444 ± 0.0245 | 0.5687 ± 0.0573 | 0.6188 ± 0.0451 | 0.6406 ± 0.0830 |
   26 | 0.6072 ± 0.0153 | 0.6500 ± 0.0723 | 0.5907 ± 0.0375 | 0.6813 ± 0.0573 |
   27 | 0.5522 ± 0.1016 | 0.6875 ± 0.1118 | 0.6454 ± 0.0819 | 0.6125 ± 0.0729 |
   28 | 0.6055 ± 0.1108 | 0.6813 ± 0.1142 | 0.6060 ± 0.0790 | 0.6844 ± 0.0867 |
   29 | 0.6535 ± 0.0587 | 0.6125 ± 0.0643 | 0.5790 ± 0.0561 | 0.6906 ± 0.0983 |
   30 | 0.6472 ± 0.0524 | 0.6188 ± 0.0800 | 0.6146 ± 0.0976 | 0.6687 ± 0.1259 |
   31 | 0.5847 ± 0.0609 | 0.7063 ± 0.0468 | 0.6222 ± 0.0980 | 0.6562 ± 0.1036 |
   32 | 0.6144 ± 0.0531 | 0.6687 ± 0.0673 | 0.6654 ± 0.0894 | 0.6500 ± 0.0925 |
   33 | 0.5319 ± 0.0781 | 0.7562 ± 0.0848 | 0.5828 ± 0.0563 | 0.7156 ± 0.0647 |
   34 | 0.5816 ± 0.0701 | 0.6813 ± 0.1159 | 0.6178 ± 0.0626 | 0.6188 ± 0.0556 |
   35 | 0.6117 ± 0.0467 | 0.6750 ± 0.0805 | 0.6044 ± 0.0340 | 0.6312 ± 0.0480 |
   36 | 0.6005 ± 0.0345 | 0.7438 ± 0.0500 | 0.5957 ± 0.0358 | 0.6844 ± 0.0719 |
   37 | 0.6112 ± 0.0458 | 0.6937 ± 0.0538 | 0.5988 ± 0.0509 | 0.6687 ± 0.0729 |
   38 | 0.6153 ± 0.0886 | 0.6625 ± 0.0871 | 0.6201 ± 0.0752 | 0.6937 ± 0.0848 |
   39 | 0.6517 ± 0.0258 | 0.6125 ± 0.0375 | 0.6059 ± 0.0370 | 0.5969 ± 0.0632 |
   40 | 0.5843 ± 0.0275 | 0.6937 ± 0.0538 | 0.6204 ± 0.0753 | 0.6906 ± 0.0932 |
   41 | 0.6490 ± 0.0497 | 0.5563 ± 0.0637 | 0.6170 ± 0.0566 | 0.6188 ± 0.0573 |
   42 | 0.5972 ± 0.0704 | 0.7063 ± 0.0580 | 0.5948 ± 0.0615 | 0.6937 ± 0.0824 |
   43 | 0.5611 ± 0.0881 | 0.7312 ± 0.0940 | 0.6295 ± 0.1011 | 0.6656 ± 0.1037 |
   44 | 0.6091 ± 0.0401 | 0.6813 ± 0.0364 | 0.5567 ± 0.0556 | 0.7156 ± 0.0796 |
   45 | 0.5944 ± 0.0579 | 0.6438 ± 0.0755 | 0.6312 ± 0.0689 | 0.6094 ± 0.0629 |
   46 | 0.6261 ± 0.0522 | 0.6750 ± 0.0580 | 0.6224 ± 0.0525 | 0.6094 ± 0.0961 |
   47 | 0.5611 ± 0.0211 | 0.7063 ± 0.0375 | 0.6168 ± 0.0913 | 0.6469 ± 0.1326 |
   48 | 0.6250 ± 0.0674 | 0.6250 ± 0.0839 | 0.5912 ± 0.0819 | 0.6687 ± 0.1120 |
   49 | 0.6124 ± 0.0571 | 0.6000 ± 0.1090 | 0.5947 ± 0.0476 | 0.6562 ± 0.0559 |
   50 | 0.6241 ± 0.0316 | 0.6125 ± 0.0729 | 0.6085 ± 0.0764 | 0.6469 ± 0.0740 |
   51 | 0.5440 ± 0.0452 | 0.6937 ± 0.0500 | 0.6255 ± 0.0655 | 0.6125 ± 0.0643 |
   52 | 0.5820 ± 0.0511 | 0.6937 ± 0.0824 | 0.6178 ± 0.0523 | 0.6531 ± 0.0584 |
   53 | 0.5833 ± 0.0492 | 0.6937 ± 0.0459 | 0.5752 ± 0.0813 | 0.7125 ± 0.0573 |
   54 | 0.5281 ± 0.0638 | 0.7625 ± 0.0829 | 0.5619 ± 0.0771 | 0.7188 ± 0.0699 |
   55 | 0.5613 ± 0.0379 | 0.6750 ± 0.0545 | 0.6212 ± 0.0562 | 0.5594 ± 0.1122 |
   56 | 0.6422 ± 0.0292 | 0.6062 ± 0.0508 | 0.6178 ± 0.0835 | 0.6594 ± 0.0844 |
   57 | 0.6384 ± 0.0632 | 0.6625 ± 0.0848 | 0.6035 ± 0.0706 | 0.6750 ± 0.0841 |
   58 | 0.6177 ± 0.0718 | 0.6500 ± 0.0723 | 0.6463 ± 0.0451 | 0.6094 ± 0.0756 |
   59 | 0.5885 ± 0.0638 | 0.6750 ± 0.0755 | 0.5984 ± 0.0512 | 0.7125 ± 0.1016 |
   60 | 0.6611 ± 0.0444 | 0.6312 ± 0.0667 | 0.5921 ± 0.0464 | 0.6875 ± 0.0576 |
   61 | 0.5626 ± 0.0505 | 0.6687 ± 0.0643 | 0.6190 ± 0.0612 | 0.6719 ± 0.0841 |
   62 | 0.5777 ± 0.0251 | 0.7125 ± 0.0538 | 0.6040 ± 0.0445 | 0.6937 ± 0.0637 |
   63 | 0.5774 ± 0.0528 | 0.7125 ± 0.0500 | 0.6387 ± 0.0879 | 0.6594 ± 0.0855 |
   64 | 0.5990 ± 0.0560 | 0.6813 ± 0.0637 | 0.5964 ± 0.0344 | 0.6750 ± 0.0375 |
   65 | 0.4983 ± 0.0677 | 0.7750 ± 0.0956 | 0.5714 ± 0.0848 | 0.7219 ± 0.0691 |
   66 | 0.6614 ± 0.0463 | 0.6625 ± 0.0364 | 0.5691 ± 0.0520 | 0.7250 ± 0.0637 |
   67 | 0.5985 ± 0.0547 | 0.6813 ± 0.0723 | 0.5917 ± 0.0666 | 0.6875 ± 0.0988 |
   68 | 0.5936 ± 0.1326 | 0.6937 ± 0.0606 | 0.5953 ± 0.0438 | 0.6844 ± 0.0493 |
   69 | 0.5377 ± 0.0264 | 0.7438 ± 0.0306 | 0.6008 ± 0.0966 | 0.6813 ± 0.1108 |
   70 | 0.5758 ± 0.0481 | 0.7125 ± 0.0637 | 0.5812 ± 0.0451 | 0.7063 ± 0.0508 |
   71 | 0.5543 ± 0.0308 | 0.6937 ± 0.0956 | 0.6094 ± 0.0599 | 0.6594 ± 0.0632 |
   72 | 0.5824 ± 0.0497 | 0.6937 ± 0.0459 | 0.5740 ± 0.0698 | 0.7250 ± 0.0590 |
   73 | 0.5998 ± 0.0508 | 0.6687 ± 0.0643 | 0.6152 ± 0.0523 | 0.6312 ± 0.0682 |
   74 | 0.6606 ± 0.0812 | 0.6438 ± 0.0468 | 0.6173 ± 0.0402 | 0.6875 ± 0.0504 |
   75 | 0.6069 ± 0.0485 | 0.6937 ± 0.1287 | 0.5936 ± 0.0427 | 0.6625 ± 0.0500 |
   76 | 0.5837 ± 0.0856 | 0.7125 ± 0.0637 | 0.6342 ± 0.0626 | 0.6531 ± 0.0632 |
   77 | 0.6656 ± 0.0339 | 0.5750 ± 0.0580 | 0.6367 ± 0.0551 | 0.6438 ± 0.0829 |
   78 | 0.6377 ± 0.0919 | 0.6687 ± 0.0805 | 0.6197 ± 0.0383 | 0.6594 ± 0.0513 |
   79 | 0.6072 ± 0.0477 | 0.6375 ± 0.0580 | 0.6224 ± 0.0529 | 0.6031 ± 0.0815 |
   80 | 0.5753 ± 0.0367 | 0.7125 ± 0.0459 | 0.6071 ± 0.0420 | 0.6656 ± 0.0542 |
   81 | 0.6461 ± 0.0635 | 0.6250 ± 0.0948 | 0.5899 ± 0.0558 | 0.7031 ± 0.0730 |
   82 | 0.6632 ± 0.0533 | 0.6000 ± 0.0723 | 0.5970 ± 0.0792 | 0.6781 ± 0.0969 |
   83 | 0.6259 ± 0.0376 | 0.6250 ± 0.0395 | 0.6230 ± 0.0808 | 0.6750 ± 0.1111 |
   84 | 0.6602 ± 0.0691 | 0.6062 ± 0.0960 | 0.6131 ± 0.0627 | 0.6719 ± 0.0961 |
   85 | 0.5710 ± 0.0358 | 0.7000 ± 0.0643 | 0.6235 ± 0.0988 | 0.6538 ± 0.1112 |
   86 | 0.6395 ± 0.0521 | 0.6625 ± 0.0723 | 0.6014 ± 0.0478 | 0.7000 ± 0.0628 |
   87 | 0.6158 ± 0.0362 | 0.6625 ± 0.0573 | 0.6111 ± 0.0392 | 0.6750 ± 0.0852 |
   88 | 0.6302 ± 0.0491 | 0.6375 ± 0.0643 | 0.6184 ± 0.0473 | 0.6500 ± 0.0696 |
   89 | 0.5825 ± 0.0759 | 0.6875 ± 0.0484 | 0.5753 ± 0.0541 | 0.7094 ± 0.0656 |
   90 | 0.5741 ± 0.0193 | 0.6625 ± 0.0306 | 0.6355 ± 0.0688 | 0.6469 ± 0.0505 |
   91 | 0.5878 ± 0.0483 | 0.7000 ± 0.0781 | 0.5943 ± 0.0925 | 0.6937 ± 0.1151 |
   92 | 0.6225 ± 0.0592 | 0.6562 ± 0.0685 | 0.6091 ± 0.0492 | 0.6469 ± 0.0862 |
   93 | 0.6048 ± 0.0546 | 0.6937 ± 0.0667 | 0.6107 ± 0.0681 | 0.6312 ± 0.0750 |
   94 | 0.5535 ± 0.0413 | 0.7438 ± 0.0573 | 0.5873 ± 0.0821 | 0.7125 ± 0.0871 |
   95 | 0.6391 ± 0.0495 | 0.6438 ± 0.0580 | 0.5708 ± 0.0577 | 0.7312 ± 0.0829 |
   96 | 0.5649 ± 0.0364 | 0.7188 ± 0.0395 | 0.6065 ± 0.0755 | 0.7031 ± 0.0702 |
   97 | 0.4720 ± 0.0365 | 0.7875 ± 0.0538 | 0.6574 ± 0.0957 | 0.6750 ± 0.1029 |
   98 | 0.6416 ± 0.0604 | 0.6312 ± 0.0914 | 0.6032 ± 0.0379 | 0.6625 ± 0.0622 |
   99 | 0.6213 ± 0.0782 | 0.7125 ± 0.0776 | 0.6878 ± 0.1199 | 0.6344 ± 0.0699 |
  100 | 0.5485 ± 0.0685 | 0.7438 ± 0.0800 | 0.5960 ± 0.0663 | 0.6906 ± 0.0820 |
  101 | 0.6031 ± 0.1168 | 0.7000 ± 0.0919 | 0.5993 ± 0.0672 | 0.6969 ± 0.0626 |
  102 | 0.6452 ± 0.0547 | 0.6062 ± 0.0960 | 0.5986 ± 0.0502 | 0.6219 ± 0.0705 |
  103 | 0.5630 ± 0.0640 | 0.6813 ± 0.0800 | 0.5861 ± 0.0468 | 0.7094 ± 0.0641 |
  104 | 0.5996 ± 0.0610 | 0.6625 ± 0.0776 | 0.5743 ± 0.0513 | 0.6875 ± 0.0740 |
  105 | 0.6142 ± 0.0559 | 0.7000 ± 0.0580 | 0.5929 ± 0.0701 | 0.6875 ± 0.0791 |
  106 | 0.5682 ± 0.0629 | 0.7063 ± 0.0702 | 0.5914 ± 0.0515 | 0.6937 ± 0.0590 |
  107 | 0.5833 ± 0.0317 | 0.7000 ± 0.0319 | 0.5692 ± 0.0513 | 0.7500 ± 0.0504 |
  108 | 0.6303 ± 0.0702 | 0.6375 ± 0.0829 | 0.5653 ± 0.0655 | 0.7438 ± 0.0904 |
  109 | 0.6196 ± 0.0471 | 0.6562 ± 0.0523 | 0.6033 ± 0.0287 | 0.7031 ± 0.0401 |
  110 | 0.6323 ± 0.0289 | 0.6000 ± 0.0538 | 0.6039 ± 0.0570 | 0.6469 ± 0.0779 |
  111 | 0.6187 ± 0.0483 | 0.6438 ± 0.0729 | 0.6082 ± 0.0612 | 0.6750 ± 0.0729 |
  112 | 0.5772 ± 0.0376 | 0.7250 ± 0.0538 | 0.5870 ± 0.0346 | 0.6875 ± 0.0464 |
  113 | 0.6400 ± 0.0674 | 0.6125 ± 0.0643 | 0.6102 ± 0.0687 | 0.6531 ± 0.0771 |
  114 | 0.5589 ± 0.0475 | 0.6750 ± 0.0545 | 0.6226 ± 0.0593 | 0.6062 ± 0.0715 |
  115 | 0.5822 ± 0.0592 | 0.6937 ± 0.0871 | 0.6041 ± 0.1069 | 0.6875 ± 0.1144 |
  116 | 0.6658 ± 0.0376 | 0.6312 ± 0.0500 | 0.6182 ± 0.0889 | 0.6687 ± 0.1000 |
  117 | 0.5984 ± 0.0198 | 0.6875 ± 0.0740 | 0.5997 ± 0.0787 | 0.6875 ± 0.0827 |
  118 | 0.5885 ± 0.0260 | 0.6562 ± 0.0342 | 0.5631 ± 0.0417 | 0.7219 ± 0.0531 |
  119 | 0.5684 ± 0.0478 | 0.7125 ± 0.0956 | 0.6131 ± 0.0598 | 0.6531 ± 0.0705 |
  120 | 0.6303 ± 0.0278 | 0.6375 ± 0.0643 | 0.6029 ± 0.0357 | 0.6406 ± 0.0509 |
  121 | 0.5713 ± 0.0328 | 0.7125 ± 0.0306 | 0.5624 ± 0.0475 | 0.6969 ± 0.0699 |
  122 | 0.5954 ± 0.0508 | 0.7625 ± 0.1019 | 0.6158 ± 0.0486 | 0.6687 ± 0.0715 |
  123 | 0.5973 ± 0.0703 | 0.7063 ± 0.0897 | 0.6052 ± 0.0569 | 0.7094 ± 0.0791 |
  124 | 0.5484 ± 0.0306 | 0.7188 ± 0.0442 | 0.5814 ± 0.0554 | 0.7000 ± 0.0687 |
  125 | 0.6050 ± 0.0884 | 0.6875 ± 0.0656 | 0.6391 ± 0.0712 | 0.6531 ± 0.0705 |
  126 | 0.5950 ± 0.0586 | 0.6937 ± 0.0750 | 0.5909 ± 0.0683 | 0.6875 ± 0.0906 |
  127 | 0.6174 ± 0.0431 | 0.7063 ± 0.0729 | 0.6199 ± 0.0503 | 0.6781 ± 0.0727 |
  128 | 0.6252 ± 0.0530 | 0.5938 ± 0.0656 | 0.5959 ± 0.0731 | 0.6875 ± 0.0978 |
  129 | 0.5867 ± 0.0294 | 0.6813 ± 0.0415 | 0.5729 ± 0.0682 | 0.6906 ± 0.0844 |
  130 | 0.6276 ± 0.0805 | 0.6562 ± 0.1027 | 0.6064 ± 0.0534 | 0.6500 ± 0.0836 |
  131 | 0.5969 ± 0.0604 | 0.6375 ± 0.1000 | 0.6073 ± 0.0646 | 0.6531 ± 0.0796 |
  132 | 0.5503 ± 0.0505 | 0.7000 ± 0.0875 | 0.6258 ± 0.0839 | 0.6438 ± 0.1320 |
  133 | 0.5622 ± 0.0673 | 0.7562 ± 0.0637 | 0.5824 ± 0.0609 | 0.6937 ± 0.0750 |
  134 | 0.5304 ± 0.0758 | 0.7375 ± 0.0643 | 0.5666 ± 0.0615 | 0.7063 ± 0.0488 |
Retrieving the datasets...
Reading ../datasets/new/classificator/random/train_random/train.csv...
Dataset of size 1432650
Sampled a subset of size 71632
Reading ../datasets/new/classificator/random/val_random/val.csv...
Dataset of size 542466
Sampled a subset of size 27123
Retrieve the model...
Retrieve the tokenizer...
Tokenizer checkpoint: Exscientia/IgBert
Retrieve the optimizer...
0.9 0.999
Start training...
Number of batches: 2239 train, 848 val.
Number of epochs: 3
Total number of batches that will be used during training: 6717
Number of batches for a single evaluation: 10
Results are reported every 50 batches
Model: ClassificationFromAveraging
Device detected: cuda
 It.  |               TRAIN               |               EVAL                |
------|-----------------------------------|-----------------------------------|
      |      loss       |    accuracy     |      loss       |    accuracy     |
------|-----------------|-----------------|-----------------|-----------------|
    1 | 0.6336 ± 0.0210 | 0.6438 ± 0.0580 | 0.6408 ± 0.0210 | 0.6375 ± 0.0508 |
    2 | 0.5929 ± 0.0289 | 0.6875 ± 0.0523 | 0.6839 ± 0.0720 | 0.5750 ± 0.0628 |
    3 | 0.6076 ± 0.0325 | 0.6813 ± 0.0234 | 0.6092 ± 0.0471 | 0.7031 ± 0.0730 |
    4 | 0.6123 ± 0.0495 | 0.6625 ± 0.0637 | 0.6400 ± 0.0706 | 0.6188 ± 0.0848 |
    5 | 0.5837 ± 0.0377 | 0.6750 ± 0.0643 | 0.5780 ± 0.0668 | 0.7031 ± 0.1120 |
    6 | 0.6581 ± 0.0626 | 0.6250 ± 0.0765 | 0.6139 ± 0.0296 | 0.6844 ± 0.0549 |
    7 | 0.5978 ± 0.0544 | 0.6937 ± 0.0871 | 0.6306 ± 0.0321 | 0.5906 ± 0.0616 |
    8 | 0.6104 ± 0.0263 | 0.7188 ± 0.0523 | 0.6175 ± 0.0452 | 0.7094 ± 0.0594 |
    9 | 0.5792 ± 0.0329 | 0.6937 ± 0.0573 | 0.5976 ± 0.0512 | 0.6656 ± 0.0713 |
   10 | 0.5967 ± 0.0468 | 0.6687 ± 0.0702 | 0.6228 ± 0.0798 | 0.6375 ± 0.1093 |
   11 | 0.6137 ± 0.0773 | 0.6625 ± 0.1225 | 0.6127 ± 0.0396 | 0.6406 ± 0.0580 |
   12 | 0.6976 ± 0.0489 | 0.5938 ± 0.0656 | 0.5927 ± 0.0605 | 0.6844 ± 0.1222 |
   13 | 0.6029 ± 0.0479 | 0.6562 ± 0.0484 | 0.6036 ± 0.0381 | 0.6500 ± 0.0710 |
   14 | 0.5789 ± 0.0628 | 0.7562 ± 0.0573 | 0.6464 ± 0.0856 | 0.6344 ± 0.0815 |
   15 | 0.6010 ± 0.0726 | 0.6687 ± 0.0897 | 0.6152 ± 0.0594 | 0.6562 ± 0.0625 |
   16 | 0.6311 ± 0.0968 | 0.6250 ± 0.1064 | 0.6052 ± 0.0690 | 0.6531 ± 0.1022 |
   17 | 0.6253 ± 0.0297 | 0.6000 ± 0.0696 | 0.6270 ± 0.0582 | 0.6500 ± 0.0750 |
   18 | 0.6122 ± 0.0394 | 0.6375 ± 0.0580 | 0.6103 ± 0.0416 | 0.6188 ± 0.0556 |
   19 | 0.5752 ± 0.0337 | 0.6625 ± 0.0459 | 0.6317 ± 0.0612 | 0.6125 ± 0.0468 |
   20 | 0.5770 ± 0.0204 | 0.6687 ± 0.0673 | 0.5923 ± 0.0500 | 0.6438 ± 0.0488 |
   21 | 0.5880 ± 0.0375 | 0.7125 ± 0.0538 | 0.6383 ± 0.0447 | 0.6531 ± 0.0691 |
   22 | 0.5469 ± 0.0316 | 0.7250 ± 0.0415 | 0.6260 ± 0.0736 | 0.6375 ± 0.0687 |
   23 | 0.5862 ± 0.0623 | 0.7000 ± 0.0424 | 0.5874 ± 0.0594 | 0.7125 ± 0.0556 |
   24 | 0.6262 ± 0.0450 | 0.6625 ± 0.0696 | 0.5931 ± 0.0554 | 0.7125 ± 0.1072 |
   25 | 0.5551 ± 0.0584 | 0.7250 ± 0.0667 | 0.6048 ± 0.0856 | 0.6531 ± 0.0549 |
   26 | 0.6315 ± 0.0251 | 0.6062 ± 0.0508 | 0.6080 ± 0.0780 | 0.6562 ± 0.1046 |
   27 | 0.5892 ± 0.0879 | 0.7188 ± 0.0815 | 0.6134 ± 0.0572 | 0.7063 ± 0.0446 |
   28 | 0.6442 ± 0.0498 | 0.6500 ± 0.0538 | 0.5783 ± 0.0553 | 0.7063 ± 0.0768 |
   29 | 0.5427 ± 0.0393 | 0.7250 ± 0.0415 | 0.5639 ± 0.0642 | 0.7469 ± 0.0513 |
   30 | 0.5897 ± 0.0672 | 0.6875 ± 0.0968 | 0.5739 ± 0.0637 | 0.7125 ± 0.0723 |
   31 | 0.6538 ± 0.0821 | 0.6438 ± 0.1019 | 0.5817 ± 0.0901 | 0.7000 ± 0.0908 |
   32 | 0.6598 ± 0.0521 | 0.6250 ± 0.0625 | 0.5970 ± 0.0580 | 0.6937 ± 0.1072 |
   33 | 0.6030 ± 0.0977 | 0.7312 ± 0.0980 | 0.6183 ± 0.0445 | 0.6875 ± 0.0504 |
   34 | 0.6099 ± 0.1003 | 0.6188 ± 0.1035 | 0.5703 ± 0.0500 | 0.6719 ± 0.0730 |
   35 | 0.6400 ± 0.0527 | 0.6750 ± 0.0375 | 0.5980 ± 0.0584 | 0.7031 ± 0.0659 |
   36 | 0.6060 ± 0.0961 | 0.6687 ± 0.0940 | 0.5779 ± 0.0821 | 0.7281 ± 0.0699 |
   37 | 0.6162 ± 0.1168 | 0.7188 ± 0.1202 | 0.6121 ± 0.0791 | 0.6813 ± 0.0836 |
   38 | 0.5156 ± 0.0875 | 0.7750 ± 0.0996 | 0.6245 ± 0.0930 | 0.7156 ± 0.0705 |
   39 | 0.6097 ± 0.0577 | 0.6375 ± 0.0580 | 0.6029 ± 0.0597 | 0.6438 ± 0.0793 |
   40 | 0.5800 ± 0.0780 | 0.6687 ± 0.0729 | 0.6110 ± 0.1244 | 0.6906 ± 0.0732 |
   41 | 0.5964 ± 0.0398 | 0.6500 ± 0.0696 | 0.6163 ± 0.0476 | 0.6719 ± 0.0613 |
   42 | 0.5897 ± 0.0737 | 0.6813 ± 0.0871 | 0.6325 ± 0.0836 | 0.6625 ± 0.0637 |
   43 | 0.5916 ± 0.0386 | 0.6625 ± 0.0415 | 0.6439 ± 0.0563 | 0.6188 ± 0.0696 |
   44 | 0.6156 ± 0.1119 | 0.7000 ± 0.0940 | 0.6248 ± 0.0822 | 0.6500 ± 0.0776 |
   45 | 0.6284 ± 0.0497 | 0.6625 ± 0.0893 | 0.6196 ± 0.0517 | 0.6781 ± 0.0791 |
   46 | 0.5969 ± 0.0464 | 0.6750 ± 0.0545 | 0.6120 ± 0.0611 | 0.6625 ± 0.0723 |
   47 | 0.5897 ± 0.0677 | 0.7063 ± 0.0940 | 0.5825 ± 0.0628 | 0.7000 ± 0.0897 |
   48 | 0.6623 ± 0.0766 | 0.6312 ± 0.0637 | 0.6083 ± 0.0527 | 0.6750 ± 0.0852 |
   49 | 0.5506 ± 0.0894 | 0.7688 ± 0.0897 | 0.5613 ± 0.0475 | 0.7000 ± 0.0612 |
   50 | 0.6196 ± 0.0347 | 0.6562 ± 0.0559 | 0.6209 ± 0.0749 | 0.6344 ± 0.0443 |
   51 | 0.6039 ± 0.0645 | 0.6562 ± 0.0593 | 0.6140 ± 0.0538 | 0.6625 ± 0.0710 |
   52 | 0.5531 ± 0.0761 | 0.7375 ± 0.0897 | 0.6508 ± 0.0685 | 0.6500 ± 0.0824 |
   53 | 0.6139 ± 0.0608 | 0.6312 ± 0.0667 | 0.5842 ± 0.0706 | 0.6656 ± 0.0791 |
   54 | 0.6182 ± 0.0414 | 0.6312 ± 0.0723 | 0.5871 ± 0.0778 | 0.6844 ± 0.0719 |
   55 | 0.5890 ± 0.0733 | 0.6500 ± 0.0914 | 0.6197 ± 0.0524 | 0.6500 ± 0.0667 |
   56 | 0.5787 ± 0.0467 | 0.6500 ± 0.0364 | 0.6116 ± 0.0696 | 0.6344 ± 0.1074 |
   57 | 0.6044 ± 0.0858 | 0.6750 ± 0.0980 | 0.5673 ± 0.0652 | 0.6969 ± 0.0699 |
   58 | 0.5720 ± 0.0382 | 0.7000 ± 0.0250 | 0.6068 ± 0.0807 | 0.6844 ± 0.0832 |
   59 | 0.5601 ± 0.0499 | 0.7188 ± 0.0656 | 0.5794 ± 0.0980 | 0.7281 ± 0.0851 |
   60 | 0.6301 ± 0.0968 | 0.6687 ± 0.0755 | 0.5754 ± 0.0811 | 0.7063 ± 0.0817 |
   61 | 0.6542 ± 0.0404 | 0.6312 ± 0.0750 | 0.6547 ± 0.0668 | 0.6188 ± 0.0848 |
   62 | 0.5962 ± 0.0989 | 0.6750 ± 0.1179 | 0.5757 ± 0.0783 | 0.7250 ± 0.0848 |
   63 | 0.6098 ± 0.0172 | 0.6687 ± 0.0781 | 0.6030 ± 0.0850 | 0.6844 ± 0.1002 |
   64 | 0.5994 ± 0.0367 | 0.6875 ± 0.0523 | 0.5783 ± 0.0803 | 0.7031 ± 0.0908 |
   65 | 0.6267 ± 0.0434 | 0.6375 ± 0.1019 | 0.5572 ± 0.0564 | 0.7562 ± 0.1099 |
   66 | 0.5927 ± 0.0419 | 0.7188 ± 0.0656 | 0.6210 ± 0.0787 | 0.6750 ± 0.0829 |
   67 | 0.6355 ± 0.0813 | 0.6500 ± 0.0723 | 0.6282 ± 0.0526 | 0.6281 ± 0.0705 |
   68 | 0.6063 ± 0.0381 | 0.6813 ± 0.0364 | 0.5712 ± 0.0686 | 0.7125 ± 0.0800 |
   69 | 0.6417 ± 0.0288 | 0.6500 ± 0.0538 | 0.5942 ± 0.0308 | 0.6719 ± 0.0898 |
   70 | 0.6202 ± 0.0721 | 0.6875 ± 0.1064 | 0.5923 ± 0.0634 | 0.6875 ± 0.0609 |
   71 | 0.5772 ± 0.1050 | 0.7312 ± 0.0829 | 0.6463 ± 0.0880 | 0.6687 ± 0.0817 |
   72 | 0.6218 ± 0.0604 | 0.6875 ± 0.0906 | 0.5792 ± 0.0570 | 0.6906 ± 0.0796 |
   73 | 0.5834 ± 0.0755 | 0.7188 ± 0.1135 | 0.5804 ± 0.0411 | 0.7031 ± 0.0644 |
   74 | 0.5900 ± 0.0912 | 0.6562 ± 0.1296 | 0.6047 ± 0.0482 | 0.6344 ± 0.0979 |
   75 | 0.5610 ± 0.0988 | 0.6937 ± 0.0637 | 0.5918 ± 0.0532 | 0.7094 ± 0.0626 |
   76 | 0.6124 ± 0.0660 | 0.6813 ± 0.0459 | 0.6412 ± 0.0652 | 0.6438 ± 0.0864 |
   77 | 0.5868 ± 0.0560 | 0.6000 ± 0.0723 | 0.6119 ± 0.0596 | 0.6031 ± 0.1046 |
   78 | 0.6200 ± 0.0610 | 0.6375 ± 0.0940 | 0.5967 ± 0.0716 | 0.6438 ± 0.0829 |
   79 | 0.5734 ± 0.0308 | 0.7125 ± 0.0459 | 0.6012 ± 0.0523 | 0.6531 ± 0.0616 |
   80 | 0.6223 ± 0.0543 | 0.6375 ± 0.1000 | 0.6064 ± 0.0488 | 0.7000 ± 0.0817 |
   81 | 0.6151 ± 0.0402 | 0.6875 ± 0.0559 | 0.5730 ± 0.0564 | 0.6875 ± 0.0699 |
   82 | 0.5655 ± 0.0338 | 0.6125 ± 0.0643 | 0.5878 ± 0.0810 | 0.6750 ± 0.0563 |
   83 | 0.5867 ± 0.0354 | 0.7000 ± 0.0153 | 0.5874 ± 0.0636 | 0.6906 ± 0.0662 |
   84 | 0.5954 ± 0.1040 | 0.7063 ± 0.1290 | 0.5977 ± 0.0881 | 0.6844 ± 0.1012 |
   85 | 0.5670 ± 0.0725 | 0.6937 ± 0.0500 | 0.6032 ± 0.0959 | 0.6852 ± 0.0794 |
   86 | 0.6560 ± 0.0830 | 0.6500 ± 0.0848 | 0.6260 ± 0.0421 | 0.6500 ± 0.0500 |
   87 | 0.5757 ± 0.0594 | 0.7188 ± 0.0713 | 0.6236 ± 0.0879 | 0.6813 ± 0.0824 |
   88 | 0.6110 ± 0.0763 | 0.6687 ± 0.0729 | 0.6121 ± 0.0533 | 0.6531 ± 0.0632 |
   89 | 0.5884 ± 0.0279 | 0.6687 ± 0.0508 | 0.6239 ± 0.0701 | 0.6375 ± 0.0658 |
   90 | 0.5830 ± 0.0664 | 0.6687 ± 0.1019 | 0.5681 ± 0.0607 | 0.7156 ± 0.1031 |
   91 | 0.5657 ± 0.0487 | 0.7188 ± 0.0839 | 0.5730 ± 0.0429 | 0.6906 ± 0.0677 |
   92 | 0.6032 ± 0.0858 | 0.6937 ± 0.1256 | 0.6046 ± 0.0893 | 0.7063 ± 0.1204 |
   93 | 0.5896 ± 0.0616 | 0.7063 ± 0.0919 | 0.6043 ± 0.0445 | 0.7188 ± 0.0685 |
   94 | 0.6445 ± 0.0480 | 0.6687 ± 0.0919 | 0.5784 ± 0.0656 | 0.6937 ± 0.0710 |
   95 | 0.6025 ± 0.0570 | 0.6875 ± 0.0559 | 0.6180 ± 0.0764 | 0.6625 ± 0.0882 |
   96 | 0.6234 ± 0.0275 | 0.6562 ± 0.0523 | 0.6160 ± 0.0373 | 0.6469 ± 0.0524 |
   97 | 0.5363 ± 0.0786 | 0.7812 ± 0.0593 | 0.5850 ± 0.0936 | 0.6937 ± 0.1151 |
   98 | 0.5716 ± 0.0307 | 0.7125 ± 0.0364 | 0.5996 ± 0.0612 | 0.6906 ± 0.0719 |
   99 | 0.6006 ± 0.0649 | 0.6813 ± 0.0776 | 0.6272 ± 0.0591 | 0.6781 ± 0.0594 |
  100 | 0.5497 ± 0.0310 | 0.7312 ± 0.0580 | 0.6232 ± 0.0800 | 0.6469 ± 0.0803 |
  101 | 0.5927 ± 0.0820 | 0.7000 ± 0.0729 | 0.5717 ± 0.0811 | 0.7063 ± 0.0841 |
  102 | 0.6090 ± 0.0719 | 0.6875 ± 0.0927 | 0.6020 ± 0.1006 | 0.6937 ± 0.1279 |
  103 | 0.6434 ± 0.0653 | 0.6188 ± 0.0459 | 0.6077 ± 0.0735 | 0.6906 ± 0.0867 |
  104 | 0.5890 ± 0.0552 | 0.6813 ± 0.0538 | 0.5961 ± 0.0493 | 0.6750 ± 0.0729 |
  105 | 0.6243 ± 0.0917 | 0.5625 ± 0.1027 | 0.5673 ± 0.0406 | 0.7000 ± 0.0781 |
  106 | 0.6681 ± 0.0597 | 0.5312 ± 0.0791 | 0.6449 ± 0.0356 | 0.6062 ± 0.0729 |
  107 | 0.5849 ± 0.0655 | 0.6937 ± 0.0364 | 0.6140 ± 0.0913 | 0.6656 ± 0.0803 |
  108 | 0.5976 ± 0.0835 | 0.7250 ± 0.0606 | 0.5852 ± 0.0605 | 0.7188 ± 0.0726 |
  109 | 0.5733 ± 0.0984 | 0.6813 ± 0.1375 | 0.6042 ± 0.0709 | 0.6281 ± 0.0745 |
  110 | 0.6105 ± 0.0808 | 0.6875 ± 0.1008 | 0.5690 ± 0.0651 | 0.7063 ± 0.0960 |
  111 | 0.5555 ± 0.0666 | 0.7250 ± 0.0956 | 0.6008 ± 0.0901 | 0.6937 ± 0.1081 |
  112 | 0.6186 ± 0.0297 | 0.6937 ± 0.0364 | 0.5861 ± 0.0404 | 0.7094 ± 0.0443 |
  113 | 0.6035 ± 0.0514 | 0.6750 ± 0.0508 | 0.5746 ± 0.0500 | 0.7188 ± 0.0576 |
  114 | 0.5919 ± 0.0648 | 0.7000 ± 0.0940 | 0.5754 ± 0.0620 | 0.7156 ± 0.0616 |
  115 | 0.5494 ± 0.0865 | 0.6750 ± 0.0852 | 0.6452 ± 0.0694 | 0.5625 ± 0.0656 |
  116 | 0.6190 ± 0.1149 | 0.6562 ± 0.0815 | 0.5871 ± 0.0457 | 0.6750 ± 0.0919 |
  117 | 0.6049 ± 0.0710 | 0.7250 ± 0.0637 | 0.5862 ± 0.0545 | 0.6906 ± 0.0808 |
  118 | 0.6308 ± 0.0495 | 0.6625 ± 0.0667 | 0.5851 ± 0.0459 | 0.7188 ± 0.0699 |
  119 | 0.6113 ± 0.0763 | 0.6937 ± 0.0500 | 0.5535 ± 0.0826 | 0.7312 ± 0.0980 |
  120 | 0.5883 ± 0.0830 | 0.6312 ± 0.1256 | 0.6322 ± 0.0774 | 0.6094 ± 0.0876 |
  121 | 0.5766 ± 0.0368 | 0.7000 ± 0.0755 | 0.5994 ± 0.0809 | 0.6781 ± 0.0699 |
  122 | 0.5920 ± 0.0576 | 0.6875 ± 0.0839 | 0.6204 ± 0.0606 | 0.6656 ± 0.0766 |
  123 | 0.5750 ± 0.0457 | 0.6937 ± 0.0606 | 0.5993 ± 0.0692 | 0.6719 ± 0.0950 |
  124 | 0.6262 ± 0.0642 | 0.6875 ± 0.0927 | 0.5810 ± 0.0629 | 0.6594 ± 0.0705 |
  125 | 0.6005 ± 0.0457 | 0.6875 ± 0.0839 | 0.6158 ± 0.0749 | 0.7031 ± 0.0447 |
  126 | 0.5968 ± 0.0469 | 0.6188 ± 0.0723 | 0.6246 ± 0.0521 | 0.5906 ± 0.0855 |
  127 | 0.6834 ± 0.0437 | 0.6125 ± 0.0508 | 0.6469 ± 0.0726 | 0.6375 ± 0.0545 |
  128 | 0.5949 ± 0.0795 | 0.7188 ± 0.0713 | 0.6680 ± 0.0638 | 0.6125 ± 0.0628 |
  129 | 0.6327 ± 0.1206 | 0.6813 ± 0.1035 | 0.6168 ± 0.0655 | 0.6875 ± 0.0740 |
  130 | 0.5690 ± 0.0663 | 0.6687 ± 0.0250 | 0.6225 ± 0.0761 | 0.6188 ± 0.1090 |
  131 | 0.6149 ± 0.0653 | 0.5813 ± 0.0424 | 0.5791 ± 0.0359 | 0.6781 ± 0.0505 |
  132 | 0.6106 ± 0.0639 | 0.6250 ± 0.0815 | 0.6079 ± 0.0745 | 0.6438 ± 0.1171 |
  133 | 0.5942 ± 0.0415 | 0.6750 ± 0.0829 | 0.5919 ± 0.0796 | 0.6844 ± 0.0820 |
  134 | 0.6207 ± 0.0443 | 0.6562 ± 0.0395 | 0.5573 ± 0.0683 | 0.7281 ± 0.0524 |
